{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f2fca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:217: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information retrieval becomes necessary for user to find out concrete information for the abstraction because of the stridently escalation of data on the web. In most of the cases, users feel bore with the very tedious and time consuming job to reveal the main gist of the outcome of the IR. (i) Increase efficiency of other researches to choose documents/information from search engines’ output, which usually contain an excess amount of replicated information. (iv) Reduce the running time of machine for translation is significantly reduced\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load data\n",
    "path_input = 'mydata.csv'\n",
    "df = pd.read_csv(path_input)\n",
    "df['summary'] = df['summary'].replace(r'\\n', '', regex=True)\n",
    "\n",
    "# Load T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", max_len=1024, \n",
    "                                        do_lower_case=True, padding=True,\n",
    "                                        bos_token=\"<s>\", eos_token=\"</s>\", unk_token=\"<unk>\", pad_token=\"<pad>\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define dataset class\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, text_max_token_len=512, summary_max_token_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_max_token_len = text_max_token_len\n",
    "        self.summary_max_token_len = summary_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.data.iloc[index]\n",
    "        text = data_row['text']\n",
    "        summary = str(data_row['summary'])\n",
    "\n",
    "        text_encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.text_max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        summary_encoding = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.summary_max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        \n",
    "        # retrieves the tokenized representation \n",
    "        # of the summary from the summary_encoding dictionary.\n",
    "        # The input_ids key holds the token IDs of the summary.\n",
    "        labels = summary_encoding['input_ids']\n",
    "        \n",
    "        \n",
    "        # 100 is often used to indicate that those positions should be ignored during training\n",
    "        # The positions with padding tokens are not considered during this process.\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': text_encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': text_encoding['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze(),\n",
    "        }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SummaryDataset(df, tokenizer)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=8)\n",
    "\n",
    "# Define summary function\n",
    "def summarizeText(text, model=model, tokenizer=tokenizer, device=device):\n",
    "    text_encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=text_encoding['input_ids'].to(device),\n",
    "        attention_mask=text_encoding['attention_mask'].to(device),\n",
    "        max_length=513,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    preds = [\n",
    "        tokenizer.decode(gen_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for gen_id in generated_ids\n",
    "    ]\n",
    "    return \"\".join(preds)\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"Outcome of the information retrieval becomes necessary for user to find out concrete information for the abstraction\n",
    "because of the stridently escalation of data on the web. Internet is widely used by people to come across information using\n",
    "proficient information retrieval (IR) tools, such as Google, Yahoo, AltaVista, etc., where findings are abundant. In most of the\n",
    "cases, users feel bore with the very tedious and time consuming job to reveal the main gist of the outcome of the IR.\n",
    "Academics and researchers are very much benefitted by using automatic text summarization system as a tool to lessen the\n",
    "amount of time spent manually extracting the chief thoughts from large documents. In addition to the above reason,\n",
    "automatic text summarization also provides its users with numerous benefits as well as:\n",
    "(i) Increase efficiency of other researches to choose documents/information from search engines’ output, which usually\n",
    "contain an excess amount of replicated information.\n",
    "(ii) Solve the limitation of information presentation on small communication devices such as PDA and mobile phone etc.,\n",
    "which is able to display abridged version of the full document.\n",
    "(iii) The running time of machine for translation is significantly reduced if a short version of text is given. \"\"\"\n",
    "\n",
    "summary = summarizeText(text)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35731a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
