{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c392084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2074 > 512). Running this sequence through the model will result in indexing errors\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average ROUGE Scores:\n",
      "ROUGE-1: 0.1049\n",
      "ROUGE-2: 0.0323\n",
      "ROUGE-L: 0.0878\n"
     ]
    }
   ],
   "source": [
    "# ROGUE SCORE T5\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Load CSV file\n",
    "\n",
    "#csv_file_path = \"testing_dataset.csv\"\n",
    "#csv_file_path = \"gen_testing.csv\"\n",
    "csv_file_path = \"math_testing.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Calculate ROUGE metrics\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Accumulate ROUGE scores\n",
    "total_rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "num_entries = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        source_text = row['source_text']\n",
    "        reference_summary = row['reference_summary']\n",
    "\n",
    "        # Check if the text is a string\n",
    "        if not isinstance(source_text, str) or not isinstance(reference_summary, str):\n",
    "            print(f\"Skipping entry at index {index} due to missing or non-text data.\")\n",
    "            continue\n",
    "\n",
    "        # Tokenize input text\n",
    "        input_ids = tokenizer.encode(source_text, return_tensors=\"pt\")\n",
    "\n",
    "        # Generate output text\n",
    "        output_ids = model.generate(input_ids)\n",
    "        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Calculate ROUGE metrics\n",
    "        scores = scorer.score(reference_summary, output_text)\n",
    "\n",
    "        # Accumulate scores\n",
    "        for rouge_type in total_rouge_scores:\n",
    "            total_rouge_scores[rouge_type] += scores[rouge_type].fmeasure\n",
    "\n",
    "        num_entries += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry at index {index}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Calculate average ROUGE scores\n",
    "average_rouge_scores = {rouge_type: total_rouge_scores[rouge_type] / num_entries for rouge_type in total_rouge_scores}\n",
    "\n",
    "# Print average scores\n",
    "print(\"\\nAverage ROUGE Scores:\")\n",
    "print(\"ROUGE-1: {:.4f}\".format(average_rouge_scores['rouge1']))\n",
    "print(\"ROUGE-2: {:.4f}\".format(average_rouge_scores['rouge2']))\n",
    "print(\"ROUGE-L: {:.4f}\".format(average_rouge_scores['rougeL']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b94c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8e7af88",
   "metadata": {},
   "source": [
    "# F1 Score etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15c78d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold Length: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Threshold Length: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Threshold Length: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Threshold Length: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Threshold Length: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1757: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Threshold Length: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1757: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Threshold Length: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1757: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Threshold Length: 195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ABD\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1757: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Load CSV file\n",
    "#csv_file_path = \"math_testing.csv\" \n",
    "#csv_file_path = \"testing_dataset.csv\"\n",
    "csv_file_path = \"gen_testing.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Iterate over different threshold lengths\n",
    "for threshold_length in range(20, 201, 25):\n",
    "    print(f\"\\nThreshold Length: {threshold_length}\")\n",
    "\n",
    "    # Initialize lists to store binary labels and predicted labels\n",
    "    binary_generated_summaries = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Accumulate metrics\n",
    "    num_entries = 0\n",
    "    total_accuracy = 0.0\n",
    "    total_precision = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_f1 = 0.0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            source_text = row['source_text']\n",
    "            reference_summary = row['reference_summary']\n",
    "\n",
    "            # Check if the text is a string\n",
    "            if not isinstance(source_text, str) or not isinstance(reference_summary, str):\n",
    "                print(f\"Skipping entry at index {index} due to missing or non-text data.\")\n",
    "                continue\n",
    "\n",
    "            # Tokenize input text\n",
    "            input_ids = tokenizer.encode(source_text, return_tensors=\"pt\")\n",
    "\n",
    "            # Generate output text\n",
    "            output_ids = model.generate(input_ids)\n",
    "            output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "            # Determine binary label based on the threshold length\n",
    "            binary_generated_summary = 1 if len(output_text) > threshold_length else 0\n",
    "            binary_generated_summaries.append(binary_generated_summary)\n",
    "\n",
    "            # Append true label and predicted label to lists (for illustration purposes)\n",
    "            # Note: Replace the following line with your actual comparison logic\n",
    "            predicted_labels.append(1 if len(output_text) > threshold_length else 0)\n",
    "\n",
    "            num_entries += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing entry at index {index}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(binary_generated_summaries, predicted_labels)\n",
    "    precision = precision_score(binary_generated_summaries, predicted_labels)\n",
    "    recall = recall_score(binary_generated_summaries, predicted_labels)\n",
    "    f1 = f1_score(binary_generated_summaries, predicted_labels)\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "    print(\"Precision: {:.4f}\".format(precision))\n",
    "    print(\"Recall: {:.4f}\".format(recall))\n",
    "    print(\"F1 Score: {:.4f}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c5b2c",
   "metadata": {},
   "source": [
    "# Extractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1bb2306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average ROUGE Scores:\n",
      "ROUGE-1: 0.2579\n",
      "ROUGE-2: 0.0853\n",
      "ROUGE-L: 0.2346\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from rouge import Rouge  # Install this library using: pip install rouge\n",
    "\n",
    "# Load CSV file\n",
    "csv_file_path = \"math_testing.csv\" \n",
    "#csv_file_path = \"testing_dataset.csv\"\n",
    "#csv_file_path = \"gen_testing.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge = Rouge()\n",
    "\n",
    "# Initialize variables for accumulating scores\n",
    "total_rouge1 = 0.0\n",
    "total_rouge2 = 0.0\n",
    "total_rougeL = 0.0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    source_text = row['source_text']\n",
    "    reference_summary = row['reference_summary']\n",
    "\n",
    "    # Tokenization and preprocessing\n",
    "    doc = nlp(source_text)\n",
    "\n",
    "    # Word frequencies\n",
    "    word_frequencies = {}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in STOP_WORDS and word.text.lower() not in punctuation:\n",
    "            word_frequencies[word.text] = word_frequencies.get(word.text, 0) + 1\n",
    "\n",
    "    # Normalize frequencies\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    word_frequencies = {word: freq / max_frequency for word, freq in word_frequencies.items()}\n",
    "\n",
    "    # Sentence scores\n",
    "    sentence_tokens = [sent for sent in doc.sents]\n",
    "    sentence_scores = {sent: sum(word_frequencies.get(word.text.lower(), 0) for word in sent) for sent in sentence_tokens}\n",
    "\n",
    "    # Extractive summarization\n",
    "    select_length = int(len(sentence_tokens) * 0.3)\n",
    "    summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
    "    generated_summary = ' '.join(word.text for sent in summary for word in sent)\n",
    "\n",
    "    # ROUGE scoring\n",
    "    if isinstance(reference_summary, str) and isinstance(generated_summary, str) and generated_summary.strip():  # Check if the generated summary is not empty\n",
    "        rouge_scores = rouge.get_scores(generated_summary, reference_summary)[0]\n",
    "\n",
    "        # Accumulate scores\n",
    "        total_rouge1 += rouge_scores['rouge-1']['f']\n",
    "        total_rouge2 += rouge_scores['rouge-2']['f']\n",
    "        total_rougeL += rouge_scores['rouge-l']['f']\n",
    "\n",
    "        # Print results for each entry (optional)\n",
    "#         print(f\"\\nEntry {index + 1}\")\n",
    "#         print(\"Source Text:\", source_text)\n",
    "#         print(\"Reference Summary:\", reference_summary)\n",
    "#         print(\"Generated Summary:\", generated_summary)\n",
    "#         print(\"ROUGE-1 F1 Score:\", rouge_scores['rouge-1']['f'])\n",
    "#         print(\"ROUGE-2 F1 Score:\", rouge_scores['rouge-2']['f'])\n",
    "#         print(\"ROUGE-L F1 Score:\", rouge_scores['rouge-l']['f'])\n",
    "    else:\n",
    "        print(f\"\\nSkipping entry {index + 1} due to invalid reference or empty generated summary.\")\n",
    "\n",
    "# Calculate average scores\n",
    "num_entries = len(df)\n",
    "avg_rouge1 = total_rouge1 / num_entries\n",
    "avg_rouge2 = total_rouge2 / num_entries\n",
    "avg_rougeL = total_rougeL / num_entries\n",
    "\n",
    "# Print average scores\n",
    "print(\"\\nAverage ROUGE Scores:\")\n",
    "print(\"ROUGE-1: {:.4f}\".format(avg_rouge1))\n",
    "print(\"ROUGE-2: {:.4f}\".format(avg_rouge2))\n",
    "print(\"ROUGE-L: {:.4f}\".format(avg_rougeL))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f481fd8",
   "metadata": {},
   "source": [
    "# F1 score etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbd5c779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entry 42 due to invalid reference format.\n",
      "\n",
      "Threshold Length: 20 - Results:\n",
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "Skipping entry 42 due to invalid reference format.\n",
      "\n",
      "Threshold Length: 45 - Results:\n",
      "Accuracy: 0.7449\n",
      "Precision: 1.0000\n",
      "Recall: 0.7449\n",
      "F1 Score: 0.8538\n",
      "Skipping entry 42 due to invalid reference format.\n",
      "\n",
      "Threshold Length: 70 - Results:\n",
      "Accuracy: 0.2959\n",
      "Precision: 1.0000\n",
      "Recall: 0.2959\n",
      "F1 Score: 0.4567\n",
      "Skipping entry 42 due to invalid reference format.\n",
      "\n",
      "Threshold Length: 95 - Results:\n",
      "Accuracy: 0.1020\n",
      "Precision: 1.0000\n",
      "Recall: 0.1020\n",
      "F1 Score: 0.1852\n",
      "Skipping entry 42 due to invalid reference format.\n",
      "\n",
      "Threshold Length: 120 - Results:\n",
      "Accuracy: 0.0204\n",
      "Precision: 1.0000\n",
      "Recall: 0.0204\n",
      "F1 Score: 0.0400\n",
      "Skipping entry 42 due to invalid reference format.\n",
      "\n",
      "Threshold Length: 145 - Results:\n",
      "Accuracy: 0.0102\n",
      "Precision: 1.0000\n",
      "Recall: 0.0102\n",
      "F1 Score: 0.0202\n",
      "Skipping entry 42 due to invalid reference format.\n",
      "\n",
      "Threshold Length: 170 - Results:\n",
      "Accuracy: 0.0102\n",
      "Precision: 1.0000\n",
      "Recall: 0.0102\n",
      "F1 Score: 0.0202\n",
      "Skipping entry 42 due to invalid reference format.\n",
      "\n",
      "Threshold Length: 195 - Results:\n",
      "Accuracy: 0.0102\n",
      "Precision: 1.0000\n",
      "Recall: 0.0102\n",
      "F1 Score: 0.0202\n",
      "\n",
      "Average Metrics:\n",
      "Average Accuracy: 0.2742\n",
      "Average Precision: 1.0000\n",
      "Average Recall: 0.2742\n",
      "Average F1 Score: 0.3245\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load CSV file\n",
    "#csv_file_path = \"math_testing.csv\"\n",
    "csv_file_path = \"testing_dataset.csv\"\n",
    "# csv_file_path = \"gen_testing.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Tokenization and preprocessing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize variables for accumulating scores\n",
    "total_rouge1 = 0.0\n",
    "total_rouge2 = 0.0\n",
    "total_rougeL = 0.0\n",
    "total_accuracy = 0.0\n",
    "total_precision = 0.0\n",
    "total_recall = 0.0\n",
    "total_f1 = 0.0\n",
    "\n",
    "# Set the range of threshold lengths\n",
    "threshold_range = range(20, 201, 25)\n",
    "\n",
    "for threshold_length in threshold_range:\n",
    "    # Initialize variables for each threshold\n",
    "    binary_generated_summaries = []\n",
    "    true_labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        source_text = row['source_text']\n",
    "        reference_summary = row['reference_summary']\n",
    "\n",
    "        # Tokenization and preprocessing\n",
    "        doc = nlp(source_text)\n",
    "\n",
    "        # Sentence scores\n",
    "        sentence_tokens = [sent for sent in doc.sents]\n",
    "        sentence_lengths = [len(sent) for sent in sentence_tokens]\n",
    "\n",
    "        # Check if the reference summary is valid\n",
    "        if isinstance(reference_summary, str):\n",
    "            # Generate binary labels based on the threshold length\n",
    "            binary_generated_summary = 1 if any(length > threshold_length for length in sentence_lengths) else 0\n",
    "            binary_generated_summaries.append(binary_generated_summary)\n",
    "\n",
    "            # Append true label to the list\n",
    "            true_label = 1 if len(reference_summary) > threshold_length else 0\n",
    "            true_labels.append(true_label)\n",
    "        else:\n",
    "            print(f\"Skipping entry {index + 1} due to invalid reference format.\")\n",
    "\n",
    "    # Check if there are valid entries\n",
    "    if true_labels:\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(true_labels, binary_generated_summaries)\n",
    "\n",
    "        # Set zero_division='warn' to handle the warning\n",
    "        precision = precision_score(true_labels, binary_generated_summaries, zero_division='warn')\n",
    "        recall = recall_score(true_labels, binary_generated_summaries)\n",
    "        f1 = f1_score(true_labels, binary_generated_summaries)\n",
    "\n",
    "        # Accumulate metrics\n",
    "        total_accuracy += accuracy\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "\n",
    "        # Print results for each threshold (optional)\n",
    "        print(f\"\\nThreshold Length: {threshold_length} - Results:\")\n",
    "        print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "        print(\"Precision: {:.4f}\".format(precision))\n",
    "        print(\"Recall: {:.4f}\".format(recall))\n",
    "        print(\"F1 Score: {:.4f}\".format(f1))\n",
    "    else:\n",
    "        print(f\"No valid entries for threshold length {threshold_length}.\")\n",
    "\n",
    "# Calculate average scores\n",
    "num_thresholds = len(threshold_range)\n",
    "avg_accuracy = total_accuracy / num_thresholds\n",
    "avg_precision = total_precision / num_thresholds\n",
    "avg_recall = total_recall / num_thresholds\n",
    "avg_f1 = total_f1 / num_thresholds\n",
    "\n",
    "# Print average scores\n",
    "print(\"\\nAverage Metrics:\")\n",
    "print(\"Average Accuracy: {:.4f}\".format(avg_accuracy))\n",
    "print(\"Average Precision: {:.4f}\".format(avg_precision))\n",
    "print(\"Average Recall: {:.4f}\".format(avg_recall))\n",
    "print(\"Average F1 Score: {:.4f}\".format(avg_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b5310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
