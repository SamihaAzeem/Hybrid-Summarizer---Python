source_text,reference_summary
"Identifying priority areas for ecological conservation and restoration based on circuit theory and dynamic weighted complex network: A case study of the Sichuan Basin Author links open overlay panelCheng Gao a b, Hongyi Pan a b, Mengchao Wang a b, Tianyi Zhang a b, Yanmei He a b, Jianxiong Cheng a b, Caiyi Yao a b Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.ecolind.2023.111064 Get rights and content Under a Creative Commons license open access Highlights • A new research framework for identifying the key Ecological nodes was proposed. • Achieved dynamic analysis on ecological networks and their components. • Weighted complex networks have been used to obtain ecological network topology information with higher credibility. • Rethinking landscape function from the perspective of animal movement. Abstract Regional ecological security is a pressing issue in the context of escalating human-environment conflicts. Ecological networks(ENs), the fundamental tool for characterizing ecosystems, have enabled further quantitative analysis at the micro level by integrating with complex networks in recent years. However, most studies neglect the unreliability of unweighted complex networks and the dynamic characteristics of ENs. This paper takes the Sichuan Basin as the research area and adopts the following methods. Firstly, it integrates landscape ecology and ecosystem services to construct the ENs using Linkage Mapper. Secondly, it introduces the cost-weighted distance as the weight to build complex networks and identifies potential pivot ecological sources and key ecological corridors based on the topological features of the weighted complex networks in 2000, 2010, and 2020. Thirdly, it applies circuit theory to detect ecological pinchpoints and ecological barrier points within the corridors as priority areas for ecological conservation and restoration. The results show that the ENs in the study area are denser on the northern and southern sides, and some ecological corridors change direction due to variations of resistance surfaces and landscape morphology. Through dynamic analysis of the weighted complex networks, 27 potential pivot ecological sources and 25 key ecological corridors are identified; then, 28 priority conservation areas and 10 priority restoration areas within these ecological corridors are extracted based on circuit theory. The study reveals a certain correlation between the distribution of ecological nodes and water bodies. Furthermore, comparing the weighted and unweighted complex network, we find that the weighted complex network is more reasonable, with 64.2% of ecological sources showing lower betweenness centrality than that in the unweighted network, reflecting the obstacles that urbanization poses to ecological networks. This study explores the impact of constantly changing resistance surfaces on the overall ENs and their components through dynamic analysis. The evolving topological features reflect the feedback of the ENs to external environmental changes, as well as the dynamic characteristics of the real ENs. Therefore, the findings of this study provide valuable references for ecological conservation and governance efforts in Sichuan Basin, promoting regional ecological security and the advancement of ecological civilization. Previous article in issueNext article in issue Keywords Sichuan BasinEcological networksComplex networkCircuitscape theory 1. Introduction Human activities are driving rapid land use change on a global scale, which involves the extensive urban development that encroaches upon natural landscapes, and the ecological environment has undergone severe deterioration, resulting in habitat fragmentation and significant loss of natural habitats (Hüse et al., 2016, Scolozzi and Geneletti, 2012). Previous research has demonstrated the vital importance of these patches in maintaining ecological connectivity and enhancing the overall ecological environment, particularly in the context of climate change (Pacifici et al., 2015, Saura et al., 2019). Consequently, it is imperative to identify potential ecological threats and address them within the framework of sustainable development. Ecological networks (ENs) are systems of material, energy, and information flow that describe the interactions within an ecosystem. They provide a scientific perspective for analyzing the ecological environment based on the connections between ecological landscapes (Fath et al., 2007, Opdam et al., 2006). The theory of ENs stems from landscape ecological planning methods and has evolved into a basic framework of “source identification - resistance surface construction - corridor extraction” (Qian et al., 2023). And this workflow can be adapted to different regional characteristics and research needs. Initially, scholars used qualitative criteria such as nature reserves and ecological redlines to identify ecological sources (Gu et al., 2017, Tang et al., 2020). Later, they introduced quantitative indicators such as ecological sensitivity, biodiversity, ecosystem services, and landscape connectivity to improve the accuracy of identifying ecological patches (Liang et al., 2018, Newton et al., 2012). In recent years, the assessment of multiple ecological indicators based on landscape ecology have become common practices for selecting significant ecological areas (Jiang et al., 2021). The development of multi-indicator and multi-perspective identification schemes for ecological sources extraction, as mentioned above, is advantageous for extracting stable ecological sources that meet the research requirements. It also partly compensates for the limitations of ENs in analyzing and implementing research feasibility in specific study areas. Meanwhile, there is an ongoing progress in the methods for constructing resistance surfaces. Initially, scholars assigned resistance values to different land-use types to create these surfaces. Over time, they have incorporated additional factors such as terrain characteristics, nighttime light intensity, and population density to better account for human activity (Peng et al., 2018). Due to the subjectivity of resistance surface construction described above, consequently, some scholars have proposed alternative and relatively objective approaches using the InVEST model. This model can be used to evaluate habitat quality that strongly influences species distribution and migration, thus, the reciprocal transformation of habitat quality could serves as resistance surfaces as areas of better quality posing less threat and resistance to species movement. (Asadolahi et al., 2018). However, the normalized habitat quality values obtained from the InVEST model are on a scale of 0 to 1. With extreme values amplified, reciprocal transforming emphasizes the intensity of the threat source itself and significantly altering the distribution of the original resistance values. And this makes it challenging to accurately represent the gradient distribution of resistance values. The aforementioned methods can provide a more objective reflection of real resistance surfaces, with regional characteristics considered and research needs met. An ecological corridor is a linear landscape that facilitates the movement of materials, energy, and information (Lin et al., 2023a). MCR is currently a widely used model for extracting ecological corridors, it reflects the least cost required for target sources to overcome resistance. Cost distance analysis was conducted to generate minimum cumulative cost routes from each landscape unit to neighboring ecological sources, serving as communication bridges between landscape units and potential pathways for material and energy transport (Ran et al., 2022). Some scholars combine the minimum cumulative resistance model and the gravity model to quantitatively evaluating the intensity of interactions and determining the importance of ecological corridors (Nie et al., 2021, Yao et al., 2023). Several researchers introduce topology indicators in graph theory to evaluate the influence of corridors on the connectivity of ecological networks (Zhou et al., 2023). Additionally, circuit theory-based methods evaluate corridor capacity levels by grading them according to current density (Liang et al., 2023). These methods evaluate corridors in the ecological network from matrix, topological structure, and behavior simulation perspectives, facilitating the identification of important ecological corridors. It's worth noting that targeted ecological management may be more effective for practical ecological conservation work compared to larger-scale ecological corridors, which forcing scholars further identify areas for conservation and restoration, such as pinch points, breakpoints, and barrier points (McRae et al., 2012a). Pinchpoints are areas where species movement is active and necessary, while breakpoints or barriers obstruct movement within ecological corridors (Barnett and Belote, 2021, Dilts et al., 2016). The localization methods for pinchpoints typically involve identifying the intersections between ecological corridors (Dong et al., 2019), the overlap zone between ecological corridors and ecological sources (Ding et al., 2022, Hu et al., 2018), as well as the intersections between the resistance surface “ridgeline” and ecological corridors (Fu et al., 2022). On the other hand, the localization of ecological breakpoints and barriers entails identifying the intersections between transportation roads or development areas and ecological corridors. Among various schemes, the Linkage Mapper(LM), incorporating the Circuitscape and MCR, integrates the extraction of ecological corridors, pinchpoints, and barriers. It is user-friendly and allows for the construction of large-scale ecological networks, making it widely used in ecological network research. Aforementioned research methods help identify key areas that impact the connectivity of ecological networks in heterogeneous spatial environments. However, In-depth quantitative and dynamic analysis are still required in further research on Ens,with landscape morphology, attributes, and land use patterns changing over time","This study focuses on ecological conservation and restoration in the Sichuan Basin, employing a novel approach that combines ecological networks (ENs), complex network analysis, and circuit theory. The research identifies key ecological nodes and corridors within the region using weighted complex networks and dynamic analysis. By considering changes in resistance surfaces and landscape morphology over time, the study extracts priority areas for ecological conservation and restoration. This comprehensive approach contributes valuable insights for promoting ecological security and sustainable development in the Sichuan Basin, addressing the challenges posed by human activities and environmental changes.





"
"Unusual Counting Systems
Written by 
Fact-checked by 
bunch of numbers
tostphoto/Fotolia
In our everyday lives, we use a counting system in which we count from 1 to 10. That seems only logical because our hands are already in what is called base 10, which means the number 10 serves as the base of our number system. Each place in a number corresponds to a different power of 10. For example, 285,553 = (2 × 100,000) + (8 × 10,000) + (5 × 1,000) + (5 × 100) + (5 × 10) + 3. (The final 3 can be considered as 3 × 100, where the zeroth power of 10 is 1.)

However, although base 10 seems logical, that doesn’t mean that it has been universally used. The ancient Babylonians used a base-60 system, which they inherited from the Sumerians. No one knows why they used such a base. It has been speculated that 60 has the mathematically pleasing property of being the smallest number divisible by 1, 2, 3, 4, 5, and 6, though if a lot of divisible numbers are wanted, a base 12 gets you 1 through 4 and 6 but with a much-less-unwieldy system.

With base 10 being a counting system based on fingers, if you bring in the toes, then you can have a base-20 system. Base 20 shows up in French, where 80 is “quatre-vingts,” or “four twenties.” In Danish, 50 is “halvtreds,” or “two and a half times twenty.” Once all the fingers and toes have been used, the body seems finished as the basis of a counting system, but the Oksapmin people of Papua New Guinea have a base-27 body-derived system. The fingers are 1 through 5, and then counting goes up the arm, with the wrist 6, mid-forearm 7, up the arm to the neck at 11, an ear at 12, an eye at 13, the nose at 14, and back down the other side of the head, neck, and arm to the fingers on the other hand being 23 through 27.

Vestiges of other counting systems survive too. The Babylonian base 60 appears in the number of minutes in an hour and seconds in a minute. Base 12 appears in measurement; 12 inches in a foot, 12 months in a year, etc. Base 20 gives the Gettysburg Address some of its rhetorical power, with “Four score and seven years ago” sounding more inspiring than the base-10 “Eighty-seven years ago.” ","Counting systems vary across cultures and history, challenging the notion of a universally logical base 10 system. The ancient Babylonians used base 60, possibly due to its divisibility properties. Base 20 emerges when fingers and toes are counted, seen in French and Danish numbering. The Oksapmin people of Papua New Guinea employ a base-27 system, extending counting from fingers to various body parts. Vestiges of these systems persist in modern measurements and language, highlighting the diversity of human numerical representations.





"
"Semiconductor, any of a class of crystalline solids intermediate in electrical conductivity between a conductor and an insulator. Semiconductors are employed in the manufacture of various kinds of electronic devices, including diodes, transistors, and integrated circuits. Such devices have found wide application because of their compactness, reliability, power efficiency, and low cost. As discrete components, they have found use in power devices, optical sensors, and light emitters, including solid-state lasers. They have a wide range of current- and voltage-handling capabilities and, more important, lend themselves to integration into complex but readily manufacturable microelectronic circuits. They are, and will be in the foreseeable future, the key elements for the majority of electronic systems, serving communications, signal processing, computing, and control applications in both the consumer and industrial markets.

Semiconductor materials
Solid-state materials are commonly grouped into three classes: insulators, semiconductors, and conductors. (At low temperatures some conductors, semiconductors, and insulators may become superconductors.) The figure shows the conductivities σ (and the corresponding resistivities ρ = 1/σ) that are associated with some important materials in each of the three classes. Insulators, such as fused quartz and glass, have very low conductivities, on the order of 10−18 to 10−10 siemens per centimetre; and conductors, such as aluminum, have high conductivities, typically from 104 to 106 siemens per centimetre. The conductivities of semiconductors are between these extremes and are generally sensitive to temperature, illumination, magnetic fields, and minute amounts of impurity atoms. For example, the addition of about 10 atoms of boron (known as a dopant) per million atoms of silicon can increase its electrical conductivity a thousandfold (partially accounting for the wide variability shown in the preceding figure).

periodic table
periodic table
Modern version of the periodic table of the elements.
The study of semiconductor materials began in the early 19th century. The elemental semiconductors are those composed of single species of atoms, such as silicon (Si), germanium (Ge), and tin (Sn) in column IV and selenium (Se) and tellurium (Te) in column VI of the periodic table. There are, however, numerous compound semiconductors, which are composed of two or more elements. Gallium arsenide (GaAs), for example, is a binary III-V compound, which is a combination of gallium (Ga) from column III and arsenic (As) from column V. Ternary compounds can be formed by elements from three different columns—for instance, mercury indium telluride (HgIn2Te4), a II-III-VI compound. They also can be formed by elements from two columns, such as aluminum gallium arsenide (AlxGa1 − xAs), which is a ternary III-V compound, where both Al and Ga are from column III and the subscript x is related to the composition of the two elements from 100 percent Al (x = 1) to 100 percent Ga (x = 0). Pure silicon is the most important material for integrated circuit applications, and III-V binary and ternary compounds are most significant for light emission.

Prior to the invention of the bipolar transistor in 1947, semiconductors were used only as two-terminal devices, such as rectifiers and photodiodes. During the early 1950s germanium was the major semiconductor material. However, it proved unsuitable for many applications, because devices made of the material exhibited high leakage currents at only moderately elevated temperatures. Since the early 1960s silicon has become by far the most widely used semiconductor, virtually supplanting germanium as a material for device fabrication. The main reasons for this are twofold: (1) silicon devices exhibit much lower leakage currents, and (2) silicon dioxide (SiO2), which is a high-quality insulator, is easy to incorporate as part of a silicon-based device. Thus, silicon technology has become very advanced and pervasive, with silicon devices constituting more than 95 percent of all semiconductor products sold worldwide.","Semiconductors are a class of crystalline solids that possess intermediate electrical conductivity, lying between conductors and insulators. They play a crucial role in electronic device manufacturing, including diodes, transistors, and integrated circuits, due to their compactness, reliability, energy efficiency, and cost-effectiveness. Semiconductors have diverse current and voltage capabilities and are suitable for integration into complex microelectronic circuits. They are essential components in electronic systems, serving communication, signal processing, computing, and control applications in both consumer and industrial sectors. Key semiconductor materials include elemental semiconductors like silicon and compound semiconductors such as gallium arsenide. Silicon, in particular, has become the dominant semiconductor material due to its low leakage currents and compatibility with silicon dioxide insulators, constituting over 95 percent of semiconductor products worldwide.





"
"Electronic properties
semiconductor bonds
semiconductor bonds
Three bond pictures of a semiconductor.
The semiconductor materials described here are single crystals; i.e., the atoms are arranged in a three-dimensional periodic fashion. Part A of the figure shows a simplified two-dimensional representation of an intrinsic (pure) silicon crystal that contains negligible impurities. Each silicon atom in the crystal is surrounded by four of its nearest neighbours. Each atom has four electrons in its outer orbit and shares these electrons with its four neighbours. Each shared electron pair constitutes a covalent bond. The force of attraction between the electrons and both nuclei holds the two atoms together. For isolated atoms (e.g., in a gas rather than a crystal), the electrons can have only discrete energy levels. However, when a large number of atoms are brought together to form a crystal, the interaction between the atoms causes the discrete energy levels to spread out into energy bands. When there is no thermal vibration (i.e., at low temperature), the electrons in an insulator or semiconductor crystal will completely fill a number of energy bands, leaving the rest of the energy bands empty. The highest filled band is called the valence band. The next band is the conduction band, which is separated from the valence band by an energy gap (much larger gaps in crystalline insulators than in semiconductors). This energy gap, also called a bandgap, is a region that designates energies that the electrons in the crystal cannot possess. Most of the important semiconductors have bandgaps in the range 0.25 to 2.5 electron volts (eV). The bandgap of silicon, for example, is 1.12 eV, and that of gallium arsenide is 1.42 eV. In contrast, the bandgap of diamond, a good crystalline insulator, is 5.5 eV.

Get a Britannica Premium subscription and gain access to exclusive content.
Subscribe Now
electron hole: movement
electron hole: movement
Movement of an electron hole in a crystal lattice.
At low temperatures the electrons in a semiconductor are bound in their respective bands in the crystal; consequently, they are not available for electrical conduction. At higher temperatures thermal vibration may break some of the covalent bonds to yield free electrons that can participate in current conduction. Once an electron moves away from a covalent bond, there is an electron vacancy associated with that bond. This vacancy may be filled by a neighbouring electron, which results in a shift of the vacancy location from one crystal site to another. This vacancy may be regarded as a fictitious particle, dubbed a “hole,” that carries a positive charge and moves in a direction opposite to that of an electron. When an electric field is applied to the semiconductor, both the free electrons (now residing in the conduction band) and the holes (left behind in the valence band) move through the crystal, producing an electric current. The electrical conductivity of a material depends on the number of free electrons and holes (charge carriers) per unit volume and on the rate at which these carriers move under the influence of an electric field. In an intrinsic semiconductor there exists an equal number of free electrons and holes. The electrons and holes, however, have different mobilities; that is, they move with different velocities in an electric field. For example, for intrinsic silicon at room temperature, the electron mobility is 1,500 square centimetres per volt-second (cm2/V·s)—i.e., an electron will move at a velocity of 1,500 centimetres per second under an electric field of one volt per centimetre—while the hole mobility is 500 cm2/V·s. The electron and hole mobilities in a particular semiconductor generally decrease with increasing temperature.

Electrical conduction in intrinsic semiconductors is quite poor at room temperature. To produce higher conduction, one can intentionally introduce impurities (typically to a concentration of one part per million host atoms). This is called doping, a process that increases conductivity despite some loss of mobility. For example, if a silicon atom is replaced by an atom with five outer electrons, such as arsenic (see part B of the figure), four of the electrons form covalent bonds with the four neighbouring silicon atoms. The fifth electron becomes a conduction electron that is donated to the conduction band. The silicon becomes an n-type semiconductor because of the addition of the electron. The arsenic atom is the donor. Similarly, part C of the figure shows that, if an atom with three outer electrons, such as boron, is substituted for a silicon atom, an additional electron is accepted to form four covalent bonds around the boron atom, and a positively charged hole is created in the valence band. This creates a p-type semiconductor, with the boron constituting an acceptor.

The p-n junction
p-n junction characteristics
p-n junction characteristics
(A) Current-voltage characteristics of a typical silicon p-n junction. (B) Forward-bias and (C) reverse-bias conditions. (D) The symbol for a p-n junction.
If an abrupt change in impurity type from acceptors (p-type) to donors (n-type) occurs within a single crystal structure, a p-n junction is formed (see parts B and C of the figure). On the p side, the holes constitute the dominant carriers and so are called majority carriers. A few thermally generated electrons will also exist in the p side; these are termed minority carriers. On the n side, the electrons are the majority carriers, while the holes are the minority carriers. Near the junction is a region having no free charge carriers. This region, called the depletion layer, behaves as an insulator.

The most important characteristic of p-n junctions is that they rectify. Part A of the figure shows the current-voltage characteristics of a typical silicon p-n junction. When a forward bias is applied to the p-n junction (i.e., a positive voltage applied to the p-side with respect to the n-side, as shown in part B of the figure), the majority charge carriers move across the junction so that a large current can flow. However, when a reverse bias is applied (as in part C of the figure), the charge carriers introduced by the impurities move in opposite directions away from the junction, and only a small leakage current flows. As the reverse bias is increased, the leakage current remains very small until a critical voltage is reached, at which point the current suddenly increases. This sudden increase in current is referred to as the junction breakdown, usually a nondestructive phenomenon if the resulting power dissipation is limited to a safe value. The applied forward voltage is typically less than one volt, but the reverse critical voltage, called the breakdown voltage, can vary from less than one volt to many thousands of volts, depending on the impurity concentration of the junction and other device parameters. ","Semiconductors are crystalline materials with electrical conductivity between conductors and insulators. In a semiconductor crystal, atoms form covalent bonds through shared electrons. The electrons' energy levels spread into energy bands due to atomic interactions in the crystal, with the highest filled band being the valence band and the next band, the conduction band, separated by a bandgap. At low temperatures, electrons are bound, but thermal energy can release them, creating free electrons and holes (electron vacancies). When an electric field is applied, both move through the crystal, producing an electric current. Doping with impurities enhances conductivity, with n-type semiconductors having excess electrons and p-type semiconductors having holes. P-n junctions, formed by abrupt changes in impurity type, rectify current flow, allowing it in one direction while blocking it in the other. Junction breakdown occurs under reverse bias.





"
"Electric field, an electric property associated with each point in space when charge is present in any form. The magnitude and direction of the electric field are expressed by the value of E, called electric field strength or electric field intensity or simply the electric field. Knowledge of the value of the electric field at a point, without any specific knowledge of what produced the field, is all that is needed to determine what will happen to electric charges close to that particular point.

Instead of considering the electric force as a direct interaction of two electric charges at a distance from each other, one charge is considered the source of an electric field that extends outward into the surrounding space, and the force exerted on a second charge in this space is considered as a direct interaction between the electric field and the second charge. The strength of an electric field E at any point may be defined as the electric, or Coulomb, force F exerted per unit positive electric charge q at that point, or simply E = F/q. If the second, or test, charge is twice as great, the resultant force is doubled; but their quotient, the measure of the electric field E, remains the same at any given point. The strength of the electric field depends on the source charge, not on the test charge. Strictly speaking, the introduction of a small test charge, which itself has an electric field, slightly modifies the existing field. The electric field may be thought of as the force per unit positive charge that would be exerted before the field is disturbed by the presence of the test charge.

Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
Britannica Quiz
Physics and Natural Law
The direction of the force that is exerted on a negative charge is opposite that which is exerted on a positive charge. Because an electric field has both magnitude and direction, the direction of the force on a positive charge is chosen arbitrarily as the direction of the electric field. Because positive charges repel each other, the electric field around an isolated positive charge is oriented radially outward. When they are represented by lines of force, or field lines, electric fields are depicted as starting on positive charges and terminating on negative charges. A line tangent to a field line indicates the direction of the electric field at that point. Where the field lines are close together, the electric field is stronger than where they are farther apart. The magnitude of the electric field around an electric charge, considered as source of the electric field, depends on how the charge is distributed in space. For a charge concentrated nearly at a point, the electric field is directly proportional to the amount of charge; it is inversely proportional to the square of the distance radially away from the centre of the source charge and depends also upon the nature of the medium. The presence of a material medium always diminishes the electric field below the value it has in a vacuum.

At times the electric field itself may become detached from the source charge and form closed loops, as in the case of charges accelerating up and down the transmitting antenna of a television station. The electric field with an accompanying magnetic field is propagated through space as a radiated wave at the same speed as that of light. Such electromagnetic waves indicate that electric fields are generated not only from electric charges but also from changing magnetic fields.

The value of the electric field has dimensions of force per unit charge. In the metre-kilogram-second and SI systems, the appropriate units are newtons per coulomb, equivalent to volts per metre. In the centimetre-gram-second system, the electric field is expressed in units of dynes per electrostatic unit (esu), equivalent to statvolts per centimetre.

The Editors of Encyclopaedia Britannica
This article was most recently revised and updated by Adam Augustyn.
quantum field theory
Table of Contents
Introduction
References & Edit History
Related Topics
Images & Videos
Feynman diagram
Learn about antimatter and its properties, and understand the annihilation of matter and antimatter
Understand matter based on the Pauli exclusion principle
Quizzes
Italian-born physicist Dr. Enrico Fermi draws a diagram at a blackboard with mathematical equations. circa 1950.
Physics and Natural Law
Italian physicist Guglielmo Marconi at work in the wireless room of his yacht Electra, c. 1920.
All About Physics Quiz
Discover
Wasp spider. Argiope bruennichi. Orb-weaver spider. Spiders. Arachnid. Cobweb. Spider web. Spider's web. Spider silk. Black and yellow striped wasp spider spinning a web.
Do We Really Swallow Spiders in Our Sleep?
Orange basketball on black background and with low key lighting. Homepage 2010, arts and entertainment, history and society
The 10 Greatest Basketball Players of All Time
Two domestic cats lying down with each other. Feline mammal snuggle whiskers
Do Cats Cause Schizophrenia?
Iraqi Army Soldiers from the 9th Mechanized Division learning to operate and maintain M1A1 Abrams Main Battle Tanks at Besmaya Combat Training Center, Baghdad, Iraq, 2011. Military training. Iraq war. U.S. Army
8 Deadliest Wars of the 21st Century
illustration of the walking titanosaurus, Patagotitan mayorum
Titanosaurs: 8 of the World's Biggest Dinosaurs
Santa Claus flying in his sleigh, christmas, reindeer
Was Santa Claus a Real Person?
Four year old boy with grandfather and father lighting Hanukkah menorah. Photo taken on: December 21st, 2009
Which Is Correct: Hanukkah or Chanukah?
Home
Science
Physics
Matter & Energy
Science & Tech
quantum field theory
physics
    
Written and fact-checked by 
Last Updated: Nov 1, 2023 • Article History
Feynman diagram
Feynman diagram
See all media
Category: Science & Tech
Key People: Oswald Veblen Vladimir Aleksandrovich Fock
Related Topics: unified field theory quantum electrodynamics supergravity gauge theory Casimir effect
Quantum field theory, body of physical principles combining the elements of quantum mechanics with those of relativity to explain the behaviour of subatomic particles and their interactions via a variety of force fields. Two examples of modern quantum field theories are quantum electrodynamics, describing the interaction of electrically charged particles and the electromagnetic force, and quantum chromodynamics, representing the interactions of quarks and the strong force. Designed to account for particle-physics phenomena such as high-energy collisions in which subatomic particles may be created or destroyed, quantum field theories have also found applications in other branches of physics.

The prototype of quantum field theories is quantum electrodynamics (QED), which provides a comprehensive mathematical framework for predicting and understanding the effects of electromagnetism on electrically charged matter at all energy levels. Electric and magnetic forces are regarded as arising from the emission and absorption of exchange particles called photons. These can be represented as disturbances of electromagnetic fields, much as ripples on a lake are disturbances of the water. Under suitable conditions, photons may become entirely free of charged particles; they are then detectable as light and as other forms of electromagnetic radiation. Similarly, particles such as electrons are themselves regarded as disturbances of their own quantized fields. Numerical predictions based on QED agree with experimental data to within one part in 10 million in some cases","Electric fields are fundamental to understanding the behavior of charged particles in physics. They represent the force experienced by a positive charge at a given point in space and are characterized by their magnitude and direction, denoted as electric field strength (E). The electric field arises from the presence of electric charges, and its strength is determined by the electric force (F) acting on a positive test charge (q). Electric fields play a crucial role in explaining the behavior of charges in various situations and are integral to understanding the principles of electromagnetism. The electric field is a vector quantity, and its direction is chosen as the direction of the force on a positive charge. It forms closed loops around electric charges, starting from positive charges and terminating on negative charges. The strength of the electric field depends on the distribution of charges and is inversely proportional to the square of the distance from the charge. Electric fields are a fundamental concept in physics, essential for describing the behavior of charged particles and the forces acting on them.





"
"general relativity
physics
    
Also known as: general theory of relativity
Written and fact-checked by 
Last Updated: Dec 10, 2023 • Article History
Category: Science & Tech
Key People: Albert Einstein Arthur Eddington Robert H. Dicke Karl Schwarzschild Vladimir Aleksandrovich Fock
Related Topics: relativity gravitational wave black hole field
Recent News
Dec. 9, 2023, 1:02 AM ET (Yahoo News)
All black holes feast chaotically, no matter how hungry they are
Dec. 5, 2023, 11:35 PM ET (MSN)
Gravitational waves rippling from black hole merger could ...
View a demonstration to understand Albert Einstein's general theory of relativity
View a demonstration to understand Albert Einstein's general theory of relativity
Explore general relativity.See all videos for this article
General relativity, part of the wide-ranging physical theory of relativity formed by the German-born physicist Albert Einstein. It was conceived by Einstein in 1916. General relativity is concerned with gravity, one of the fundamental forces in the universe. Gravity defines macroscopic behaviour, and so general relativity describes large-scale physical phenomena.

General relativity follows from Einstein’s principle of equivalence: on a local scale it is impossible to distinguish between physical effects due to gravity and those due to acceleration. Gravity is treated as a geometric phenomenon that arises from the curvature of space-time. The solution of the field equations that describe general relativity can yield answers to different physical situations, such as planetary dynamics, the birth and death of stars, black holes, and the evolution of the universe. General relativity has been experimentally verified by observations of gravitational lenses, the orbit of the planet Mercury, the dilation of time in Earth’s gravitational field, and gravitational waves from merging black holes. (For a more detailed treatment of general relativity, see relativity: General relativity.)","General relativity is a fundamental physical theory developed by Albert Einstein in 1916, addressing the concept of gravity, one of the fundamental forces in the universe. It is based on Einstein's principle of equivalence, which states that locally, there is no distinction between the effects of gravity and acceleration. In general relativity, gravity is viewed as a result of the curvature of space-time. This theory has been experimentally verified through observations of phenomena such as gravitational lenses, the orbit of Mercury, time dilation in Earth's gravitational field, and gravitational waves from black hole mergers. General relativity provides insights into planetary dynamics, star life cycles, black holes, and the evolution of the universe, describing large-scale physical phenomena.





"
"Feeding black holes lurking at the center of most galaxies gulp nearby stars, gas and dust the same way irrespective of how hungry they are, new research suggests.

Until now, there appeared to be an order to these massacres. The hungriest of black holes, which also beam out very powerful radiation, were thought to ""eat"" one star about the size of our sun every year. Astronomers think matter collapses into disks around these very hungry cosmic beasts, which are then fed in a somewhat organized way. By contrast, less hungry black holes were thought to take something like 10 million years to consume a sun-sized star and are thought to be surrounded by chaotic streams of matter rather than neat disks.

However, astronomers now say both systems are more similar than currently appreciated.

Related: James Webb Space Telescope spies a newborn star in its cosmic crib

The chaotic process typically associated only with the latter systems, the less bright black holes, might in fact play an important role in the way the brightest black holes feed, study lead author Ilaria Ruffa of Cardiff University told Space.com in an email.

""This result was totally unexpected and can thus completely change our understanding of the physical processes through which different types of active black holes 'eat' the surrounding material,"" she said. ""This is really puzzling and exciting at the same time.""

To arrive at their conclusions, Ruffa and her colleagues studied 136 black holes, millions of times more massive than our sun, that sit billions of light-years away from us. This included voids sitting in about 30 nearby galaxies studied by the powerful ALMA telescopes in Chile. The team found that light detected from all feeding black holes, especially in the microwave radiation region, is actually coming from disordered streams of matter.

This is ""changing our view on how these systems consume matter, and grow to be the cosmic monsters we see today,"" Ruffa said in a statement.

Related Stories:

— James Webb Space Telescope finds water and methane in atmosphere of a 'warm Jupiter'

— James Webb Space Telescope pierces through dust to find an ancient ghostly galaxy

— James Webb Space Telescope gazes into a galactic garden of budding stars (image)

It also appears, the team says, that the matter tightly bound around the black holes shone the same way in both microwave and X-ray wavelengths, hinting that for highly luminous black holes, the observed glow is ""incompatible with an ordered flow of matter,"" said Ruffa.

Studying this light may also offer a new indirect method to estimate black hole masses, a crucial parameter to understand how these beasts, colossal in their own right but ""tiny"" when compared to an entire galaxy, ""manage to affect — sometimes, in a dramatic way — the life of the host galaxy itself,"" said Ruffa.

This research is described in a paper published Dec. 5 (Monday) in the journal Monthly Notices of the Royal Astronomical Society.","New research suggests that feeding black holes at the center of galaxies, regardless of their hunger, consume nearby stars, gas, and dust in a more similar manner than previously thought. While it was believed that hungrier black holes devoured stars in an organized way, and less hungry ones did so chaotically, astronomers now suggest that chaotic processes play a crucial role in how even the brightest black holes feed. The study, which examined 136 massive black holes using telescopes like ALMA, found that light from all feeding black holes, including the brightest ones, comes from disordered streams of matter. This discovery could reshape our understanding of how black holes consume surrounding material and their impact on galaxies.





"
"What Is the Difference Between Influenza and COVID-19?
Written by 
Fact-checked by 
Novel Coronavirus SARS-CoV-2 - Colorized scanning electron micrograph of an apoptotic cell (green) heavily infected with SARS-COV-2 virus particles (purple), isolated from a patient sample. Image captured and color-enhanced at the NIAID Integrated...
Courtesy, NIAID
Influenza and the coronavirus disease COVID-19 appear to be very similar. After all, both are respiratory diseases, and they are transmitted via contact with infectious respiratory droplets. Beyond that, however, they differ in important ways. What are some of these differences, and why do they matter?

Contagiousness
One difference between COVID-19 and influenza is that the former appears to be more contagious than seasonal influenza. A person infected with influenza spreads the disease to another 1.3 individuals. For COVID-19, an infected person spreads illness to another 2 to 2.5 persons.

Severity of illness and death rate
COVID-19 also appears to cause more severe illness more frequently than seasonal influenza. Part of this difference may be attributed to the fact that COVID-19 is caused by a new type of coronavirus, against which humans have no immunity. By contrast, many people have at least some degree of immunity against seasonal influenza, enough to prevent hospitalization and complications in most instances.

Along those same lines, COVID-19 is deadlier than influenza. The mortality rate of influenza is roughly 0.1 percent. Meanwhile, the case fatality rate for COVID-19 is estimated to be about 1.4-4.5 percent, with risk of death being significantly higher for older persons than for younger individuals.

Seasonal nature
Seasonal influenza, as its name suggests, tends to come and go as the weather changes. Influenza viruses circulate year-round, but the number of new influenza cases generally increases in the cooler months and tapers off in the warmer months of the year. This does not mean that cold weather causes the flu; rather, cool weather, by bringing people indoors, along with other changes, is a contributing factor.

It is possible that COVID-19 could turn out to be a seasonal illness, similar to influenza. However, experts warn that, at least for now, amid the ongoing pandemic, warmer weather is unlikely to drive the disease away. COVID-19 is a new disease, and there are many millions of people worldwide who have not yet been exposed. This deep reserve of potential hosts could fuel sustained transmission through summer in the Northern Hemisphere and winter in the Southern Hemisphere. Many factors, however, determine seasonality of diseases, and more time is needed before conclusions can be drawn about whether COVID-19 is a seasonal illness.

Causative agent
A more obvious difference between influenza and COVID-19 is in their causative agents. Influenza viruses belong to a virus family known as Orthomyxoviridae. COVID-19 is caused by a coronavirus named SARS-CoV-2, which is classified in the family Coronaviridae. Both families consist of RNA viruses, but they differ particularly with regard to the protein layer that encapsulates the RNA.

More specifically, influenza viruses express two surface antigens (foreign proteins)—hemagglutinin (H) and neuraminidase (N)—which trigger an immune response. The exact form of these antigens changes every now and then, resulting in the periodic emergence of new, more virulent influenza viruses with the potential to cause a pandemic. The surface of SARS-CoV-2 does not have these antigens. Rather, similar to other types of coronaviruses, its outer surface is studded with glycoprotein spikes, which give such viruses a crownlike, or coronal, appearance. Spike glycoproteins are responsible for triggering the immune response, and they carry out the critical function of enabling the coronavirus particle to enter cells, where it then replicates. ","Influenza and COVID-19, while both respiratory diseases transmitted through respiratory droplets, differ in several crucial ways. COVID-19 is more contagious, with an infected person spreading the disease to 2 to 2.5 individuals compared to 1.3 for influenza. COVID-19 causes more severe illness and has a higher mortality rate, estimated at 1.4-4.5%, compared to influenza's 0.1%. Influenza has a seasonal pattern, but it's unclear if COVID-19 will follow the same pattern. Additionally, they are caused by different types of viruses; influenza by Orthomyxoviridae and COVID-19 by SARS-CoV-2, a coronavirus with spike glycoproteins on its surface.





"
"Collar recognition and matching of clothing style drawings based on complex networks
Author links open overlay panelLinlin Wei
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.jrras.2023.100687
Get rights and content
Under a Creative Commons license
open access
Abstract
Seeking a fast and effective algorithm for describing and classifying clothing styles is currently a hot topic in the textile and clothing industry. The study first constructs a sample library of collar styles, preprocesses clothing images, and uses complex network models to describe and extract features; Implement clothing image classification based on support vector machines, and finally use image matching algorithms to match the image to be matched with the target object. It was indicated that the complex network model established by the research institute can effectively depict the characteristics of various collar styles, and the complex network model can effectively depict the characteristics of various collar styles. In 10 experiments, the overall average overall accuracy was 98%; Regarding different types of collars, the average accuracy of 10 experiments is above 96%, indicating that the collar classification accuracy based on support vector machine method is very high. The classifying results of the 10 categories obtained by the research method have little change and are relatively stable. For each category, the research method is higher than the other two methods. The results can indicate that the research method has effect and is more suitable for collar recognition and classification. Compared with traditional Hu invariant moments and HOG feature extraction algorithms, the research algorithm has stronger noise suppression ability, and using Euclidean distance as a similarity measure has great advantages.

Previous article in issueNext article in issue
Keywords
Complex networksStyleSVMClothing imagesFeature extraction
1. Introduction
Traditional Clothing Style (CS) mostly relies on professional plate makers. Due to its own limitations, its production process is time-consuming, labor-intensive, and expensive, and the final result also depends on the professional level and experience of the plate making personnel, with strong subjectivity (Luo. et al., 2020). In addition, due to the change in people's attitudes towards clothing consumption, clothing consumption is gradually developing towards small batches and multiple varieties. How to improve rapid response capability in a short period of time has become an important means for clothing enterprises to improve their competitiveness. Rapid response has become an urgent research topic (Liu. et al., 2019). A complex network is a network model that describes the geometric characteristics of objects by many nodes and their connections, as well as their continuous connections. Complex networks not only exhibit small world characteristics, but also exhibit high clustering characteristics (Ling. et al., 2019). The small world property refers to the more connections between two nodes, the more paths there are between them. The main contribution and innovation of the research are based on complex networks to achieve recognition and matching of clothing style and collar patterns, thereby enhancing the competitiveness of enterprises. The article mainly contains five parts. Introduction is part 1, which mainly introduces the research background and purpose. Literature review is part 2, mainly summarizing the current research status of different scholars at home and abroad on clothing recognition. Research method is part 3, including clothing image preprocessing, image feature extraction based on complex networks, clothing image classification methods, and image matching of CS and collar patterns. Result analysis is part 4, including the feature description analysis of the target component, the SVM classification effect analysis, the performance analysis of feature extraction and image matching algorithm. The fifth part is the conclusion.

2. Related works
With multimedia and communication developing, people have entered the information age. In recent years, images have been increasingly applied in fields such as medicine, aviation, aerospace, and communication. On this basis, it is gradually applied to the testing of textiles to improve the scientific and objective nature of the test results. While conducting in-depth research on image processing technology, some researchers have also applied it to industries such as clothing.

To improve CS recognition accuracy, scholars such as Li et al. proposed a multi depth feature fusion method to recognize clothing image style. This algorithm can improve object detection method and utilize it to extract features from all regions of the clothing that make up the image. Overall features of clothing image are fused to obtain multi category fusion features. The research results indicate that the proposed algorithm can effectively avoid the influence of interference factors and improve the accuracy of CS recognition (Li. et al., 2021). Researchers such as Zhou et al. have established an optimized clothing classification method to address the issue of insufficient accuracy in current clothing image recognition. This method includes parallel convolutional neural network (CNN) and optimization of random vector function chains. To improve the stability and accuracy of classification, two layers are used to reduce computational complexity, and the Grasshopper optimization algorithm is used to classify features. The research results indicate that the research method can make clothing image recognition accuracy improved, with a recognition accuracy of 92.95% on Fashion-Mnist data (Zhou. et al., 2022). Lee et al. proposed two branch feature selection network for category classification and attribute prediction to improve the efficiency of landmark information in clothing recognition. The research results indicate that unlabeled clothing recognition based on two branch feature selection network can effectively learn the differential row feature representation of clothing images, and the proposed network can produce better performance (Lee. et al., 2019). Halstead et al. proposed an improved clothing texture recognition method that integrates support vector machines to improve the accuracy of supervised search for clothing recognition. This method constructs a search framework and designs clothing texture and color information for the torso and leg regions. Through monitoring the subjects, the results indicate that this method can achieve efficient clothing recognition results under supervision (Halstead. et al., 2019). To improve the effectiveness of visual feature extraction in clothing compatibility prediction, scholars such as Lu et al. proposed a fusion framework that combines multi-layer non local features. This feature fusion model can combine high-level and low-level features and detect global features. The test results indicate that the proposed method has a high level of compatibility prediction in dataset detection (Lu. et al., 2021).","The study focuses on developing a fast and effective algorithm for classifying clothing styles, a critical topic in the textile and clothing industry. It utilizes complex network models to describe and extract features from a sample library of collar styles, followed by clothing image classification with support vector machines and image matching algorithms. The research demonstrates that the complex network model effectively captures collar style characteristics, achieving an average accuracy of 98%. The classification accuracy for different collar types is above 96%, outperforming traditional methods. The study highlights the algorithm's noise suppression ability and its suitability for collar recognition and classification, offering potential benefits to the clothing industry.





"
"Computerized simulation techniques have been employed to develop a model for maintenance problems in the published literature (Sheikhalishahi, 2014). Simulation models have the potential capability to establish performance goals with respect to the available supply of production system and the stochastic demand as major considerations (Dalziell and McManus, 2004). Yang et al. (2008) outlined a novel approach to a scheduled maintenance in a manufacturing process. They continuously evaluated the performance of production facilities, as well as the complicated relationship between production system and maintenance practices. A discrete-event simulation was conducted to assess the effect of each maintenance schedule based on the estimated probabilities for equipment failures in production system. The findings indicated significant potential for improving the cost-effectiveness of maintenance schedules by incorporating forecasts of machine performance. Laggoune et al. (2009) took an approach to preventive maintenance in a multi-component system where failures occurred in a random pattern and cost reduction was made in the course of a whole lifetime. Monte Carlo simulation with an informative search was used to make a performance improvement in replacing the pieces of machinery. Multi-objective problems have recently attracted a considerable interest (Berrichi et al., 2009, Ok et al., 2013). According to Peters and Madlener (2017) maintenance optimization proposes a solution for the trade-off between maintenance requirements and the resources needed. Wang and Tsai (2014) established a bi-objective imperfect preventive maintenance model of a series–parallel system. They simultaneously improved mean system reliability and total maintenance cost by determining the most appropriate manner in maintenance. Sheikhalishahi et al. (2014) arrived at an optimal solution by considering cost and reliability as two major objectives.

Cost and reliability are of particular importance in industries with high-risk strategies, such as the gas industry. Pipelines constitute an item of equipment carrying this useful but dangerous energy in the gas industry. They are used as a suitable alternative for transporting a large amount of combustible materials, specifically natural gas (NG) (Batzias et al., 2011). Moreover, the increasing level of NG consumption has expended pipeline networks around the world (Ramírez-Camacho et al., 2017, Vianello and Maschio, 2014). Since natural gas transportation systems (NGTS), especially city gate stations (CGSs) (which refers to a gas measurement and pressure control system that is typically located outside the city boundaries along a gas pipeline) are in proximity to urban regions, any intentional or accidental damage to the system can lead to disastrous consequences. Thus, public safety has a great dependence on safe practices in NGTS (Batzias et al., 2011). High flammability, explosibility, and dispersion as the important properties of NG have made incidents experienced in the NGTS different from other industrial accidents (Han and Weng, 2011). An explosion with more than 200 injuries and 14 fatalities in a NG factory of Belgium in 2004, and an explosion caused by leaking NG Moscow (Han and Weng, 2011) are some examples. It necessitates the risk assessment of NGTS equipment one of the most important which is CGS. There is at least one CGS in the entrance of any city. Decreasing gas pressure from 500 to 1400 psig to 0.25–300 psig by regulators, determining the flow rate of gas, and adding odorant to the gas for safety considerations are several particular instances of CGS operations. CGS takes an influential role in a safe short NG supply route to industrial, residential, and commercial customers (Zarei et al., 2017). Given the fact that NG is highly explosive and flammable, a gas leak from the station equipment could have catastrophic impacts on the nature and people (Mohammadfam and Zarei, 2015, Russo and Parisi, 2016). It poses serious dangers on stations situated adjacent to heavily populated regions. Incidents caused by CGS are perceived as a threat to safety improvements in urban life and they are a controversial matter related to the public (Wang and Duncan, 2014, Russo and Parisi, 2016). Thus, the risk assessment of CGS operations is a matter of great importance to create a safe environment for human life (Han and Weng, 2011, Saffari et al., 2015, Saffari et al., 2023).

This study presents a fresh idea for optimizing the performance of CGS maintenance department (The unit that is responsible for the maintenance and repairs of CGS) with respect to cost and reliability. As the first step, information concerning maintenance plans and unexpected failures is retrieved from the maintenance software after a basic familiarity with how the CGS system operates (works). A simulation of the station maintenance is provided by the obtained information. Different scenarios were created by Taguchi orthogonal array method using Minitab software, and cost and reliability based on customer dissatisfaction rate (modified- reliability) were measured for each scenario and the results then were analyzed through Taguchi method. In fact, the modified- reliability is the reliability considering the customer dissatisfaction rate. The association of the maintenance work load per component with cost and modified- reliability was examined by regression method. After that, a mathematical model was constructed to reveal the best maintenance plan with the aim of cost reduction and reliability increase, and an assessment was conducted based on the comparison of all results. Finally, after identifying the most significant components in terms of cost and modified- reliability by sensitive analysis, an expert system is designed to determine the maintenance actions during the maintenance plan. It is worth mentioning that the present study has made a significant contribution to the literature by a mathematical model for optimizing the performance of CGS maintenance department. An integrated Simulation- Mathematical programming- Expert system approach to the performance improvement of CGS maintenance department with respect to cost and reliability has made this study distinct from other research studies. In fact, this study stands out from other research because it focuses on evaluating the program and maintenance operations of CGS in terms of cost and reliability (determined by customer's dissatisfaction rates). This assessment is achieved through the use of the integrated approach. Additionally, the study explores human errors alongside equipment errors. It's important to highlight that the integrated model we've introduced can be applied to maintenance systems in various contexts, particularly in high-risk sectors like the case study, where errors can result in significant human and financial implications. In essence, the rationale for taking into account the customer dissatisfaction rate remains consistent and it can be evaluated in the reliability of other complex systems. It can be highly effective in helping managers to decrease the rate of incidents caused by weaknesses in maintenance plans through identifying the influential parts of the system and improving the procedure by signaling the maintenance experts to pay more attention to these parts. ","This study focuses on optimizing the performance of city gate station (CGS) maintenance departments in the gas industry, considering cost and reliability as key factors. The research employs an integrated approach involving information retrieval, simulation, Taguchi orthogonal array method, mathematical modeling, and expert systems. It aims to reduce costs and increase reliability in CGS maintenance operations by evaluating different maintenance scenarios. By considering customer dissatisfaction rates as modified reliability, the study offers a unique perspective on optimizing maintenance plans and highlights the importance of addressing both equipment and human errors in high-risk sectors like the gas industry, ultimately enhancing safety and efficiency.





"
"An experimental investigation on using heat pipe heat exchanger to improve energy performance in gas city gate station
Author links open overlay panelAraz Alizadeh a, Hossein Ghadamian a, Mohammad Aminy a, Siamak Hoseinzadeh b, Hamed Khodayar Sahebi a, Ali Sohani c
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.energy.2022.123959
Get rights and content
Abstract
Natural gas pressure drops in city gate stations, in the natural gas distribution network, and bypassing through throttle valves. On the other hand, sudden gas pressure drop causes extreme temperature drop. To prevent natural gas from being hydrated in city gate stations, a water bath heater is used, leading to heating natural gas before the pressure reduction stage.

The main consequence of this research is the consideration of the challenge of low thermal efficiency in water bath heaters and the high potential of flue gases, which leads to the preheating of the gas by heat loss recovery and consequently improving the energy performance of the whole system.

In this study, a heat pipe heat exchanger was laboratory designed and constructed by using the ε-NTU method. The heat exchanger performs to recover partly the waste energy of flue gas emitted to the environment through heat pipes. This has been investigated in several experiments, based on conducted studies, measured data from a city gate station during a year, and also corresponding the experimental results with real conditions. In addition, to determine energy saving in fuel consumption, the city gate stations process was simulated in HYSYS software and evaluated according to gas temperature in new conditions.

As major findings applying heat pipes reduces natural gas consumption by 510,132 SCM a year; it also annually prevents 756 tons of CO2 from being emitted off a city gate station.","This experimental study focuses on improving energy performance in gas city gate stations, where natural gas pressure drops and causes extreme temperature drops. To address this issue and enhance thermal efficiency, the research introduces a heat pipe heat exchanger designed using the ε-NTU method. The heat exchanger recovers waste energy from flue gases and preheats the gas, resulting in significant energy savings. The experiments demonstrate a reduction of 510,132 SCM in natural gas consumption annually and the prevention of 756 tons of CO2 emissions from city gate stations, highlighting the potential for energy efficiency improvements in gas distribution networks.





"
"Thermodynamic modeling and analysis of a novel heat recovery system in a natural gas city gate station
Author links open overlay panelShoaib Khanmohammadi a, Morteza Saadat-Targhi b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.jclepro.2019.03.167
Get rights and content
Abstract
In the present study, five configurations of integrated system are introduced and comprehensive thermodynamic modeling and analysis for different arrangements are carried out. An organic Rankine flash cycle (ORFC) and a thermoelectric generator (TEG) are integrated with a real city gate station (CGS) to recover the thermal energy of exhaust gas at 400 
 in an indirect water bath heater (IWBH). The new aspect of the present research is proposal the novel arrangements of the TEG modules, ORFC and IWBH to improve the thermal performance of the CGS integrated system.

Energy and exergy analyses employed to evaluate different arrangements of integrated systems.

Comparative analysis of different suggested systems revealed that integrated system including IWBH, ORFC and TEG modules with placing the TEG modules after ORFC (Case V) shows better performance. Due to the better performance, Case V is considered for more studies. Under the given condition for the selected case, in a complete year, results indicate that the maximum and minimum values of electrical output power for TEG modules is 132 W and 100 W and for ORFC is 11.5 kW and 2.1 kW. Also, the parametric study of the Case V inferred that with increasing the amount of excess air form 50%–200%, the fuel mass flow rate increased by 35.6% which ultimately leads to a reduction in thermal efficiency of the system. The results suggests that proper arrangement of TEG modules and ORFC in the IWBH can improve the total thermal efficiency as well as the total electrical output power. Analyses of different proposed arrangements, also indicates that with the use ORFC and TEG modules in the worst and best cases 2.284 kW and 5.904 kW electrical energy can be recovered from the IWBH in the CGS.

Graphical abstract
Image 1
Download : Download high-res image (222KB)
Download : Download full-size image

Introduction
Environment deterioration and energy crisis are two major problems for the 21st century. Many types of research have been conducted to abate the adverse effects of the wastes of human activities (Hanifzadeh et al., 2017). Different methods employed to remove detrimental material from waste water (Sepehri and Sarrafzadeh, 2018). On the other side, innovative methods to recover waste energy and make optimal use of available energy resources can help to mitigate the negative effects of environmental issues. Different ways suggested by researchers to recover waste energy of industrial processes. The main point related to this issue is the smart combination of various systems to achieve maximum performance in such cases.

Among different ways to transmit the natural gas (NG) from production area to consumers’ area, the high-pressure transmission pipeline is widely used. The pressure of the NG transmission pipelines is much higher than the pressure demanded by consumers; hence, near the cities this pressure reduces to a desirable value. According to the Joule-Thompson coefficient of NG in the pressure reduction process the temperature of the NG will drop. To prevent the hydrate formation and unfavorable icing effects an inline NG heater warms up the NG stream. The combustion products of inline NG heater leave the stack at a high temperature of about 381 
–476 
 (Ebrahim et al., 2011). There are numerous studies investigated inline NG heater in CGSs. To better study the literature the reported researches categorized in four parts. A part of studies has been considered the use of renewable energy combined with inline NG heater. In a research Farzaneh-Gord et al. (2012) studied a solar assisted CGS to assess the thermal/economical behavior of the system. Their system includes an array of 450 collector modules and a storage thank with 45 
 capacity. As it is reported, this system can save 99260 standard 
 of natural gas that is 11.3% of fuel consumption and their economic analysis showed that the return time of investment is 6.9 years.

In another study, Farzaneh-Gord et al. (2016) proposed a vertical ground source heat pump to integrate with a conventional CGS. The proposed system implemented for two CGSs in different climatic conditions of Iran to eliminate the fuel consumption for preheating the NG stream. Considering indirect fuel consumption of electrical heat pumps revealed that the potential for energy saving is 65%. Assessment of the natural expansion plant integrated with a vertical ground-coupled heat pump investigated by Ghezelbash R. et al. (Ghezelbash et al., 2015). In the proposed system geothermal energy as a renewable source employed for providing part of NG heating demand. They concluded that the fuel saving potential of the system is 45% annually and economically the discounted payback period is about 6 years. There is numerous research in the field of employment the renewable energy source in the CGSs. Arabkoohsar A. et al. (Arabkoohsar et al., 2016) investigated a combination of a grid connected PV plant with a compressed air energy storage system (CAES) and a CGS. They found that 7000 evacuated tube collectors are required to eliminate air heater from the CAES system and 17.2% fuel saving can be achieved in the CGS.

Part of the researches has been addressed energy recovery from pressure reduction procedure to produce mechanical work to generate electrical power. To achieve this objective, instead of the pressure reduction valve, the pressure of the NG stream reduces via a turbo-expander which along with output mechanical work. Cascio E. L. et al. (Cascio et al., 2017) studied an urban integrated electrical, thermal and gas grids. The main system include a retrofitted natural gas pressure reduction station where a turboexpander allows to recover energy from the process. Kostowski W. J. and Uson S. (Kostowski and Usón, 2013). investigated the thermoeconomic assessment of an expansion system in a CGS. Their proposed system includes two turboexpander stages reducing the natural gas pressure and generate mechanical work. The hybrid energy generation unit derives partially with exergy of pressurized natural gas and partially from the primary fuel energy. The exergy efficiency as a suitable indicator used to assess the system which was 49.2% for their case study and reported as a good value. Yaxuan X. et al. (Xiong et al., 2018) proposed an expander-depending natural gas pressure regulation configuration which regulates the pressure of natural gas and harvests the pressure of natural gas. Their thermodynamic analysis revealed that the daily round-tip efficiency of the combined system could be more than 25%. Kanmohammadi S. et al. (Khanmohammadi et al., 2015) and Nesli et al. (Neseli et al., 2015) conducted studies on the energy recovery from pressure reduction procedure in CGSs. The other part of researches focus on improvement the heat transfer characteristics of heat transfer medium (ethylene glycol/water) in this type of heater or tries using turbulators as a passive method for heat transfer enhancement (Rahmati and Reiszadeh, 2018).

A review of the literature shows that a large part of the energy in the CGSs dissipates in the exhaust via combustion flue gas which depletes to the environment (Olfati et al., 2018). There are different reports that present innovative ways to recover this type of waste energy. Sanaye and Mohammadi Nasab (Sanaye and Nasab, 2012) investigated the modeling and optimization of a CHP system for a NG city gate station. A new and fast procedure for the size and number of equipment selection is introduced. They introduce a parameter named actual annual benefit (ABB) which considered both thermal and economical aspects of the studied system for optimization. Moreover, the payback period of the system in the optimum state was estimated 1.23 years. Gheibi et al. (Ghaebi et al., 2018) carried out energy, exergy, economic and environment (4E) analysis of a system derived with CGS heater to produce power and hydrogen. The combined system includes a Rankine cycle (RC), an absorption power cycle (APC), and a proton exchange membrane (PEM). The thermal efficiency of combined CGS/PEM-RC and -APC was calculated 32.93% and 33.63%, respectively.   ","This study presents comprehensive thermodynamic modeling and analysis of a novel heat recovery system integrated into a natural gas city gate station (CGS). The system incorporates an organic Rankine flash cycle (ORFC) and a thermoelectric generator (TEG) to recover thermal energy from exhaust gas in an indirect water bath heater (IWBH). Multiple configurations are explored, and the results highlight that a particular arrangement (Case V) with TEG modules placed after ORFC demonstrates the best performance. The system achieves significant electrical power output and energy savings, making it a promising approach for improving the thermal efficiency of CGS integrated systems and reducing environmental impact.





"
"Design and analysis of a novel self-refrigerated natural gas liquefaction system integrated with helium recovery and CO2 liquefaction processes
Author links open overlay panelAli Palizdar a, Ali Vatani a b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.jclepro.2023.138600
Get rights and content
Abstract
Utilizing the high-pressure exergy of a pipeline gas is a route to overcome the challenge of high power consumption in liquefying natural gas. Through the expansion in city gate stations (CGSs), a notable cold duty is achieved that can liquefy a part of a natural gas feed. Based on this concept, the present research proposes a novel system for natural gas liquefaction utilizing pipeline exergy integrated with helium recovery and innovative carbon dioxide capturing and liquefaction. The process was evaluated by energy, exergy, economic, and exergoeconomic analyses. Results showed that the proposed system can co-produce LNG, liquid helium, and liquid CO2 products with low energy consumption, simplicity, and high profitability. The specific power consumption (SPC) and the exergy efficiency of the natural gas liquefaction system are 0.227 kWh/kgLNG and 64.24%, respectively, which shows a desirable performance as well as a notable potential for improvement. The process equipment of the helium liquefaction system also offers a higher total cost rate, mostly constituting an exergy destruction cost rate. The integrated system exhibits appropriate profitability by the internal rate of return of 47% and the payback period of 3.25 years.

Graphical abstract
A conceptual illustration of the integrated LNG/LHe/LCO2 process.
Image 1
Download : Download high-res image (245KB)
Download : Download full-size image

Introduction
Many outlook reports affirm that the demand for liquefied natural gas (LNG) will exceed double the current value until 2040 (Shell Company, 2020; ExxonMobil Company, 2019; International Energy Agency, 2020). As evidence, world LNG reports from International Gas Union reveal that global LNG trade has grown annually by 4.5% from 2018 to 2022 and reached 401.5 million tonnes (MT) (International Gas Union, 2023). Nevertheless, the high energy consumption of natural gas liquefaction units remains a significant challenge. The utilization of more refrigeration cycles (two or three cycles) is a way to decrease the power consumed in the compression section (Mokhatab et al., 2014; Song et al., 2017; Tak et al., 2023). However, the number of equipment and total capital investment increases (Bahadori, 2014; Zhang et al., 2020). Therefore, other economical solutions, such as optimizing refrigerant composition (He and Lin, 2020; He et al., 2020; Sayadmosleh et al., 2022; Kim et al., 2023), recovery of LNG boil-off gas (Fernández et al., 2017; Al-Sobhi et al., 2021), and utilization of various refrigerants (Wang et al., 2020; Waqar et al., 2022), etc., have also been investigated.

City gate stations (CGSs) are a vital natural gas distribution network segment. In these stations, recovery of exergy or useful work of a high-pressure gas within a pipeline during pressure reduction and the subsequent gas expansion is an efficient way to save energy (Lo Cascio et al., 2018a). Several studies have been recently carried out on the usage of pipeline exergy for different targets such as power generation (Khanmohammadi and Saadat-Targhi, 2019; Kostowski and Usón, 2013; Li et al., 2019; Xu et al., 2022), integration with other processes (Uwitonze et al., 2022), membrane electrolyzer (Ebrahimi-Moghadam and Farzaneh-Gord, 2021), gas liquefaction, etc. The liquefaction of natural gas through gas expansion in CGSs aiming to minimize energy requirements has been of interest topic in recent years. Natural gas trunk-lines usually operate at high pressures (more than 6 MPa) (Mokhatab et al., 2019). The high pressure decreases at CGSs of urban gas distribution networks through an irreversible throttling process in which a part of the useful work will be inevitably lost. Therefore, any design that can prevent exergy destruction is desirable.

The first studies in partial liquefaction of pipeline gas using gas expansion at CGSs were carried out by He and Ju. They proposed (He and Ju, 2013) a novel process for liquefying a part of the natural gas stream in a high-pressure pipeline by expansion. The system's novelty is using a temperature decrease resulting from the pressure drop of inlet natural gas to supply cold utility requirement (a self-refrigerated design) leading to a low energy consumption (0.03975 kWh/Nm3). In the subsequent study (He and Ju, 2014), they improved the previous design so that the required energy from external sources reached zero. The optimal process can liquefy 12% of the natural gas stream using only the high-pressure exergy of the pipeline. The proposed design is novel and self-sufficient in energy; however, (1) a pretreatment unit has not been embedded within the system to remove a high content of impurities in pipeline gas, (2) an NGL recovery system has not been considered to reduce a high content of heavier hydrocarbons in LNG product, (3) the returned stream to supply the cold duty is 87% of the inlet gas and if pretreatment unit is existed in the process, sending this pretreated stream back to the pipeline is unfavorable. Tan et al. (2016) enumerated some disadvantages of He and Ju's study and proposed a new design to liquefy a part of pipeline gas via the expansion process. The system showed three advantages over He and Ju's work: (1) adjusting the required power and liquefaction rate by expansion without external sources, (2) adjusting the flow of returned stream with change in operating condition to preserve total energy consumption, (3) utilizing throttling valve instead of turboexpander at low temperatures which results in a higher tolerance of the process against liquid formation in the expansion outlet stream. Nevertheless, in this system, like the He and Ju's design, no route is considered for feed pretreatment and use of the return gas flow. In another research, Guo et al. (2018) offered a novel liquefaction process based on the Joule-Thomson cycle utilizing pressure exergy of high-pressure natural gas. In the process, a part of the cooling duty of low-temperature heat exchanger can be supplied by expanding the high-pressure natural gas itself. Indeed, two distinct refrigeration cycles provide the central part of cooling capacity. In addition, the compression and isenthalpic expansion with multiple pressure levels can enhance thermal efficiency and reduce the irreversible losses in the heat exchangers. The process was also evaluated by exergy analysis, and results indicated that compressors and coolers produce the highest exergy destruction. Pajaczek et al. (Pajączek et al., 2020) compared several liquefaction designs through high-pressure pipelines from technical and economic viewpoints. They investigated three configurations integrating pressure reduction with turboexpander and NG liquefaction varying internal efficiency of expander and size of liquefaction unit using energy, exergy, and thermoecological cost analysis. Results showed that the energy efficiency of the integrated expansion-liquefaction system varies from 35% to 67%, while its exergy efficiency ranges from 16% to 46%, depending on the size of the liquefaction unit and the gas liquefaction method. Li et al. (2021) proposed a novel natural gas pressure reduction and liquefaction system (PRLS) to recover the exergy efficiently and produce the liquefied natural gas through isentropic expansion. Two system configurations were studied based on Linde and Claude cycles, and thermodynamic performances were analyzed by exergy analysis. Results reveal that the maximum liquid yield and exergy efficiency of the proposed PRLS are 23.1% and 66.7%, respectively. The liquid yield is relatively high for a self-refrigerated system however, lacking a pretreatment unit.         ","This research introduces a novel natural gas liquefaction system integrated with helium recovery and innovative carbon dioxide (CO2) capturing and liquefaction processes. The system utilizes the high-pressure exergy of pipeline gas during expansion at city gate stations (CGSs) to achieve natural gas liquefaction with low energy consumption. Energy, exergy, economic, and exergoeconomic analyses were conducted, demonstrating that the proposed system can co-produce LNG, liquid helium, and liquid CO2 products efficiently and profitably. The specific power consumption (SPC) and exergy efficiency of the natural gas liquefaction system show favorable performance, with potential for further improvement. This integrated system offers a promising solution to address the high energy consumption challenges in natural gas liquefaction.





"
"Recent efforts to study the helium extraction process originated first by Mehrpooya and Shafaei. They investigated (Mehrpooya and Shafaei, 2016; Shafaei and Mehrpooya, 2018) three modified integrated processes for producing LNG and raw helium (50% He + 50% N2) designed by Linde AG, Air Products and Chemicals, Inc. (APCI), and ExxonMobil using advanced exergy analysis. The processes are based on flash vaporization and cryogenic distillation concepts. Results indicated that the efficiency of the integrated processes is higher than the flash method. Also, endogenous/exogenous study results show that the portion of endogenous exergy destruction in the components is higher than in the exogenous part. Continuing the previous research, Mehrpooya et al. (Ansarinasab et al., 2017, 2018; Mehrpooya et al., 2019) also evaluated the abovementioned processes using advanced exergoeconomic analysis. It can be concluded from the endogenous investment and exergy destruction cost results that the interactions between the process components are not strong and do not significantly affect the processes' economics. Hamedi et al. (2019a) conducted a comprehensive analysis of the helium extraction process, emphasizing upgraded helium (more than 90% He) production. They proposed an energy-integrated design for the two helium separation technologies, cryogenic distillation and membrane, with a single-column NRU. The systems' energy consumption was optimized using the particle swarm optimization method, and then an economic analysis was adopted. The results reveal that if nitrogen is considered an effluent, the cryogenic distillation technology shows more advantages in both power consumption and capital costs and can reduce power consumption by 10–40%. In another work (Hamedi et al., 2019b), they also presented a novel, cost-effective silica membrane-based process to extract helium from natural gas to omit the costly inter-stage compression leading to a 30% reduction in the capital cost. In another study, Mehrpooya et al. (2020) introduced and analyzed a system to produce LNG and He integrated with an absorption refrigeration cycle, a molten carbonate fuel cell, and a power plant. The fuel cell's duty is to supply the required heat and power. The integration led to a significant decrease in the energy requirement of the natural gas liquefaction down to 0.2086 kWh/kgLNG. Pakzad et al. (2021) presented a novel integrated system to liquefy helium and hydrogen simultaneously. The liquid hydrogen is produced in two stages with mixed refrigerant and helium refrigerant systems. Also, liquid helium is extracted in three steps: extraction, pressure swing adsorption, and helium liquefaction subprocesses. Results indicated that the exergy efficiency in the whole process is equal to 67%, and specific energy consumption and coefficient of performance are 18.96 kWh/kg and 0.03. According to the reported results, the energy consumption of the integrated process is high. This mainly stem from the physical characteristics of both He and H2 gases. Hamedi (2021) proposed a new process configuration for the co-production of sales gas, natural gas liquids (NGLs), and crude helium. The novel design decreased the equipment by omitting a refrigeration cycle and a propane precooling system in NGL plants. High ethane and helium recovery, high nitrogen rejection rate, and production of upgraded helium are the advantages of the novel process. Al-Sobhi et al. (2022) assessed three cryogenic processes involving cold box, single-column, and double-column systems from technical, economic, and environmental perspectives. To develop NG liquefaction models for different helium concentrations, the Aspen HYSYS simulation package was used. Results revealed that the single-column and double-column technologies show the highest decrease in energy and emissions by integrating helium extraction. Jiang et al. (2023) recently proposed and optimized a process for helium extraction and LNG coproduction (He-LNG), includes two recovery columns and a main multi-stream heat exchanger. Results showed that the reflux flow from the helium concentrator (The second recovery column) makes the effect of helium content proper. Also, the process exhibits good adaptability to various feed gas with different N2 and CO2 contents.

Unlike worthy research on the usage of high-pressure exergy of pipeline gas for liquefying natural gas as well as helium recovery system integrated with the LNG process, there is still a shortcoming in comprehensive study on the economic effect of pretreatment unit on the liquefaction system in CGSs, feasibility of helium recovery integrated with these liquefaction systems and also feasibility of CO2 capturing from the pretreatment unit using cold duty of liquefaction system. Thus, further studies are still needed in this field, and the present work focuses on the mentioned literature gaps. This study introduces a novel self-refrigerated liquefaction process using high-pressure exergy of pipeline gas integrated with helium recovery, purification, and liquefaction processes. The system is equipped with a pretreatment unit before the liquefaction process and a novel CO2 liquefaction system to capture acid gas from the overhead stream of the regeneration column in the sweetening unit. The innovative aspects of this study are (1) a pretreatment unit that includes acid gas removal, dehydration process, and NGL recovery considered in the design of the liquefaction system, (2) a helium extraction unit integrated with the liquefaction system based on the high-pressure exergy of a pipeline, and (3) a novel system for CO2 capturing from regenerator column embedded in the liquefaction system using its cold duty. Moreover, the proposed process is evaluated using energy, exergy, economic, exergoeconomic, and profit analyses to deeply understand the novel process configuration, process equipment, and their interaction from technical and economic viewpoints. The proposed system can co-produce LNG, liquid helium, and liquid CO2 with low energy consumption, simplicity, and high profitability.","Recent research efforts have focused on helium extraction processes integrated with LNG production. Studies have examined various processes, including those by Linde AG, Air Products and Chemicals, Inc. (APCI), and ExxonMobil, utilizing advanced exergy and exergoeconomic analyses. These efforts aimed to improve efficiency and economics. Additionally, research has explored novel designs for helium extraction, such as cryogenic distillation, membrane-based processes, and integration with other systems like absorption refrigeration cycles and fuel cells. However, there remains a need for a comprehensive study on the economic impact of pretreatment units in liquefaction systems at city gate stations, the feasibility of helium recovery integrated with LNG processes, and CO2 capturing from pretreatment units using the cold duty of liquefaction systems. This study addresses these gaps by introducing a self-refrigerated liquefaction process integrated with helium recovery, pretreatment units, and CO2 capture, evaluating its technical and economic aspects.





"
"Global cost optimization of a mini-scale liquefied natural gas plant
Author links open overlay panelAmir Hamzeh Aslambakhsh a, Mohammad Ali Moosavian b, Majid Amidpour c, Mohammad Hosseini d, Saeedeh AmirAfshar e
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.energy.2018.01.127
Get rights and content
Abstract
Cryogenic natural gas liquefaction plant has huge capital and operating expenses corresponding to operating equipment and energy utilization. Considering ever-increasing energy price, therefore, minimization of energy consumption rate for a better profit is highly required. However, any un-engineered energy cut off would result in larger surface area of heat-exchanger and hence bigger capital cost. Here, the net profit of establishing a mini 50 ton/day liquefied natural gas facility, operating for 25 years, is optimized via Genetic Algorithm technique. Poly Refrigerant Integrated Cycle Operations (PRICO) process is simulated in HYSYS environment and linked to MATLAB software for subsequent maximization. The simulation resulted in total consumed power, heat exchanger area and total profit by 2745.33 kW, 3285.58 m2 and 1266.64 million$, respectively. In order to determine unit efficiency and plant irreversibility rate, exergy analysis is performed on individual equipment. Basically, thirteen independent variables are considered for optimization of objective function. Sensitivity analysis for objective function is considered by altering each variable. Final results indicate 9.26% rise in total profit (1383.95 million$) by 59% reduction in energy utilization (1127.68 kW) and 37.50% in heat-exchanger size (2053.7 m2). Meanwhile, the total and heat-exchanger exergy losses are decreased by 65.8% and 80.7%, respectively.

Introduction
In recent years, natural gas utilization for residential appliances has also expanded to industries since it is the most environmentally benign fossil fuel. Its market is growing steadily and 4.5% growth (p.a.) is projected for global LNG (Liquefied Natural Gas) supply until 2030, which is twice of total global gas production (2.1% p.a.) and faster than inter-regional pipeline trade (3.0% p.a.) [1]. Here, mini LNG facilities can secure natural gas supply to areas with low demand and also cover peak shaving in more intense market regions than pipeline. LNG merit against natural gas (NG) would be in its storage capacity (1/600th of the original volume) at near atmospheric pressure [2].

Although mass quantity production of LNG is more economical, there are challenges that entail establishing small plants. Lower total cost (sum of capital and operational expenses), fewer equipment as well as facility of installation, maintenance and operation in mini processes would be advantageous to base load projects which normally require several years of engineering, procurement, construction and commissioning. Hence, construction of mini plants can speed to market delivery compare to larger scale infrastructures [3].

Single Mixed Refrigerant (SMR) technology is extensively used in LNG plants since it has high efficiency, small pieces of rotating equipment and flexibility for altering feed gas as well as operating environment conditions [3]. Minimization of operating costs and equipment sizes has extensively practiced for SMRs to enhance final unit profit. Mokarizadeh and Mowla [4] optimized a two-compression stage process with GA method which improved the compressors’ energy consumption by 3% and 6.5% per kg of LNG when compared to Lee’s work [5]. Mehrpooya et al. [6] achieved a 28% rise in profit by optimization of NGL recovery unit. Aspelund et al. [7] used Tabu Search (TS) and the Nelder-Mead Downhill Simplex (NMDS) method to find the total refrigerant flow rate, composition and the refrigerant suction and condenser pressures that minimize the energy requirements of a PRICO process. Wahl et al. [8] proposed a constraint handling method for PRICO process by utilizing process characteristics and compared with static penalty function formulations. Considering plant’s total energy consumption as objective function, Alabdulkarem et al. [9] optimized a C3MR process with 22 independent variables via genetic algorithm. For more convenience, firstly the MCR section and subsequently the propane cycle was optimized which reduced energy consumption in each section by 13.28% and 17.16, respectively. Nogal et al. [10] performed a review on LNG cascade technologies and enhanced the sum of total cost and energy consumption via GA. Quite recently, Hwang and Lee [11] considered elaborate offshore selection criteria including connectivity costs, deck area construction expenses plus Main Cryogenic Heat Exchanger (MCHE) and separators’ distance from the centerline of hull to identify an optimal liquefaction process system. Also, Pham et al. [12] exploited process knowledge inspired decision-making method for liquefied natural gas process optimization. Energy saving of 30.6% can be accomplished by the optimization. Ding et al. [13] simulated, analyzed and optimized mixed fluid cascade process by genetic algorithm. Song et al. [14] modeled a single nitrogen expansion process with carbon dioxide pre-cooling in Aspen HYSYS, which is connected to MATLAB by ActiveX technology to establish a hybrid simulation platform. Qyyum et al. [15] modeled a SMR process in Aspen Hysys then connected to a Microsoft Visual Studio environment and optimized SMR process by proposing hybrid modified coordinate descent (HMCD) algorithm. A modiﬁed DIRECT (DIviding a hyper-RECTangle) algorithm with a sub-dividing step for considering hidden constraints is proposed by Na et al. [16] which by applying proposed algorithm the speciﬁc power required for natural gas liquefaction decreased to 18.9%. Lee et al. [17] proposed a process design of Organic Rankine Cycle using Aspen Plus with a stochastic solver, GA, and an Aspen Plus-MATLAB interface.

According to the results of previous works, it can be concluded that mixed refrigerant technology is a suitable option from energy consumption aspect. Among the large variety of mixed refrigerant processes, PRICO process is the best choice for a mini-scale LNG plant due to its simplicity and therefore, it is considered as baseline process for this study. Moreover, performance analysis and cost optimization have not been performed on a mini-scale LNG plant in the recent studies. Thus, in this paper, an analysis and optimization of a liquefied natural gas plant were carried out through genetic algorithm. Since the objective function is total profit, there would be a tradeoff between consumed electricity charges and total heat exchanger surface area. Although minimizing compressor power might substantially increase heat transfer surface area for MHEX. Hence, the objective function is considered to cover both aspects (capital and operation costs).","The study focuses on optimizing the net profit of a mini-scale liquefied natural gas (LNG) facility with a capacity of 50 tons per day over a 25-year operating period. Genetic Algorithm (GA) techniques are employed to maximize profit by minimizing energy consumption while considering capital costs associated with heat exchanger area. The Poly Refrigerant Integrated Cycle Operations (PRICO) process is simulated in HYSYS and linked to MATLAB for optimization. Exergy analysis is used to assess equipment efficiency and plant irreversibility. The results show a significant increase in total profit, a reduction in energy utilization, and a decrease in heat exchanger size, emphasizing the economic and energy-saving benefits of the optimized mini LNG plant.





"
"Modified Galileo self-refrigerated liquefaction process for economical heavy hydrocarbon removal during gas liquefaction
Author links open overlay panelMahsa Yousefikhanghah a, Laleh Shirazi b, Abbas Naderifar a, Mehran Sarmad b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.seta.2021.101539
Get rights and content
Abstract
Effective removal of heavy hydrocarbons prior to liquefaction processes has attracted attention. The purpose of this study was to introduce a modified version of Galileo self-refrigerated liquefaction process and to investigate economical removal of C5+ components under lean feed conditions. A separation section, composed of a two-phase separator and a self-refrigerated exchanger is integrated with the main Galileo process to remove C5+ hydrocarbons from feed gas. This play a complementary role for Galileo ZPTS® conditioning unit for removing non-hydrocarbon impurities. The simulation results showed acceptable removal for C5+ hydrocarbons from feed gas with different compositions before LNG (Liquefied Natural Gas) production. However, for modified process the overall specific power consumed increased (0.628–0.692 kW/kg LNG) for 10% compared with the original process. The modified process was economically compared with adsorption method for heavy hydrocarbon removal and results indicated new scheme can separate heavy components with 56% lower purchased costs and 9% lower total annualized cost compared with an adsorption system. Exergy analysis was also conducted and result showed 11% increase (351.46–392.25 kW) in total exergy destruction of the proposed scheme, however overall exergetic performance of both structures were almost the same.

Introduction
Liquefied natural gas (LNG) is widely used in the transport of natural gas from gas fields that are located far away from the world market [1]. Throughout the past decades and due to low emissions and high energy capacity, small scale LNG production (<0.5 MTPA) has become more popular as a substitutive fuel for diesel in vehicles [2]. Small scale LNG plants may also be utilized for peak-shaving or energy supply to remote areas [3].

Skid-mounted LNG plants basically refer to several pre-built modules that produce LNG at small capacities ranging from 2000 to 60,000 Nm3 per day. These modules are small enough to be packaged onto skids and they are transported easily from one gas source (flare gas, pipeline gas, associated gas and etc.) to another by trucks [3]. Selecting a suitable liquefaction process can lead to significant reduction in capital and operational costs [4]. Based on process configuration and working fluids, skid-mounted liquefaction packages are classified into three types: self-refrigerated (open loop) cycle, nitrogen expansion cycle and single mixed refrigerant (SMR) cycle. Compared with self-refrigerated (open loop) systems, mixed refrigerant cycles are often more sophisticated since several equipment are required for the operation and maintenance of external refrigerant fluid and this clearly makes the plant difficult to control. Besides, utilizing an external refrigerant increases the risk of environmental hazards [5]. Likewise, the nitrogen expansion cycle requires significant nitrogen supply and high power consumption [6]. However, out of all these structures, self refrigerated cycle can be a good option for skid-mounted LNG plants due to its reliability, safety, simplicity and high efficiency in operation (start-up and shut-down) as well as lower capital and operational costs.

One of the basic problems in gas liquefaction processes is the existence of heavier components in feed gas that leads to blockage of pipelines and other process equipment. These components (such as C6, C7, C8, C9 and BTX) can freeze at LNG temperatures, thus their concentration must be decreased to permissible values before LNG production [7].

The feed gas to a skid-mounted package is normally rich in methane and lean in NGL compared with gas from a regular source, however the amount of heavy hydrocarbon (HHC) existing in the feed is still more than the allowable LNG specification and therefore must be removed from the feed to prevent freezing during the liquefaction process [7]. Presence of heavy hydrocarbons in feed is even of more concern when using the pipeline gas (with high levels of methane). Because of annual changes in the amount of HHC in pipeline gas, an appropriate separation method should be applied to decline levels of heavy components and restrain possible fluctuations in their concentration.

To date, there have been several mature technologies available in industry that remove heavy hydrocarbons from lean feed gas in LNG plants including partial condensation systems, adsorption systems and combined systems. Partial condensation is a process whereby temperature of gas is lowered in a liquefaction unit and thus part of heavy components are condensed into the liquid phase. The vapor-liquid stream is then separated into residual rich liquid and overhead treated gas by a two-phase separator. The NGL byproduct that contains small amounts of methane can either be utilized as fuel or domestic pipelines make-up or it can be processed to recover the methane content. In a partial condensation system, there are no lower limit on NGL content in feed gas, also the system is easy to adjust to annual fluctuations in feed composition. Partial condensation efficiently removes hexane and heptane in a wide range of temperatures, however removing heavier constituents (e.g. C8, C9, Benzene) requires lower operating temperatures (e.g. −80 °C) that increase methane loss in NGL byproduct. A partial condensation system must be operated at a pressure below the critical point in order to reach an efficient removal of HHCs. However, this can cause a reduction in liquefaction efficiency and increase power consumption. Adsorption systems are another commercially proven technology used for removal of C5 ~ C7 and low solid solubility components (C8+) from feed gas. In this arrangement, impurities are selectively removed by passing the feed gas through a packed adsorbent bed and then are desorbed by heated regeneration gas. The purified gas from the adsorption unit is then recycled to be utilized for both liquefaction and regeneration purposes. Scale of an adsorption system is influenced by the amount of HHC present in feed gas i.e. the higher the HHC concentration, the larger the size of the adsorption equipment, and the adsorbent bed is saturated with impurities more frequently. Thus this system is not an economical option for treatment of feed gas containing large amounts of HHC. Moreover, many novel combined methods are designed for treatment of lean gas. Development of an integrated adsorption/partial condensation system by Air Product company is a case in point where a portion of the HHCs are removed by an adsorption system and the remaining HHC is condensed in the liquefaction unit followed by a separator [7]. Many studies are conducted for NGL removal and natural gas liquefaction cycles. Vatani et al. presented an NGL/LNG process for large-scale plants that were equipped with dual mixed refrigerant systems, and concluded acceptable efficiency (0.414 kWh/kg LNG) and high ethane recovery (higher than 93.3%) under rich feed gas conditions [8]. Ghorbani et al. investigated an integrated process of NGL/LNG with a nitrogen removal unit and the results presented lower power consumption (around 0.33–0.343 kWh/kg LNG) and higher thermal efficiency (equal to 62.82%) than other similar technologies [9]. Nikkho et al. simulated and optimized two modified SMR liquefaction cycles for mini-scale LNG production. The results of exergetic optimization showed that specific energy consumption and overall exergy destruction were decreased by 19.4% and 25.4%, respectively. Also the overall exergy efficiency and minimum SEC were obtained 47.51% and 0.221 kWh/kg LNG, respectively [10]. An improved LNG/NGL co-production process was proposed by Ghorbani et al. for reduction of overall energy consumption. In this scheme the precooling stage of the mixed fluid Cascade system was replaced by an absorption refrigeration system. As a result the overall exergy efficiency was decreased by 12.72% in comparison to the main process due to use of an absorption precooling system. Also the net annual benefit was increased by 6.2% [11]. Jin et al. compared the economic performance of several NGL removal configurations with lean feed gas conditions and they found that scrub column had the best economic performance for lean feed gas composition in comparison with other schemes [12]. A novel scheme of NGL recovery process was suggested by Park et al. for off-shore applications. Nine patented NGL recovery processes were selected and modified for off-shore applications and the results suggested that the proposed NGL process had the lowest operating and capital cost compared with other processes [13]. Junior et al. compared and analyzed several condensation mechanisms for NGL recovery process to obtain a suitable mechanism that meets market required specification for NGL. The results showed that turbo-expander with mechanical refrigeration (TEMR) presented the best economic performance for all feed gas compositions than other schemes [14]. Yuan et al. designed and optimized an SMR liquefaction process with a CO2 precooling stage to obtain conversion efficiency of 0.77 with power consumption of 9.90 kW/kmol/h. They found that the proposed process in terms of simplicity, operability and safety is suitable for small-scale liquefaction [15]. Zhang et al. compared specific power consumption and investments costs of four expansion based liquefaction cycle consisting of a single nitrogen cycle with and without ammonia precooling, a single methane cycle with and without ammonia precooling. Results indicated that the single-methane with ammonia precooling had the lowest power consumption (28–48%) and 13–43% lower investment costs [16]. A parallel nitrogen expander cycle for skid-mounted liquefaction packages was designed and optimized by He et al. to reach the minimum unit energy consumption. The results showed higher adjustability to varying gas composition and 4.69% lower power consumption than conventional nitrogen expander cycles [17]. Cao et al. compared and analyzed key operational parameters (such as refrigerant flowrate, specific power consumption and liquefaction rate etc.) and heating-cooling curves of two types of liquefaction cycles for skid mounted packages and concluded that methane-nitrogen expander cycle was superior to the MR cycle due to the lack of a propane pre-cooling stage in the latter one [18]. Remeljej et al. simulated and optimized four small-scale liquefaction processes including SMR liquefaction cycle, a two-stage nitrogen expansion cycles and two methane expansion cycle. The exergy analysis indicated that the SMR process had the lowest unit energy consumption with 5.67 kWh/kmol for single-compression stage and 5.10 kWh/kmol for double-compression stage and the specific power of the open loop cycle as well as the other two schemes were obtained 10% and 6–7% higher than the SMR respectively [19]. Ansarinasab et al. proposed and analyzed a dual cycle LNG production process composed of a precooling absorption and mixed refrigerant cycle. The results suggested that the specific power of this process was 20.38% lower than that of conventional MR refrigeration process with a propane-precooling cycle. Also, the efficiency reduced to 0.21 kW/kg h compared with typical liquefaction processes [20]. ","This study introduces a modified version of the Galileo self-refrigerated liquefaction process for the economical removal of heavy hydrocarbons (C5+) from lean feed gas before liquefying it into natural gas (LNG). The modification involves integrating a separation section, including a two-phase separator and a self-refrigerated exchanger, with the main Galileo process. The simulation results demonstrate effective removal of C5+ hydrocarbons from feed gas with different compositions, addressing the issue of heavy hydrocarbon presence in lean gas. However, this modified process exhibits a slight increase in specific power consumption compared to the original process. Economically, it outperforms adsorption methods for heavy hydrocarbon removal, offering lower purchased costs and total annualized costs. Exergy analysis shows a slight increase in total exergy destruction for the modified scheme, but overall exergetic performance remains comparable to the original structure.





"
"Methods and materials
2.1. Thermal dose
Compared to structure fires, where heat, oxygen depletion and inhalation of combustion toxic gases are the main causes of lethality, in outdoor fires the exposure to heat flux is the main cause of casualties. To estimate the level of injury (or death) of a human during fire, the received thermal dose by the human should be quantified as a function of the heat flux and the duration of exposure. Having the thermal dose, the probability of injury (e.g., 2nd degree burn) or death can be estimated using dose-response functions.

Assael and Kakosimos [40] proposed Eq. (1) to calculate thermal dose D (
) as a function of the exposure time 
 (s) and heat flux 
 as:
(1)
(2)
where 
(s) is the initial reaction time, i.e., the average time it takes for an unexpected person to conceive the emergency situation and make decision (∼ 5 s); 
(s) is the escape time, i.e., the time it takes for the person to reach a safe spot (where heat radiation intensity is below 
, either by sufficiently distancing from the fire or by taking refuge in a shelter); x0 (m) is the initial distance of the person from the fire, and x1 (m) is the distance of the safe spot from the fire; and u (m/s) is the average running speed of a person (∼ 4 m/s).

Eq. (1) in the current form, however, tends to overestimate the thermal dose as it assumes that during the entire escape time te a person is exposed to the same amount of initial heat flux that he was exposed to during the reaction time. This assumption is obviously oversimplified because radiant heat flux decays exponentially with distance, and as the person runs away from the fire the heat flux they receive diminishes proportionally with the distance. As such, Eq. (1) can be modified as:
(3)
q0 is the initial heat flux the person receives during the reaction time, and q(x) is the heat flux as a function of distance from the fire. In chemical and process plants, due to the plant complexity and presence of equipment and installations or due to multiple simultaneous fires, it is usually neither possible nor safe to escape from fire(s) in a straight line. Presenting the plant's layout as a two-dimensional grid, Eq. (3) can be modified as Eq. (4):
(4)
where i and i + 1 are two neighbour nodes (Moore neighbourhood) on the grid with 
 and 
 as the respective heat flux intensities; 
 is the distance between the i and i + 1, and I is the index of the node representing the safe spot. Several probit functions have been developed to calculate the probability of different levels of injury or death given a certain thermal dose:
(5)
where pr is the probit value, and c1 and c2 are the constants. The probit constants proposed in [40] for assessing the impact of heat flux on humans are presented in Table 1.

Table 1. Probit function's constants proposed for impact of fire on human [40].

Level of injury or death        c1        c2
1st degree burns        −39.83        3.0186
2nd degree burns        −43.14        3.0186
Death        −36.38        2.56
Given the probit value pr, the probability of injury or death can be estimated as:
(6)
where 
 is the cloths coefficient, accounting for the role of cloths in protecting an exposed person, with typical values of 0.85 and 0.18 for summer and winter outfits, respectively [40], and 
 is the cumulative density function of standard normal distribution.

2.2. Dijkstra's algorithm
A variety of greedy algorithms or dynamic programming techniques can be used to identify the shortest path between two nodes on a graph (or mesh). In this section, the application of the Dijkstra's shortest path algorithm [41], which is a greedy algorithm, is demonstrated via a simple example. The application of this algorithm to identifying the safest (and not necessarily the shortest) evacuation path is discussed in Section 3.

Consider a weighted graph 
 with the node set V, edge set E, and the weight set W, where 
 is the weight of the edge 
. Given a starting node, the shortest path from the starting node to all other nodes in the graph can be found using the Dijkstra's algorithm.

In the Dijkstra's algorithm, the state of a node is identified by two variables: the distance value of the node 
, which is its distance from the starting node, and the status label of the node, which can be permanent (p) or temporary (t). The status label for a node is permanent if the distance value of the node shows the shortest distance from the starting node; otherwise, it is temporary. The Dijkstra's algorithm measures and updates the distance values of the nodes until all the status labels turn to permanent. The algorithm for finding the shortest paths between a starting node and the other nodes of the graph can be explained below:
•
Initialization


Assign zero to the distance value and “permanent” to the status label of the starting node i: The state of this node is now (0, p). Assign to every other node a distance value of ∞ and a status label of “temporary”. The states of the other nodes are now (∞, t). Designate the starting node i as the “current” node.
•
Updating the distance values


Find the set J of nodes with temporary labels that can be reached from the current node i via a single link. For each 
, the distance value of node j is updated as: 
, where 
 is the weight of the edge linking node i to node j. amongst the nodes, the one 
 with the smallest updated distance value is added to the set of current nodes, and its status is changed to “permanent”. Subsequently, the state of this node is changed into (
).
•
Termination


The previous step, i.e., updating the distance values, is repeated yet this time by finding the set K of nodes with temporary labels that can be reached from any of the current nodes i or 
 via a single edge. For each 
, the distance value is updated as: 
, where 
 and 
 are, respectively, the weights of the single edges linking nodes i and 
 to node k.

The algorithm terminates when the status labels of all the nodes (or the destination node of interest) that can be reached from the starting node i have been turned to “permanent.”

The steps of the Dijkstra's algorithm can be demonstrated using the simple graph in Fig. 2 where the distance between the neighbouring nodes is presented as the numbers attached to the directed arcs. The aim is to find the shortest paths from the starting node A to node D. Since node A is the starting node, in the first step (Fig. 2a) its distance value and status label have been set to (0, p); for the other nodes, it is (∞, t) for now.

Fig 2
Download : Download high-res image (334KB)
Download : Download full-size image
Fig. 2. Application of the Dijkstra's algorithm [44] to find the shortest path and distance from node A to node D. Panels (a)-(d) present the sequential steps via the algorithm where the distance values and status labels of the nodes are sequentially updated until the algorithm is terminated.

As such, the updated distance values of nodes B and C, which are the neighbours of node A, can be calculated as 
 and 
. Since the updated distance value of B is lower than that of C, it can be selected as the closest node to node A, and thus its distance value and status label can be updated to (2, p), as shown in Fig. 2b. For the sake of clarity, the nodes with permanent distance values are highlighted while the shortest paths between the neighbouring nodes are denoted with bold arcs.

Having A and B as the neighbours of C and both with permanent status labels, the distance value of C can further be updated once by considering the distance from A to C and the other time by considering the distance from B to C, as 
. As such, the updated distance value and status label of C would be (4, p), as shown in Fig. 2c. Likewise, the updated distance of node D can be calculated by considering the length of the paths ending in D from its neighbouring nodes B and C as 
, showing that the distance from C to D is shorter than the distance from B to D. As such, the distance value and status label of D can be updated to (5, p) as shown in Fig. 2d. Subsequently, the compound path 
 can be identified as the shortest path from A to D with a length of 5.

For large graphs, several software tools can be used to implement the Dijkstra's algorithm to find and calculate the shortest path from a starting node to a destination node. For instance, using the “igraph” package [42] in R (a programming language and software for statistical computing and graphics), both the shortest path and the respective distance between any two nodes of a graph can be identified. Fig. 3 depicts the code implemented in R for calculating the shortest path and distance between nodes A and D in Fig. 1.

Fig 3
Download : Download high-res image (216KB)
Download : Download full-size image
Fig. 3. Implementation of the Dijkstra algorithm in R to find the shortest path between nodes A and D. The command lines (1)-(7) define the weighted directed graph depicted in Fig. 2; the output of the command line (8) is the distance value of 5 while the output of the command line (9) is the compound path A, B, C, D.","This study employs the use of thermal dose calculations to assess the potential injury or death of individuals in outdoor fires based on their exposure to heat flux and duration of exposure. The thermal dose is quantified using Assael and Kakosimos' equation, taking into account factors such as initial reaction time, escape time, distance from the fire, and running speed. The study also introduces modifications to the equation to better account for the decreasing heat flux as a person moves away from the fire. Probit functions are utilized to estimate the probability of injury or death based on the thermal dose. Additionally, the Dijkstra's shortest path algorithm is discussed, demonstrating its application in identifying the safest evacuation paths in complex layouts, such as chemical and process plants, considering factors like heat flux intensities and distances. This algorithm is valuable for ensuring the safety of individuals during evacuations in such environments.





"
"To date, there are various LNG self-refrigeration technologies introduced to the market such as LNGO™ (Dresser-Rand company), VX™ cycles (Expansion-Energy), MMS™ (Chester company), LNG in a Box™ (GE oil & Gas company) [25]. Among them, Galileo Cryobox® system is a cost effective and practical solution for liquefaction of gas from unconventional resources. Cryobox® is a portable and cost-efficient liquefaction package that produces 12–16 tons of LNG per day. Based on an adsorption treatment system, Galileo company has designed and developed a ZPTS® gas conditioning plant that is capable of adsorbing some of gas impurities including nitrogen, mercury, water, mercaptan etc, however there are no strong evidence that the abovementioned plants can remove heavy hydrocarbons along with non-hydrocarbon impurities [25]. Integration of a HHC removal system with Galileo process that specifically targets C5+ components can be economically advantageous since two integrated processes can use the same refrigeration system, as a result a number of process equipment are eliminated [26]. Therefore, in this study, a partial condensation system is coupled with Galileo process to investigate the economical removal of C5+ components under lean gas conditions. It must be realized that the integrated partial condensation system is expected to play a complementary role for ZPTS® unit i.e. a part of gas impurities, mainly including non-hydrocarbons, are removed by ZPTS® while the remaining hydrocarbon impurities (C5+) are removed by the partial condensation system. This pattern is similar to the combined separation system presented by Air Product Inc.    ","This study discusses various LNG self-refrigeration technologies available in the market, including the Galileo Cryobox® system, which is known for its cost-effectiveness and practicality in liquefying gas from unconventional sources. While the Galileo ZPTS® gas conditioning plant can adsorb certain gas impurities, it lacks evidence of effectively removing heavy hydrocarbons (C5+). To address this, the study proposes the integration of a partial condensation system with the Galileo process to economically remove C5+ components, complementing the ZPTS® unit's removal of non-hydrocarbon impurities. This integrated approach aims to streamline the liquefaction process and enhance impurity removal, similar to a combined separation system offered by Air Product Inc.





"
"Methodology
3.1. Identifying the safest path to a safe spot
Evacuation in chemical and process plants is not only a matter of time (the shortest distance to a safe spot) but more importantly the intensity of heat fluxes the evacuees would be exposed to while escaping to the safe spot. These two factors, i.e., time and heat flux, can be encapsulated in a single factor, that is, the thermal dose. In other words, the safest path to a safe spot would be the path with the lowest accumulated thermal dose and thus the lowest probability of injury (or death).

That being said, if a chemical plant is represented as a two-dimensional grid, the amount of thermal dose an evacuee would receive when traversing across the grid from point i to point j can be assigned as the weight of the edge 
 connecting i to j. The Dijkstra's algorithm can then be used to identify the path with the lowest accumulated thermal dose. As can be noted, when the distance between i and j is replaced with the accumulated thermal dose between i and j, the shortest path identified using the algorithm between any two points on the grid represents the path with lowest accumulated thermal dose between those points.

For illustrative purposes, consider a hypothetical case in Fig. 4a where an evacuee is exposed to heat flux of fire. Representing the premise as a grid with 100 m × 100 m cells, the goal is to identify the safest path from where the evacuee is standing – cell A – to a shelter at cell E. It is ideally assumed that as soon as the evacuee reaches the shelter, they would be safe, being exposed to zero heat flux.

Fig 4
Download : Download high-res image (249KB)
Download : Download full-size image
Fig. 4. (a) modelling a process plant as a two-dimensional grid. The letters identify the cells while the numbers in the brackets present the amount of heat flux at the centre of each cell, in 
. (b) Presenting the grid as a thermal-dose graph, the weight of each edge represents the average amount of thermal dose (w4/3m−8/3 s) the evacuee would receive while running from one end of the edge to the other end.

Given the type and dimension of the fire and also the weather conditions (wind speed and direction, relative humidity, etc.), the amount of heat flux at the centre of each cell can be calculated using a variety of models and software such as ALOHA [43]. Assume that in Fig. 4a the amount of heat flux (
) at the centre of each cell has already been calculated using a fire model as presented in the brackets. Aside from the initial thermal dose the evacuee may receive during a reaction time of 5 s, the amount of thermal dose they may receive while running from the centre of cell i to the centre of an adjacent cell j can be estimated as 
, in which 
 and 
 (
) are, respectively, the heat radiation amounts at the centre of cells i and j, and 
 (m) is the distance between the centres. (The diagonal distance between the centres of two adjacent cells, e.g., B and D, is about 141 m).

For instance, given a running speed of u = 4 (m/s), the amount of thermal dose the evacuee would receive while escaping from cell B to D can be calculated as 
 (w4/3m−8/3 s). This value is assigned as the weight of the edge 
 in the corresponding thermal-dose graph in Fig. 4b. Following the same approach, the weight of the other edges can be calculated.

Implementing the weighted graph of Fig. 4b in the igraph [42] and applying the Dijkstra's algorithm, the shortest (safest) path from cell A to cell E, associated with the least possible cumulative thermal dose, is identified as A → B → D → E with a total distance (cumulative thermal dose) of 6.35 × 106 (w4/3m−8/3 s). It should be noted that the amount of thermal dose the evacuee would receive during the reaction time (the first term on the right-hand side of Eq. (4)) is not needed for identifying the safest path because it is a denominator to all the paths and thus can be factored out when comparing different path alternatives.

3.2. Optimal evacuation in case of multiple safe spots
In the case of several evacuees and multiple limited-capacity safe spots in a facility, it is crucial to identify the number of evacuees to be sent to each safe spot so that the total probability of injury could be minimized. To this end, mathematical programming can be used for optimal allocation of evacuees to the safe spots. Mathematical programming has long been recognized and employed in process safety, amongst others, for optimal allocation of safety resources [44,45], optimal design of plants’ layout [46], and optimal firefighting [3].

To demonstrate the application of mathematical programming to optimal evacuation, consider the same example as in the previous section yet with more evacuees and more shelters on the premise (Fig. 5). Assume that there are two cells i = {1, 2} which need to be evacuated (e.g., cells A and C), and there are two shelters j = {1, 2} with limited capacities (e.g., cells E and G) to accommodate the evacuees. Given the previous fire scenario, the safest paths from cells A and C to cells E and G can be identified using the Dijkstra's algorithm, as described in the previous section. Figs. 6a and 6b depict the safest paths from cell A and cell C, respectively, to the shelters. Having the amount of the cumulative thermal dose associated with each safe path (Table 2), the probability of injury 
 associated with the safest path from cell i to shelter j can be calculated using the probit functions given in Eqs. (5) and (6).

Fig 5
Download : Download high-res image (106KB)
Download : Download full-size image
Fig. 5. An example of a multiple-evacuees multiple-shelters optimal evacuation problem. The evacuees are at cells A and C while there are two limited-capacity shelters in cells E and G.

Fig 6
Download : Download high-res image (290KB)
Download : Download full-size image
Fig. 6. Thermal-dose graphs used for identifying the safest paths, using the Dijkstra's algorithm, (a) from cell A to the shelters in cells E and G, and (b) from cell C to the shelters in cells E and G. The numbers on to the edges denote the amounts of respective thermal dose (w4/3m−8/3 s).

Table 2. Accumulated thermal dose an evacuee may receive while escaping from cell i to cell j in Fig. 6, and the respective probability of second-degree burn. A cloths coefficient of FC = 0.8 has been considered for illustrative purposes.

Empty Cell        Shelter cell (j)
Evacuation cell (i)        Accumulated thermal dose (Dij)        Probability of 2nd degree burns (Pij)
E        G        E        G
A        6.35 E + 06        7.34 E + 06        0.157        0.270
C        3.18 E + 06        6.35 E + 06        0.001        0.157
For instance, given that an evacuee may receive a total accumulated thermal dose of 6.35 × 106 (w4/3m−8/3 s) while escaping from cell A to the shelter in cell E (Fig. 6a), the probability of the evacuee suffering a 2nd-degree burns can be estimated in two steps: (i) calculating the probit value using Eq. (5) in which c1 and c2 for the 2nd-degree burns are read from Table 1, as pr = - 43.14 + 3.0186 ln(6.35 × 106) = 4.14, and (ii) calculating the probability of injury using Eq. (6) while assuming a cloths coefficient of FC = 0.8, for illustrative purposes, as P = 0.8 Φ(4.14 – 5) = 0.157. The results for the other escape paths have been summarized in Table 2.

To develop the objective function and constraints required in the mathematical programming, the number of evacuees at cell i is denoted as 
 while the capacity of the shelter at cell j is denoted as 
. Furthermore, the number of evacuees from cell i who take shelter in cell j is denoted as 
.

To consider the worst-case scenario where the total number of evacuees may exceed the total capacity of the shelters (i.e., 
), the number of people from cell i who do not succeed to take shelter (for instance, because the shelters are over-capacitated) can be denoted as 
, and a respective injury probability of 
 can be considered for whom. Having the parameters identified, the mathematical programming for identifying the optimal evacuation plan can be developed as:
(7)
(8)
where j = 0 denotes the evacuees, who fail to take shelter. The objective function z in Eq. (7) denotes the expected number of injuries which should be minimized. In Eq. (8), the first restriction (equality relationship) ensures that the total number of evacuees from cell i, those who take shelter plus those who do not, is equal to the population of cell i. The second restriction (inequality relationship) ensures that the total number of evacuees taking shelter at shelter j does not exceed the capacity of the shelter. The third restriction enforces a non-negative solution. Solving Eqs. (7) and (8) for a scenario, the values obtained for 
 represent the optimal evacuation plan for that scenario. To make the case more concrete, assume that the number of people in cells A and C are, respectively, 
and 
, and the capacity of the shelters in cell E and G are, respectively, 
 and 
. Inserting the numbers in Eqs. (7) and (8), the objective function and restrictions would be as:
 

Solving the system of equations in Microsoft Excel®, the optimal values of the variables are determined as: 
, with a total number of injuries as z = 4.37. In other words, all the 15 people at cell A should take shelter in cell G while 10 out of 12 people at cell C should take shelter at cell E, and the rest at cell G with no people from the either cell being left out.

Now consider another scenario in which the capacities of the shelters are the same as the previous scenario but the number of people at cells A and C are, respectively, 
and 
. In this scenario, since the total number of evacuees is larger than the total capacity of the shelters, it is inevitable that some evacuees are left out, not being accommodated into the either shelter. Solving the system of equations (the same objective function yet different constants on the right-hand side of the first two restrictions), the optimal numbers of the evacuees from each cell that have to be allocated to each shelter are determined as: 
. As expected, 3 evacuees from cell A were left out, not being accommodated into any of the shelters.

Comparing the results of the two foregoing scenarios, a few points are worth noting: First, due to the relative locations of cells A and C with regards to the shelters and accordingly the amounts of receive thermal dose and the probabilities of injury, the priority is given to the evacuation of cell C to lower the expected total casualties. Second, when the capacity of the shelters is not enough, some people from cell A should be left out for the others to be able to take shelter. Third, in case the population of cell C is higher than the capacity of shelter E, some people from cell C should be sent to the shelter at cell G rather than E and thus take a higher risk of injury (see the injury probabilities in Table 2). But who will decide which evacuees should take shelter and which evacuees should not, or which evacuees from cell C should take shelter in cell G instead of E? Although the first two points can seem to be addressed via optimal identification of the location and capacity of the shelters (and units) in design stage of chemical plants, the third point remains as an ethical issue which is beyond the scope of the present study","The provided text discusses the methodology for identifying the safest evacuation path in chemical and process plants during fires. It combines the concepts of thermal dose, Dijkstra's algorithm, and mathematical programming to determine the path with the lowest accumulated thermal dose, thereby minimizing the probability of injury or death. The methodology involves representing the plant layout as a grid, calculating thermal doses for different paths, and optimizing the allocation of evacuees to shelters based on thermal dose considerations. This approach aims to enhance safety during evacuations in complex industrial environments.


"
"A goal programming approach to multi-objective optimization of firefighting strategies in the event of domino effects
Author links open overlay panelNima Khakzad
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.ress.2023.109523
Get rights and content
Abstract
In the event of fire in tank terminals, an ideal firefighting strategy would include simultaneous suppression of all burning tanks and cooling of all exposed tanks. This strategy, if effective, would confine the fire and prevent its escalation throughout the plant. However, limited firefighting resources, which is the case at most tank terminals, do not usually allow for conducting an ideal firefighting particularly if fire propagates from an originally burning tank to adjacent tanks, creating a domino effect. As such, an optimal strategy would be needed to determine which burning tanks to suppress and which exposed tanks to cool so as to best satisfy the safety objectives. For tank terminals that are near communities or offsite assets, the situation can become more challenging as a multi-objective firefighting strategy would be needed not only to limit the possibility and extent of domino effects within the plant but also to protect people and assets located outside the premises of the plant. For this purpose, in the present study, a methodology is developed based on goal programming – a multi-objective optimization technique – for identifying optimal firefighting strategies while considering both onsite and offsite risks of domino effects. The resulting firefighting strategies are demonstrated to be more effective and consistent than the ones identified using single-objective optimization techniques or general guidelines.

Introduction
Firefighting at tank terminals is usually considered the last on the list when it comes to fire prevention and protection [1]. Firefighting, compared to fire prevention measures (e.g., regulations for safe storage of flammable substances) and fire protection measures (e.g., sprinkler systems), is much costlier, more resource-demanding, and riskier. In 2018, a single tank fire at an oil storage terminal in Singapore took 6 h of intense firefighting to extinguish, involving 31 firefighting vehicles and 128 personnel [2].

Depending on the size and type of fire, a passive, offensive, or defensive firefighting strategy can be adopted. In a passive strategy, a burning vessel is left to burn out with no attempt to extinguish the fire. This strategy is usually applied to jet fires where the high momentum and the turbulent weather produced by the fire do not let the foam reach the fire [3]. In an offensive strategy, burning vessels are suppressed whereas in a defensive strategy the burning units are left to burn and the exposed units are cooled to prevent or reduce their damage likelihood. In practice, however, most firefighting strategies are a mix of the foregoing strategies. In other words, in the event of multiple fires, some burning vessels are suppressed and some left to burn out, and some of the exposed vessels are cooled.

An ideal firefighting strategy should confine the fire and prevent its propagation to adjacent vessels by suppressing all the burning vessels and cooling all the exposed vessels until the fires are fully extinguished. However, when the total number of burning and exposed vessels exceeds the available resources, conducting an ideal firefighting is not feasible particularly if fire propagates from an originally burning vessel to adjacent vessels and creates a domino effect. In the event of domino effects, the number of units in need of cooling grows exponentially with the number of burning vessels [1], quickly making the initially scarce resources even less sufficient to conduct an ideal firefighting. In such cases, an optimal firefighting strategy would be needed to limit or slow down the progress of domino effect until more resources become available. Such strategies should help the firefighters prioritize the endangered vessels and decide which ones to include in firefighting [4,5].

For tank terminals that are near residual communities or industrial areas, an optimal firefighting strategy should not only limit and control potential domino effects inside the plant but also reduce the risk of damage to neighbor plants and communities, for instance, via external domino effects [6], [7], [8]. Failures to comply with regulations such as occupational health and safety acts and land-use development directives, which set out onsite and offsite safety objectives, may cause a process plant to lose its licence temporarily or permanently [9]. To achieve these multiple safety objectives via a single firefighting strategy, methodologies are required that can combine domino effect models with multi-objective optimization techniques while considering the resources, constraints, and safety objectives.

Despite many studies on modeling and risk assessing of domino effects and relevant fire protection systems [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], work devoted to domino effects’ emergency response, including evacuation [27] and firefighting [1,4,5,[28], [29], [30]] has been few. Khakzad [4] developed a methodology for identification of optimal firefighting strategies in tank terminals by combining graph theory and information theory. Cincotta et al. [5] developed a methodology based on Bayesian network and resilience theory to identify firefighting strategies that maximize the resiliency of the process plant. Zhou et al. [28] employed the event sequence diagram while Zhou et al. [29,30] used Petri net to identify and optimize firefighting schedules. Khakzad [1] developed and compared methodologies based on dynamic influence diagram (as an extension of dynamic Bayesian network) and mathematical programming for optimal firefighting of domino effects.

In the foregoing studies, the firefighting strategies were identified considering merely one objective: Minimizing internal risks of domino effects (i.e., risk of damage to plant's properties). However, as discussed before, an effective firefighting strategy should achieve several onsite and offsite safety objectives. In addition to the previous studies, there are some general recommendations as to which vessels to extinguish and cool to prevent from domino effects [3,[31], [32], [33]] yet without either a detail modeling and analysis of domino effects or considering onsite and offsite risks.

To address the drawbacks of the previous studies, which did not consider either offsite safety objectives [1,4,5] or potential domino effects [31], [32], [33], in the present study, a methodology is developed based on goal programming (a multi-objective optimization technique) for identification of optimal firefighting strategies in tank terminals with the aim of minimizing the risk of damage to the plant (onsite risk) and risk of damage to people and properties outside the plant (offsite risk). To minimize offsite risks, land-use development regulations are considered as benchmarks. In Section 2, fundamentals of firefighting, land use planning, and goal programming are briefly reviewed. Section 3 presents the methodology. Section 4 presents and discusses the results. Section 5 summarizes the main outcomes of the study","The study introduces a goal programming approach for optimizing firefighting strategies in tank terminals facing potential domino effects during fires. It aims to balance onsite and offsite safety objectives, considering the risk of damage to both plant properties and neighboring communities or assets. Firefighting strategies are optimized to effectively suppress burning tanks and cool exposed ones. The methodology incorporates multi-objective optimization techniques and land-use development regulations to achieve comprehensive safety goals, addressing the limitations of previous studies that focused solely on onsite risks or lacked detailed domino effect modeling. This approach enhances the effectiveness and consistency of firefighting strategies in complex industrial settings.





"
"Cost-effective allocation of safety measures in chemical plants w.r.t land-use planning
Author links open overlay panelNima Khakzad a, Genserik Reniers a b c
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.ssci.2015.10.010
Get rights and content
Abstract
Land-use planning (LUP) has widely been employed as a protective safety measure in risk management of major hazard installations such as chemical plants. In the European Union countries, a majority of relevant work over the past years has been inspired by the Seveso II Directive. The inclusion of LUP in the Seveso II Directive has been with the aim of mitigating off-site damage of major accidents on public via setting criteria for (i) the identification of the location and layout of new installations, (ii) the development of existing installations, and (iii) the land developments in the vicinity of existing installations. We, in the present study, have proposed a methodology based on Bayesian network (BN) for cost-effective allocation of safety measures in chemical plants so that both internal and external risks could effectively be mitigated, particularly in compliance with the requirements of LUP. We first employed BN to calculate risks, and then extended the BN to a limited memory influence diagram using additional decision and utility nodes so that it can be used for multi-attribute decision analysis. The development and application of the methodology have been illustrated via fireproofing of a hypothetical fuel storage plant.

Introduction
The importance of land-use planning (LUP) as a nonstructural protective safety measure in the context of natural hazards has long been recognized by safety experts and risk managers. However, the application of LUP to mitigate the off-site impacts of major technological accidents such as fires, explosions, and toxic gas dispersion is relatively new (Christou and Poerter, 1999, Christou et al., 1999, Christou et al., 2006, Laheij et al., 2000). Early applications of LUP to protect the public from the consequences of major accidents in major hazard installations such as chemical plants in Europe dates back to the 1970s when the Flixborough disaster in 1974 in the UK led to the Health and Safety At Work Act in the same year, requiring industries to keep internal risks (on-site risks) as well as external risks (off-site risks) as low as reasonably practicable (HSE, 1989).

The majority of relevant work over the past two decades, however, has been inspired by the EU Council Directive 96/82/EC, also known as the Seveso II Directive. Article 12 of the Seveso II Directive explicitly requires the EU countries to consider LUP for the limitation of the impact of major accidents on residential areas, areas of public use (e.g., schools, airports, stadiums), and areas of particular natural sensitivity and interest (e.g., government buildings, landmarks) (Christou et al., 2006).1

As shown in Fig. 1, LUP can be considered as a safety element in addition to safe technology, safe management, and emergency planning (Christou and Poerter, 1999, Christou et al., 2006). Safe technology and safe management are mainly aimed at preventing or reducing the probability of major accidents. LUP and emergency management, on the other hand, are aimed at controlling or limiting the consequences of major accidents by decreasing the exposure of the public to dangerous amounts of heat radiation, overpressure, or toxic gas concentration generated by major accidents.

Article 12 has been aimed at setting criteria for (i) the sitting of new installations or the development of existing installations considering nearby existing land developments, and (ii) land developments in the vicinity of existing installations, particularly those developments which would increase either the number or the vulnerability of population at risk. A vast majority of previous work has been devoted to realize the second requirement; that is, based on the calculated external risks, the land in the vicinity of chemical plants were allocated to particular developments (e.g., to build factories, residential houses, schools) according to their vulnerability and the level of risk (Papazoglou et al., 1998, Laheij et al., 2000, Franks, 2004, Hauptmanns, 2005, Kontic and Kontic, 2009, Taveau, 2010, Cozzani et al., 2014). On the other hand, only a few attempts have been performed to address the first requirement, i.e., considering LUP in the development of existing chemical plants (Papazoglou et al., 2000, Sebos et al., 2010) or in the design (or sitting) of new chemical plants (Bernechea and Arnaldos, 2014, Khakzad and Reniers, 2015a).

In the present study, we have introduced a methodology for cost-effective allocation of safety measures in new or existing chemical plants so that not only the LUP requirements can be met but also the level of internal risk can be mitigated. For this purpose, first we use Bayesian network (BN) both to model accident scenarios (including domino effects) and to estimate internal and external risks. The developed BN is then extended to a limited memory influence diagram (LIMID) by adding decision and utility nodes. As a result, both risk analysis and decision making can be performed using the same framework. Considering the cost of safety measures along with their impacts on the amounts of the internal and external risks, the developed LIMID can be used for a cost-effective allocation of safety measures in chemical plants. The outcome of such a cost-effective safety analysis will be an optimal determination of the number and location of safety barriers given a limited budget.

In the next section, the fundamentals of LUP and adapted approaches in chemical plants, particularly the risk-based approach on which the present work is based, are described. After a brief description of BN and LIMID in Section 3, the developed methodology and its application to accident modeling, risk analysis, and decision making will be illustrated through the cost-effective fireproofing of fuel storage plants. The conclusions are presented in Section 5.

Section snippets
Land use planning
Several methods have been adapted around the world for LUP: (i) risk-based method, (ii) consequence-based method, and (iii) method of generic distances. These methods are not necessarily contradictory, and in most cases a combination of them are employed. For example in the UK, the consequence-based approach is applied to leakage of toxic gases while the risk-based approach is employed to fire as the dominant accident scenarios in chemical plants (Franks, 2004). Comprehensive reviews and

Bayesian network
Bayesian network (BN) is a directed acyclic graph (DAG) for knowledge elicitation and reasoning under uncertainty (Pearl, 1988), with a wide variety of applications in risk, safety, and reliability analysis of dependent and complex systems (Khakzad et al., 2011, Khakzad et al., 2013a, Khakzad et al., 2013b, Khakzad et al., 2013c, Khakzad et al., 2013d, Weber et al., 2012). BN takes advantage of a flexible graphical structure to represent the (causal) relationships among the components of a

Methodology
To both develop the methodology and demonstrate its application, consider a hypothetical fuel storage plant (Fig. 5) which is planned to sit near a residential area and a hospital.

The distances from the center of the plant to the residential area and the hospital are 100 m and 150 m, respectively. Furthermore, the plant is required to store 24,000 m3 of crude oil equally in four similar 6000 m3 atmospheric storage tanks as shown in Fig. 5. The storage tanks have a diameter of 30 m and height of 10 m, 

Conclusion
In this study we illustrated an application of limited memory influence diagram (LIMID) to multi-attribute decision analysis. To demonstrate the application of the methodology, we employed LIMID to cost-effective allocation of safety measures (fireproofing of storage tanks in this study); for this purpose, the cost of fireproofing, risk of damage to the storage tanks (internal risk) and individual risks (probability of death) at off-site targets (external risks) were considered as the decision","This study presents a methodology for the cost-effective allocation of safety measures in chemical plants, with a focus on land-use planning (LUP) requirements and risk mitigation. The methodology employs Bayesian networks (BNs) to model accident scenarios, estimate internal and external risks, and extend the BN to a limited memory influence diagram (LIMID) for decision-making. By considering the cost and impact of safety measures, the approach optimizes the allocation of safety barriers within a limited budget. The study demonstrates the application of this methodology to fireproofing a hypothetical fuel storage plant near residential areas and hospitals, addressing both LUP requirements and internal risk mitigation.





"
"The Exactness of the 
 Penalty Function for a Class of Mathematical Programs with Generalized Complementarity Constraints
Author links open overlay panelYukuan Hu a b, Xin Liu a b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.fmre.2023.04.006
Get rights and content
Under a Creative Commons license
open access
Abstract
In a Mathematical Program with Generalized Complementarity Constraints (MPGCC), complementarity relationships are imposed between each pair of variable blocks. MPGCC includes the traditional Mathematical Program with Complementarity Constraints (MPCC) as a special case. On account of the disjunctive feasible region, MPCC and MPGCC are generally difficult to handle. The 
 penalty method, often adopted in computation, opens a way of circumventing the difficulty. Yet it remains unclear about the exactness of the 
 penalty function, namely, whether there exists a sufficiently large penalty parameter so that the penalty problem shares the optimal solution set with the original one. In this paper, we consider a class of MPGCCs that are of multi-affine objective functions. This problem class finds applications in various fields, e.g., the multi-marginal optimal transport problems in many-body quantum physics and the pricing problems in network transportation. We first provide an instance from this class, the exactness of whose 
 penalty function cannot be derived by existing tools. We then establish the exactness results under rather mild conditions. Our results cover those existing ones for MPCC and apply to multi-block contexts.

Graphical abstract

Download : Download high-res image (198KB)
Download : Download full-size image

Keywords
mathematical program with generalized complementarity constraints
 penalty functionexact penaltymulti-affine objective functionerror bound
2010 MSC
65K0590C2690C3090C33
1. Introduction
A Mathematical Program with Complementarity Constraints (MPCC) takes the form
(1)
where 
, 
, 
, and 
. MPCC has found wide applications in economics and engineering design. For a review on this topic, interested readers may refer to [1], [2] and the references therein. If 
 and 
 are affine, the MPCC (1) reduces to a Linear Program with Complementarity Constraints (LPCC). For problems with multiple variable blocks, the complementarity relationships imposed between each block pair lead to the following Mathematical Program with Generalized Complementarity Constraints (MPGCC)
(2)
where 
 for 
, 
, and 
.

It is worth mentioning that MPCC (1) and MPGCC (2) violate standard constraint qualifications at any feasible point, such as the Mangasarian-Fromovitz constraint qualification (MFCQ) [3]. Consequently, the local solutions, and of course optimal solutions, do not necessarily satisfy the associated Karush-Kuhn-Tucker conditions, which renders these problems particularly difficult to cope with. It can also be proved that globally solving a general MPCC or MPGCC is NP-complete [4], [5], [6]. To this end, several tailored constraint qualifications and stationarity notions have been proposed for MPCC; see [3], [7], [8], [9], [10], [11], [12] for example.

Instead of facing the original MPCC (1) or MPGCC (2), one could alternatively penalize the (generalized) complementarity constraints using 
 penalty term, and then turn to consider the penalty problem. Taking the MPGCC (2) as an instance, we write its 
 penalty counterpart as
(3)
where 
 refers to the penalty parameter and the 
 penalty term
(4)
Due to the nonnegative constraints, (3) is equivalent to
(5)
Since (5) is free of complementarity constraints, standard constraint qualifications may readily hold. Nevertheless, it remains unclear about the exactness of the 
 penalty function, i.e., whether or not the optimal solution sets of (2) and (5) (or (3)) coincide for all sufficiently large 
. Note that penalty terms of other types are also available, some of which favor exactness under mild conditions [13], [14]. Nevertheless, they are either nonsmooth or unstable, and hence are not as preferable as the 
 penalty term from the computational aspect.

In this paper, we consider a class of MPGCC as follows
(P)
Here, 
 is multi-affine, i.e., for 
, it is affine with respect to 
 after fixing the other 
 blocks. The sets 
 are polyhedrons in 
. In the sequel, we denote the feasible region of (P) by 
 and let 
, 
 (
) for brevity.

The relationships among the general LPCC, MPCC, MPGCC as well as the scope of the model (P) are depicted in Figure 1. The cyan ellipsoid with solid boundary stands for MPGCC, the larger cyan disk with dashed boundary for MPCC, and the smaller cyan disk with dotted boundary for LPCC. The red ellipsoid with dashdotted boundary refers to the scope of the model (P).

Fig. 1
Download : Download high-res image (75KB)
Download : Download full-size image
Fig. 1. Relationships among the general LPCC, MPCC, MPGCC as well as the scope of (P).

The model (P) can be found in various fields. In [15], [16], the authors formulate the discretized multi-marginal optimal transport problems arising in quantum physics under the so-called Monge-like ansatz into an MPGCC as (P). The generalized complementarity constraints are present to prevent the unfavorable clustering of electrons. As an LPCC or MPCC, (P) is able to model sequential decision processes such as pricing in network taxation [17], [18], [19], [20], biofuel production [21], airline industry [22], and telecommunication services [23], [24], [25].

Similar to (3) and (5), the 
 penalized (P) is
(6)
which is equivalent to

In the sequel, we focus on 
 and our results apply to (6) as well. Note that 
 remains to be multi-affine. We denote the feasible region of 
 as 
, where “
” stands for the Cartesian product among sets.

The aim of this paper is to establish the exactness of the 
 penalty function for (P) via exploring the relationship between the optimal solution sets of (P) and 
. The global optimization algorithms for (P) and 
 go beyond the range of this work.

1.1. Literature Review
We review in this part the literature on the exactness of the 
 penalty function for MPGCC. These existing results can be divided into two parts: one is devoted to the general MPCC (1), while the other focuses on the special cases of the model (P). The limitations are gathered at the end of each part, from which we draw our motivation.

For the general MPCC (1), one could establish the exactness of the 
 penalty function by imposing additional regularity assumptions. In [1], [14], the authors analyze the exactness with the aid of strict complementarity condition and the following error bound
(7)
where 
 and 
 refer to the feasible regions of (2) and (3), respectively. Here, the strict complementarity condition means that 
 for any 
. Later on, the authors of [26] establish the exactness under the so-called positive-multiplier nondegeneracy condition and the MFCQ of the penalty problem. The above mentioned regularity assumptions may appear to be restrictive. The strict complementarity condition in [1], [14] can easily fail for a general MPCC [15], [26]. Moreover, it is nontrivial to check priorly whether the nondegeneracy condition in [26] holds at the points of interest. As for the general MPGCC (2), the theoretical properties of the 
 penalty function have not yet been investigated.

There are also works dedicated to the 
 exact penalty for (P) with affine objectives and two variable blocks. The authors of [27], [28] show the exactness based upon the finiteness of extreme point sets; see also earlier works [19], [20], [29], [30], where any optimal solution of (P) is proved to solve 
 but the reverse direction is ignored. All the works just mentioned concentrate on (P) with affine objectives and two variable blocks, while no theoretical results are known for the 
 cases or when the objectives are nonlinear. Nevertheless, the authors of [15] report rather encouraging numerical results in solving an MPGCC as (P) via the 
 penalty method.

To sum up, the limitations of the existing works motivate us to pursue the 
 penalty exactness result on (P) with arbitrary 
 and nonlinear objectives under weaker assumptions than those in [1], [14], [26].

1.2. Contributions
We provide an example of (P), the exactness of whose 
 penalty function cannot be implied by the existing results. Leveraging the special structure of (P), we show the exactness of the 
 penalty function under a rather mild assumption. Our results cover those for LPCC in [19], [20], [27], [28], [29], [30] and apply to the multi-block settings with nonlinear objectives. For a view of our position, please refer to the red ellipsoid in Figure 1.

1.3. Notations and Organization
We denote scalars, vectors, and matrices by lower-case letters, bold lower-case letters, and upper-case letters, respectively. The notations “
” and “
” stand for the all-one vector and identity matrix in proper dimension, respectively. The support of a matrix 
 is presented by 
. Given a matrix 
, the inequality “
” (resp. “
”) implies 
 (resp. 
) for any 
. We use “
” to vectorize matrices by column stacking. The Kronecker product between two matrices is denoted by “
”. The operator “
” represents the standard inner product of two vectors or matrices, while “
” represents the induced norm.

We denote respectively the extreme point sets of 
 by 
 for 
, the extreme point set of 
 by 
, the optimal solution set of (P) by 
, and the optimal solution set of 
 by 
 for any 
. Let 
 and 
 denote the extreme-point optimal solution sets of (P) and 
, respectively. The cardinality of a set 
 is represented by 
. We present the Cartesian product among sets by “
” or power exponents (e.g., 
). The line segment connected by two points, 
 and 
, is denoted by 
. The notation “
” refers to the relative boundary of a set 
. The distance between a point 
 and a closed set 
 is represented by 
.

We organize this paper as follows. The example of (P) falling outside the literatures is detailed in section 2. The main results of the 
 penalty exactness on (P) are elaborated in section 3. We draw conclusions and perspectives in section 4.       ","This paper addresses the exactness of the penalty function for a class of Mathematical Programs with Generalized Complementarity Constraints (MPGCCs) with multi-affine objective functions. MPGCCs encompass problems in various fields, including quantum physics and network transportation pricing. The study establishes the exactness of the penalty function under mild conditions, extending previous results for MPCCs and applying to multi-block contexts. The research contributes to understanding and solving challenging mathematical programs with complementarity constraints, making progress in areas where existing methods had limitations.





"
"Abstract
We consider a class of mathematical programs with complementarity constraints (MPCC) where the objective function involves a non-Lipschitz sparsity-inducing term. Due to the existence of the non-Lipschitz term, existing constraint qualifications for locally Lipschitz MPCC cannot ensure that necessary optimality conditions hold at a local minimizer. In this paper, we present necessary optimality conditions and MPCC-tailored qualifications for the non-Lipschitz MPCC. The proposed qualifications are related to the constraints and the non-Lipschitz term, which ensure that local minimizers satisfy these necessary optimality conditions. Moreover, we present an approximation method for solving the non-Lipschitz MPCC and establish its convergence. Finally, we use numerical examples of sparse solutions of linear complementarity problems and the second-best road pricing problem in transportation science to illustrate the effectiveness of our approximation method for solving the non-Lipschitz MPCC.  ","This paper addresses mathematical programs with complementarity constraints (MPCC) that involve non-Lipschitz sparsity-inducing terms in the objective function. Existing constraint qualifications for locally Lipschitz MPCC may not guarantee necessary optimality conditions for local minimizers due to the non-Lipschitz term. The paper introduces necessary optimality conditions and tailored qualifications for such non-Lipschitz MPCCs, ensuring that local minimizers satisfy these conditions. An approximation method for solving these non-Lipschitz MPCCs is presented, along with its convergence analysis. Numerical examples involving sparse solutions and road pricing problems demonstrate the effectiveness of the proposed approximation method.





"
"User
Abstract
The proximal alternating linearized minimization (PALM) method suits well for solving block-structured optimization problems, which are ubiquitous in real applications. In the cases where subproblems do not have closed-form solutions, e.g., due to complex constraints, infeasible subsolvers are indispensable, giving rise to an infeasible inexact PALM (PALM-I). Numerous efforts have been devoted to analyzing the feasible PALM, while little attention has been paid to the PALM-I. The usage of the PALM-I thus lacks a theoretical guarantee. The essential difficulty of analysis consists in the objective value nonmonotonicity induced by the infeasibility. We study in the present work the convergence properties of the PALM-I. In particular, we construct a surrogate sequence to surmount the nonmonotonicity issue and devise an implementable inexact criterion. Based upon these, we manage to establish the stationarity of any accumulation point, and moreover, show the iterate convergence and the asymptotic convergence rates under the assumption of the Łojasiewicz property. The prominent advantages of the PALM-I on CPU time are illustrated via numerical experiments on problems arising from quantum physics and 3-dimensional anisotropic frictional contact.","This study investigates the convergence properties of the Infeasible Inexact Proximal Alternating Linearized Minimization (PALM-I) method. PALM-I is used to solve block-structured optimization problems with complex constraints. The paper addresses the nonmonotonicity issue caused by infeasibility and establishes stationarity, iterate convergence, and asymptotic convergence rates under the Łojasiewicz property assumption. Numerical experiments demonstrate PALM-I's advantages in terms of CPU time in quantum physics and anisotropic frictional contact problems.





"
"Integrating machine learning and mathematical programming for efficient optimization of operating conditions in organic Rankine cycle (ORC) based combined systems
Author links open overlay panelJianzhao Zhou a, Yin Ting Chu a, Jingzheng Ren a, Weifeng Shen b, Chang He c
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.energy.2023.128218
Get rights and content
Abstract
Operations optimization in an organic Rankine cycle (ORC) based combined system is important while computationally difficult by using mechanistic models due to complex nonlinearities and constraints. In this study, a hybrid framework integrating machine learning and mathematical programming has been proposed to optimize the operations of the system for the best exergy performance. The combined system is first decomposed into two single ORCs for reducing computational complexity. Classification models and regression models based on artificial neural network (ANN) and linear regression are developed using simulation data, where classifications can be employed for high-throughput screening feasible inputs which meet the mechanistic constraints in ORC. The results demonstrate high performances of machine learning with at least 99% accuracies for classifications and with mean relative errors of less than 1% for regressions. These data-driven models and the relation of two ORCs were then embedded with mathematical programming for optimization and maximum net exergy of 28.66 MW is obtained. By linear expansion of ReLU operators in ANN, mixed-integer linear programming (MILP) based on machine learning models achieve high efficiency with ∼0.1 s required for optimization compared to mixed-integer nonlinear programming (MINLP) (>1000 s) and heuristic optimization based on mechanistic models (>10 h).

Introduction
The industrial sector accounts for approximately 50% total world's energy consumption [1] and is responsible for 36% of energy-related greenhouse gas emissions [2]. In industrial production, over half of fuel energy was estimated to be wasted as heat, which not only exacerbates energy shortages but causes environmental thermal pollution [3]. Therefore, waste heat recovery has intensively attracted researchers' attention as the most prospective technology for sustainable energy conservation [4,5]. The vast majority of waste heat is low-grade and its utilization is challenging due to economic and environmental considerations associated with working fluids [6]. Organic Rankine cycle (ORC) was proposed to be used to recycle the low-temperature heat in 1996 [7] by utilizing organic working fluids with lower boiling points compared to water. It has emerged as one of the most promising technologies for thermal-to-power conversion [[8], [9], [10]] due to its compact structure, medium operating conditions, and low-sealing requirements as well as low operating and maintenance costs [3,11,12]. The average thermal efficiency of ORC systems ranges from 2% to 19%, while the exergy efficiency typically falls between 30% and 50% [[13], [14], [15]].

To further improve the energy efficiency of ORC, extensive research and development efforts have been conducted in recent decades focusing on cycle configuration [16], working fluid screening [17] and process optimization [18]. Different structures of ORC have been designed and developed ranging from simple ORC systems [9,19] to dual-loop ORC systems [20] and regenerative ORC systems [21]. In addition, working fluid selection has been actively researched as it has a significant impact on thermal efficiency. At the early stage, Hung et al. demonstrated wet fluids were better than dry fluids for increasing the system efficiency by comparing various working fluids such as benzene, ammonia, R11, R12, R134a and R113 [22]. More recently, specific working fluids like R1233zd(E) have been identified as suitable alternatives to enhance ORC system performance by experimental and numerical analysis [23,24]. Given an ORC system and the employed working fluids, process operation optimization also plays a critical role in efficiency improvement. For example, increasing pump pressure consumes more pump power but also increases the power generation in the expander. Analysis at various reference temperatures and pressures was conducted and results showed that high turbine inlet pressure positively increased the system thermal efficiency while turbine inlet temperature has little influence [25]. Furthermore, Dai et al. conducted parametric optimization by adapting a genetic algorithm to achieve the best exergy performance of ORC [26]. Their studies selected the turbine inlet pressure and temperature as objective operations while keeping waste heat source temperature and mass flow rate, ambient temperature, evaporator pitch temperature, and condensing temperature at constants. The effects of operations such as working fluid mass flow rate and expander inlet pressure on the efficiency of a simple ORC were also investigated by detailed modeling [27]. With various ORC systems and working fluids, it remains a challenge to effectively and efficiently find a set of operating parameters that maximize energy recovery efficiency.

Two primary methods are commonly used for optimizing operations in chemical processes: mechanism-based methods and data-driven methods [28]. Mechanism-based models considering material balance, energy balance, equilibrium, summation, and hydraulic performance have been implemented in commercial simulators like Aspen Plus, gPROMS, Aspen Hysys, Pro-II [29]. These models may involve numerous nonlinearities, making mathematical programming methods challenging to implement for optimization. As a result, non-derivative heuristic algorithms become attractive. By linking an external genetic algorithm to Aspen Hysys, Ibrahim et al. optimize the operational variables in a crude oil processing system [30]. Similarly, Gomez-Castro et al. combined a genetic algorithm written in MATLAB and rigorous simulations in Aspen Plus to optimize the configurations of dividing wall columns [31]. With process models developed in simulators, particle swarm optimization was also widely adopted for operations optimization [32,33]. The other method is applying emerging machine learning to construct the relationship between process operations and objective variables to reduce the computational complexity and then optimization can be conducted based on these surrogate machine learning models. Fang et al. established models for predicting yield and annual profit of propane dehydrogenation process by the K-nearest neighbors, decision tree, support vector machine, and artificial neural network based on data generated from first-principle simulation [34]. By selecting robust models, particle swarm optimization (PSO) is used to optimize process operations for maximum yield and annual profit. A similar optimization framework was also used in finding an operating point, resulting in a high CO2 capture rate of 72.2% and low specific reboiler duty with 4.3 MJ/kg [35]. Though heuristic algorithms have demonstrated remarkable performance in solving optimization problems as mentioned above, it is no denying the fact that numerous functions evaluations are necessary and this method lacks deterministic convergence proof [36]. It has been proven that large-scale instances of complex problems can be effectively solved by mixed-integer linear programming [37,38]. More recently surrogate machine learning models were embedded into mathematical programming for optimizing the natural gas liquefaction process and this method surpassed well-established meta-heuristics from 13.57% to 53.26% [36].

In addition to the high computational cost from mechanistic models, constraints like unallowed temperature cross in the evaporator and strict phase requirement in the turbine and pump hinders the search of optimum operation points even the feasible operations for optimizing the operating parameters of ORC system. This makes the optimization more difficult and time-consuming. In this study, a novel framework integrating machine learning and mathematical programming has been developed and validated by optimization of operating conditions in an ORC-based combined system which is appropriate for recovering multi-waste heat [[39], [40], [41]]. Exergy performance, as a measure of how effectively energy is converted into useful work, allows a more comprehensive assessment of the system's thermodynamic efficiency by considering the quality of enegy. It has been widely used to evaluate the energy utilization of ORC-based power systems [42,43], providing valuable insights into the potential for energy savings and cost optimization. Hence, the exergy performance of ORC-based combined system is selected as the objective variable here. The combined system was first decomposed into individual ORCs and their simulation process models were developed in Aspen Plus. Collecting a small amount of data from simulations, data-driven classification models were established for identifying feasible inputs of different ORCs. Then high-throughput inputs screening and process simulation were conducted to generate data for regression models training. These data-driven models (classification and regression) and constraints from relations of different ORCs were integrated into a mathematical programming model. Mix-integer linear programming (MILP) and mix-integer nonlinear programming (MINLP) problems were designed and solved in GAMS to obtain the operation solution for the best exergy performance. The efficiency and effect of mathematical programming were finally compared with heuristic optimization based on mechanistic models.","This study presents a novel approach that combines machine learning with mathematical programming to optimize the operating conditions of organic Rankine cycle (ORC) based combined systems. These systems are challenging to optimize due to their complex nonlinearities and constraints. The approach decomposes the combined system into individual ORCs and uses classification and regression models based on artificial neural networks to identify feasible inputs. These data-driven models, along with mathematical programming, are then used to optimize the system for maximum net exergy. The results demonstrate the effectiveness of this approach, with significant reductions in CPU time compared to heuristic optimization based on mechanistic models, making it a promising method for efficient ORC system optimization.





"
"2. Materials and methods
2.1. Materials
Gelatine and methacrylic anhydride (MA) were purchased from Macklin Chemical Reagent Co., Ltd. (Shanghai, China). Chitosan (CS) and glycidyl trimethylammonium chloride (GTMAC) were obtained from Sigma‒Aldrich Biochemical Co., Ltd. (Shanghai, China). The photo-initiator 2‑hydroxy-4′-(2-hydroxyethoxy)−2-methylpropiophenone (L2959), paraformaldehyde, paraffin, isoflurane, agarose, peptone, yeast extract, and NaCl were purchased from Sinopharm Co., Ltd. (Shanghai, China). Mouse lung fibroblasts (L929), human umbilical vein endothelial cells (HUVECs), and rat bone marrow mesenchymal stem cells (BMSCs) were kindly provided by the Medical Research Institute (MRI), Zhongnan Hospital of Wuhan University. Escherichia coli (E. coli) and Staphylococcus aureus (S. aureus) were purchased from China General Microbiological Culture Collection centre (Beijing, China). Dulbecco's modified Eagle's medium (DMEM), Roswell Park Memorial Institute-1640 (RPMI-1640) medium, foetal bovine serum, normal saline, trypsin solution, and phosphate-buffered saline (PBS) were purchased from Thermo-Fisher Scientific Co., Ltd. (Waltham, USA). A polydimethylsiloxane (PDMS) mould was customized by Zhongding Yuxuan New Material Technology Co., Ltd. (Anhui, China). A live/dead cell staining kit was purchased from Dojindo Co., Ltd. (Shanghai, China). Other chemicals were used without further purification.

2.2. Preparation of GelMA/QCS composite hydrogel
The GelMA was synthesized and characterized according to previous literature [39], and QCS was also synthesized and characterized following published literature [40] (see the Supplementary Material). Then, the obtained GelMA and QCS were dissolved in deionized water to prepare 20% GelMA and 2% QCS solutions. These solutions were blended at a specific proportion. The photo-initiator L2959 was added to the mixed solution to reach a final concentration of 0.05%. After centrifuging at 3000 rpm to remove air bubbles, the obtained solution was poured into a mould and cross-linked by UV irradiation at a power of 500 W for 180 s. Thus, the prepared GelMA and QCS composite hydrogels were coded as GHCH-n (n = 0, 5, 10, and 15, corresponding to the weight percentage of QCS). The codes and composites of GHCH-n are shown in Table 1.

Table 1. The codes and compositions of the GelMA/QCS composite hydrogels.

Empty Cell        20% GelMA
(g)        2% QCS
(g)        5% L2959
(g)        UV power
(W)        UV time
(s)
GHCH-0        100        0        1        500        180
GHCH-5        65.5        34.5        1        500        180
GHCH-10        47.3        52.7        1        500        180
GHCH-15        34.8        65.2        1        500        180
2.3. Fabrication of GHCH-10 MNPs
Microneedle patches (MNPs) were fabricated using a polydimethylsiloxane (PDMS) mould with a centre-to-centre interval of 700 µm and sharp needles tapering to a 10 µm radius and an 800 µm height. These needles were arranged in a 14 × 14 array at the base. The GHCH-10 as-gelled solution was added to the PDMS mould, and a pumping vacuum was applied for at least 5 min. After that, the samples were irradiated with UV at a power of 500 W for 180 s. The resultant GHCH-10 MNPs were detached from the mould for further use.

2.4. Physiochemical characterizations
The morphology of the GHCH-n hydrogels and GHCH-10 MNPs was observed using a scanning electron microscope (SEM, SIGMA 500, Zeiss, USA). The chemical structures of GHCH-n hydrogels and raw materials were identified by Fourier transform infrared spectroscopy (FT-IR, Nicolet iS50, Thermo-Fisher, USA), X-ray diffraction (XRD, D8 advance, Bruker, Germany) and solid-state 13C NMR (Bruker Avance III 500 MHz). For FT-IR analysis, the wavenumbers used ranged from 4000 to 400 cm−1. For XRD analysis, the diffraction angles used ranged from 5° to 60°. For 13C NMR analysis, the chemical shift used ranged from 235 to −70 ppm. For the rheology test of the hydrogels, oscillation test and viscosity test were performed (Discovery DHR-2 (USA)) (oscillation-frequency scanning: 25 mm plate, 20 ℃ at constant 1% strain angular frequency: 1–100 s−1; flow-shear scanning: 25 mm plate, 20 ℃ at the shear rate: 1–100 s−1). The swelling kinetics of the GHCH-n hydrogels were detected following a previous report [41]. The compression tests were carried out using a universal material testing machine (HD-B609B-S, Haida, China). The stress‒strain curves and compressive strength at 35% strain were analysed.

2.5. Degradation test in vitro
The GHCH-n hydrogels were cut into pieces for degradation test in vitro. All the samples were freeze-dried and weighted (W1) before use. The samples were then incubated with 10 mL lysozyme solution (5 mg/mL) at 37 ℃. The lysozyme solution was replaced every day. Atregular time intervals, the samples were freeze-dried again and weighed (W2). The degradation rate (DR) of the hydrogels was calculated as follows:

2.6. Biocompatibility evaluation in vitro
2.6.1. Haemolysis test
This study was performed with the approval of the Animal Care & Welfare Committee of Wuhan University. A healthy New Zealand rabbit was obtained from the Experimental Animal centre of Three Gorges University. After anaesthetization by isoflurane inhalation, the animal was fixed onto a corkboard to collect fresh anticoagulant blood, which was then diluted with 0.9% normal saline at a weight ratio of 1:1.25. The GHCH-n hydrogels were incubated with 10 mL normal saline and 200 µL diluted blood at 37 ℃ for 60 min. The positive and negative control groups were set up according to our previous report [42]. All samples were centrifuged at 1500 rpm for 10 min to separate the supernatants. The absorbence of the supernatants at 545 nm was detected by a multifunctional microplate reader (Spectrum M2, MD, USA).

2.6.2. Preparation of GHCH-n extracts
The extracts of GHCH-n hydrogels were prepared under the guidance of ISO10993–12:2007. Briefly, the GHCH-n hydrogels were freeze-dried at −60 ℃ for 48 h and then completely sterilized by ultraviolet (UV) irradiation. The samples (0.2 g) were incubated with an FBS-containing medium at 37 ℃ for 72 h to obtain 1 mL extracts. The extracts were filtered using a 0.22 µm filter and stored at −80 ℃ for further study.

2.6.3. MTT assay
In this study, three different cell lines (L929, BMSC, and HUVEC) were used as model organisms to evaluate the cytocompatibility of the GHCH-n hydrogels. The cells were seeded onto 96-well tissue culture plates at a density of 1.5–3.0 × 103 cells/well. After cell adhesion overnight, the culture medium was replaced with a fresh medium containing 25% of the extracts. The blank control was treated with a culture medium containing no extracts. At regular time intervals, the samples were treated with MTT reagent according to the manufacturer's protocols. The absorbence at 490 nm was detected by a multifunctional microplate reader.

2.6.4. Morphological observation
The cells were seeded onto 6-well tissue culture plates. After cell confluence reached 30%, the culture medium was replaced with a fresh medium containing 25% of extracts and cultured for another 48 h. All samples were rinsed with PBS 3 times and then treated with a live/dead cell staining kit. Live cells were stained with calcein-AM (green), and dead cells were stained with propidium iodide (red). An inverted fluorescence microscope (IX73, Olympus, Japan) was used for data acquisition.

2.7. Biocompatibility evaluation in vivo
Fifteen female Sprague‒Dawley (SD) rats weighing approximately 180 g were obtained from the Experimental Animal centre of Three Gorges University. Before evaluations in vivo, all rats were kept in the specific pathogen-free (SPF) Experimental Animal centre of Wuhan University for at least 7 days to relieve environmental stress. A distinctive intraperitoneal transplantation model was constructed for biocompatibility evaluations in vivo [43]. GHCH-n hydrogels (1 cm × 1 cm) were transplanted into the abdominal cavity of SD rats, followed by conventional feeding for 14 days. The animals were anaesthetized by isoflurane inhalation. Fresh whole blood was collected into pro-coagulation tubes and then centrifuged at 3000 rpm for 10 min to separate the plasma. A series of biochemical indices of plasma were automatically detected by the clinical laboratory. Meanwhile, the GHCH-n hydrogels as well as the organs (heart, liver, spleen, lung, and kidney) were resected and fixed in a 4% paraformaldehyde (PFA) solution. An H&E staining assay was performed to visualize the histopathological characteristics. An inverted fluorescence microscope was used for data acquisition.","The ""Materials and Methods"" section provides a comprehensive overview of the experimental procedures conducted in the study. It includes details on the materials used, the preparation of GelMA/QCS composite hydrogel, fabrication of GHCH-10 MNPs, physiochemical characterizations, degradation tests in vitro, and biocompatibility evaluations both in vitro and in vivo. The section covers various aspects such as material synthesis, characterization techniques, and biological assays, demonstrating a systematic approach to the research. Additionally, animal models are used to assess biocompatibility, further enhancing the depth of the study's analysis and findings.





"
"Neuroimmune interactions and their roles in neurodegenerative diseases
Author links open overlay panelBai-Rong Chen 1, Ting Wu 1, Ting-Hui Chen 1, Yun Wang 1 2
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.fmre.2023.04.002
Get rights and content
Under a Creative Commons license
open access
Abstract
The nervous system possesses bidirectional, sophisticated and delicate communications with the immune system. These neuroimmune interactions play a vitally important role in the initiation and development of many disorders, especially neurodegenerative diseases. Although scientific advancements have made tremendous progress in this field during the last few years, neuroimmune communications are still far from being elucidated. By organizing recent research, in this review, we discuss the local and intersystem neuroimmune interactions and their roles in Alzheimer's disease, Parkinson's disease and amyotrophic lateral sclerosis. Unveiling these will help us gain a better understanding of the process of interplay inside the body and how the organism maintains homeostasis. It will also facilitate a view of the diseases from a holistic, pluralistic and interconnected perspective, thus providing a basis of developing novel and effective methods to diagnose, intervene and treat diseases.

Keywords
the nervous systemthe immune systemneuroimmune interactionsAlzheimer's diseaseParkinson's diseaseamyotrophic lateral sclerosis
1. Introduction
Until recently, the nervous system and the immune system were thought to be two autonomous functional entities that acted independently [1]. However, accumulating evidence has suggested that an intimate crosstalk between the nervous system and the immune system exists.

The nervous system has the capacity to influence the immune system. It can regulate the generation of immune cells by mobilizing hematopoietic stem cells into the blood through glucocorticoids, noradrenaline, and neuropeptide Y, among others [2,3]. By releasing multiple mediators, such as calcitonin gene-related peptide (CGRP), substance P (SP) and TAFA chemokine-like family member 4 (TAFA4), the nervous system can also affect the trafficking and migration of immune cells [4]. To respond to signals derived from the nervous system, immune organs and immune cells also express many kinds of receptors for neurotransmitters, such as adrenergic receptors [5]. Accordingly, via norepinephrine, the sympathetic nervous system (SNS) can regulate many immunological processes by activating different subtypes of adrenergic receptors on immune cells [6,7]. In addition, acetylcholine, dopamine and serotonin can also exert an effect on immune cells (Di Benedetto et al., 2017.).

In turn, neurons also express a variety of immune-related receptors, such as receptors for TNF and IL-1 (Pavlov and Tracey, 2017.). By releasing immune mediators to interact with these receptors, the immune system functions as a regulator of the nervous system. For instance, immune cells can regulate the proliferation, differentiation and migration of neural stem and progenitor cells through cytokines and trophic factors, resulting in alterations in neurogenesis [8]. In addition, in the context of pathological pain, TNF, IL-1β, CCL2 and other immune mediators can powerfully enhance neuronal excitability by strengthening excitatory synaptic transmission and diminishing inhibitory synaptic transmission, contributing to nociceptive hypersensitivity [9].

All the above research implies that the nervous system and the immune system can communicate with each other in distinctive manners, and that they interact in both physiological and pathological states. During inflammation, immune cells release TNF, IL-1, IL-6 and other immune mediators that act on nociceptive sensory neurons, resulting in the generation of pain and its signal transmitted to the central nervous system (CNS) [7,10]. At the same time, these nociceptive sensory neurons secrete several neuropeptides, such as substance P (SP) and vasoactive intestinal peptide (VIP), in turn influencing the functions of immune cells and inflammation [7,10]. To date, a growing body of evidence has demonstrated that these neuroimmune interactions are of vital significance in maintaining homeostasis and play an important role in the initiation and development of many disorders, such as inflammatory bowel diseases [11], asthma [7], pain [9,10] and neurodegenerative diseases [12], [13], [14].

With the advancement of technology, the field of neuroimmune interactions has flourished in recent years and many surprising and exciting breakthroughs have been achieved. However, due to the sophistication of both systems, the neuroimmune dialogs are still complex. The detailed processes and mechanisms underlying neuroimmune interactions remain unclear; thus, more exploration is necessary. Organizing recent research, we highlight the local and intersystem neuroimmune interactions and their roles in Alzheimer's disease (AD), Parkinson's disease (PD) and amyotrophic lateral sclerosis (ALS). Unraveling neuroimmune communications and their effects on diseases will substantially broaden our understanding of the operation inside the body as well as the initiation and development of diseases, providing us with new insights into the diagnoses, interventions and treatments for diseases.

2. Neuroimmune interactions in local tissues
In local peripheral tissues, neurons and immune cells often coexist at defined anatomical locations [3,7,15], where they interact productively with each other. These functional sites have been defined as neuroimmune cell units (NICUs) [3,15], which may be the anatomical bases for neuroimmune interactions in local peripheral tissues. Similar structures can also be found in the CNS. For example, microglia have close contact with the dendrites and synapses of surrounding neurons [12]. Astrocytes can form tripartite synapses with neurons [16]. These formations may establish the structural foundations for local neuroimmune communications.

2.1. Neuroimmune interactions in local peripheral tissues
Neuroimmune interactions in local peripheral tissues consist of communications between neurons in the peripheral nervous system (PNS) and peripheral immune cells. In different peripheral tissues, neuroimmune interactions are diversified, as are their functions and roles in disease.

In the gut, enteric neurons are involved in the regulation of local inflammation [11,17]. Reciprocally, intestinal immune cells and inflammation participate in maintaining homeostasis of the enteric nervous system (ENS) [17]. In the pancreas, the activity of sympathetic nerves is also associated with local inflammation [18]. One example supporting this idea is that sympathetic denervation could halt the immune response in pancreatic islets in RIP-LCMV-GP type I diabetes mice, leading to delayed progression of diabetes [18].

Likewise, in the skin, sensory neurons exhibit bidirectional crosstalk with a wide variety of immune cells, including T cells, neutrophils, mast cells and dendritic cells [15] (Fig. 1). IL-4 secreted from T cells can activate dermal sensory neurons, leading to chronic itch [15]. In turn, signals derived from neurons regulate immune cells. For example, nociceptive sensory neurons can suppress the recruitment of neutrophils and their opsonophagocytic activity via calcitonin gene-related peptide (CGRP) during necrotizing infection [15]. In addition, dermal peptidergic sensory neurons are able to drive mast cell degranulation through substance P (SP), while non-peptidergic sensory neurons inhibit this process via glutamate [5]. These studies suggest that different sensory neurons can interact with different immune cells via diverse mediators. However, their relationships are still elusive, and whether they match by the methods of one-to-one, one-to-many, many-to-one or many-to-many is unknown. It is possible that all of these exist but under different conditions. In addition, it is worth interrogating whether the opposite effects caused by different sensory neurons on the same immunological response can be achieved by altering the contacts between different immune cells rather than releasing different molecules to act on the same immune cell.","This review explores the intricate and bidirectional neuroimmune interactions within the body, shedding light on their significant roles in various disorders, particularly neurodegenerative diseases like Alzheimer's, Parkinson's, and amyotrophic lateral sclerosis (ALS). The communication between the nervous and immune systems is highlighted, revealing their ability to influence each other in physiological and pathological states. These interactions are discussed at both local and systemic levels, emphasizing their potential impact on maintaining homeostasis and their implications for novel diagnostic and therapeutic approaches. However, despite recent advancements, the complexity of these interactions calls for further research to fully understand their mechanisms and consequences in disease progression.





"
"Neuroimmune interactions in the CNS
When compared to local peripheral tissues, the major feature of neuroimmune interactions in the CNS is the crosstalk between glial cells and neurons (Fig. 1). Microglia, the resident immune cells in the CNS, have been implicated in the regulation of neurogenesis and neurodevelopment via cytokines, which can also change the size of the neural progenitor cell pool through phagocytosis and regulate synaptic plasticity as well as the formation of neural circuits through classical complement cascades [19], [20], [21]. Similar to microglia, astrocytes can release a variety of substances, such as ATP, glutamate, D-serine and L-lactate, to modulate the activity of neurons, synaptic transmission and plasticity [16]. Furthermore, astrocytes can use brain-derived neurotrophic factor (BDNF) to orchestrate neuronal network oscillations [16] and mediate adult hippocampal neurogenesis [22]. In addition to acting alone, microglia and astrocytes can also cooperate to act by interacting with each other. IL-1α, TNF and C1q released by activated microglia, for example, can induce the generation of neurotoxic reactive astrocytes [23]. The latter triggers the death of neurons and oligodendrocytes by saturated lipids [24].

As previously stated, microglia and astrocytes can influence the function of neurons. In turn, they also recognize and respond to neuron-derived signals. Activated neurons are able to modify the morphology and function of microglia via ATP [25]. They can also induce focal and rapid depolarizations in peripheral astrocyte processes, which impact the capacity of astrocytes to clear glutamate [26]. Another study also illustrated that the axonal terminals of neurons in the hypothalamic paraventricular nucleus might directly activate oxytocin receptor-expressing astrocytes in the lateral central amygdala by secreting oxytocin [27].

Taken together, glial cells, such as microglia and astrocytes, communicate with neurons bidirectionally in the CNS. These communications are of crucial importance for the proper functioning of the brain. However, our knowledge of how glial cells function in the CNS, especially their interactions with neurons, is still in its infancy. Many unknowns remain to be investigated.

3. Neuroimmune interactions between peripheral tissues and the CNS
Neuroimmune interactions are present not only in local sites but also between peripheral tissues and the CNS. These intersystem neuroimmune interactions are also bidirectional.

3.1. Interactions between peripheral neurons and central immune responses
In neuropathological pain, the process of pain is mediated by local neuroimmune interactions at the site of peripheral nerve injury, which can also induce similar neuroimmune communications in the CNS [9]. It is unknown if these communications can be regarded as the replaying of local neuroimmune interactions in the CNS, and if so, how the interactions in local peripheral tissue are transformed into communications in the CNS. Signals from peripheral tissues can be relayed to the brain by neurons in the PNS. Therefore, peripheral neurons are likely to act as carriers of these interactions. In the case of peripheral nerve injury, it seems that injured afferent sensory neurons release chemokines, ATP and other molecules in their innervation sites in the CNS, followed by the activation of microglia, which then activate astrocytes and induce the infiltration of peripheral immune cells via a variety of immune mediators [9]. This suggests that immune signals in peripheral tissues can be converted into immune signals in the CNS by afferent neurons, i.e., “immune signals-neural signals-immune signals”. In this process, immune responses in peripheral tissues activate neurons (i.e., immune signals are transformed into neural signals). Through neural transmission, these signals are relayed to the CNS, where the activated neurons release related factors to replay immune responses (i.e., neural signals are transformed into immune signals). This may imply that neural signals can be converted into immune signals and vice versa.

In addition to causing the infiltration of peripheral immune cells through immune mediators, activated peripheral neurons can directly open a path for peripheral immune cells to reach the CNS, providing another way to induce central immune responses, thus creating a vicious cycle. For instance, the activation of sensory neurons in soleus muscles led to alterations in dorsal blood vessels in the fifth lumbar cord through sympathetic nerves, which opened a gateway for transferred pathogenic CD4+ T cells to migrate into the CNS, contributing to the development of experimental autoimmune encephalomyelitis (EAE) [28]. This pathway for peripheral immune cells to enter the CNS needs the help of peripheral nerves and local vessels. Moreover, central neurons may also be involved in this process. This idea has been supported by a study showing that pain induction by ligating the middle branch of the trigeminal nerves activated sensory neurons, and the signals were then relayed to anterior cingulate cortex (ACC) and in turn descended via sympathetic nerves, which mediated the accumulation of MHC class II+ CD11b+ cells in the ventral vessels of the fifth lumbar cord and further recruited multifarious immune cells, including pathogenic CD4+ T cells, resulting in the relapse of EAE [29].

Taken together, the activation of peripheral neurons appears to be able to affect central immune responses. This raises the question of whether central immune responses can in turn influence peripheral neurons. Central immune responses may, in theory, alter the activity of neurons in the CNS, which then relay the signals to peripheral neurons and modulate them via efferent nerves. In addition, it is still poorly understood whether the effects of peripheral neurons on central immune responses can be achieved directly through their secretions crossing the blood-brain barrier and functioning in the CNS. Further studies are warranted to fully elucidate these processes.                  ","Neuroimmune interactions within the central nervous system (CNS) primarily involve bidirectional communication between glial cells (microglia and astrocytes) and neurons. These interactions play a pivotal role in regulating neurogenesis, synaptic plasticity, and neural circuit formation. Glial cells can release various substances to modulate neuronal activity, while neurons can influence glial cell morphology and function. Furthermore, neuroimmune crosstalk extends to interactions between peripheral tissues and the CNS, with peripheral neurons serving as potential carriers of immune signals to the brain. This complex interplay highlights the importance of understanding the mechanisms by which immune signals are transformed into neural signals and vice versa in both local and systemic contexts.





"
"Local neuroimmune interactions and neurodegenerative diseases
4.1.1. Alterations in peripheral immune responses
During the process of AD, the populations of peripheral monocytes and their gene expression change [42]. For example, the expression of proinflammatory genes for IL-6, IL-1β, NLRP3, TNF, IL-18 and others was decreased in the prodromal stage but significantly increased in the advanced disease stage [42]. In addition, the number and inhibitory function of myeloid-derived suppressor cells (MDSCs) are augmented in the prodromal stage of AD but reduced in the later stage [42]. In PD, peripheral monocytes are pathologically hyperactive and possess a proinflammatory predisposition (Grozdanov et al., 2014.). It was demonstrated that a lower quantity of lymphocytes in circulating blood was linked to an increased risk for PD (Jensen et al., 2021.). Naive CD4+ and CD8+ T cells as well as naive B cells in peripheral blood are reduced among early-stage PD patients, while the number of central memory CD4+ T cells, IL-17-producing CD4+ Th17 cells, IL-4-producing CD4+ Th2 cells, IFN-γ-producing CD8+ T cells and TNF-α-producing CD19+ B cells are increased (Yan et al., 2021.). In addition, immature transitional B cells and follicular T cells are also reduced and produce a proinflammatory profile among PD patients (Li et al., 2022.). At the same time, the interactions between follicular helper T cells and B cells are aberrant (Li et al., 2022.). Similar to AD and PD, the subtype distribution, gene expression signature and function of peripheral monocytes also change in patients with ALS [43]. For instance, CD16− monocytes are decreased among ALS patients [44]. In addition, circulating neutrophils are augmented [44]. It has been shown that a higher neutrophil count in peripheral blood was significantly relevant to shorter survival among ALS patients (Murdock et al., 2021.), and the ratio of neutrophils to CD16− monocytes, which was dramatically increased in ALS, was closely associated with the progression of the disease [44]. Moreover, the inhibitory function of regulatory T lymphocytes in peripheral blood is also abnormal in ALS patients (Beers et al., 2017.).

Taken together, these studies imply that there are tremendous alterations in the peripheral immune system in the progression of these neurodegenerative diseases. How do these changes interact with the nervous system to take effect in these disorders? Existing studies suggest that they may participate in this process by influencing intersystem neuroimmune interactions, such as by the means of infiltrating the CNS or secreting immune mediators to cross the blood-brain barrier and function in the CNS, which can give rise to neurotoxicity as well as neuroinflammation [45]. For example, in AD, neutrophils can enter the brain via LFA-1 integrin attachment [46] and cause toxicity directly to neurons by releasing IL-17 and neutrophil extracellular traps (NETs) [45,47]. And in ALS, elevated proinflammatory cytokines and chemokines in the periphery, such as TNF-α, IL-1β and IL-6, which may be the results of dysregulation of the peripheral immune system, can alter the function of resident cells and exacerbate neuroinflammation in the brain after they enter the CNS [48]. However, the question of how the changes of peripheral immune responses alter local peripheral neuroimmune interactions to affect the progression of neurodegeneration is still unclear. These changes are likely to be the bases of the alterations of local peripheral neuroimmune interactions in these diseases. This still needs further research to take deep insight into.

As mentioned above, the number, type and function of immune cells and immune mediators in peripheral tissues change markedly during the progression of neurodegenerative diseases, which leads to prominent alterations in peripheral immune responses. These alterations may promote the development of the diseases and hence exacerbate the symptoms. Sommer et al. demonstrated that in vitro, T cells mediated the death of midbrain neurons derived from human induced pluripotent stem cells (hiPSCs) from PD patients via the IL-17-IL-17R signaling pathway [49]. On the other hand, these alterations can also stifle or slow down the progression of diseases, thereby alleviating symptoms. Furthermore, there exist certain resemblances but also differences in the alterations of peripheral immune responses among different neurodegenerative diseases, such as the changes in peripheral monocytes noted above. Whether these similarities are the root causes of the similar symptoms and whether these distinctions, to some extent, determine the specificity of the diseases remains unclear. To further clarify the relationship between the alterations of peripheral immune responses and neurodegenerative diseases, it is essential to systematically detail the changes in immune cells, immune mediators and other components in peripheral tissues in different neurodegenerative disorders and make comparisons among them. Furthermore, despite extensive research into the alterations of peripheral immune responses in several neurodegenerative diseases, a full understanding of how these changes take effect in peripheral tissues to mediate the disorders is still lacking. Nevertheless, targeting these alterations is still able to delay or alleviate the symptoms of neurodegenerative diseases. For example, peripheral administration of IL-33 ameliorated AD-like pathology and rescued cognitive deficits in APP/PS1 mice (Fu et al., 2016.).

4.1.2. Alterations in central immune responses
Both astrogliosis and microgliosis have been observed in the brains of AD, PD and ALS patients [12]. In addition, neurotoxic reactive astrocytes, which can induce the death of neurons in the CNS, were found in the hippocampus and prefrontal cortex of AD patients, the substantia nigra of PD patients and the motor cortex of ALS patients [23]. These findings imply that changes in central immune responses mediated by microglia and astrocytes also have a role in the development of neurodegenerative diseases. In this section, we will focus on AD to examine the effect of central immune response-related changes on the progression of neurodegenerative disorders.

A previous study indicated that microglia could constitute a barrier around amyloid plaques to restrict the expansion and toxicity of the plaques [50]. Similarly, activated phagocytic microglia were also able to prevent the seeding of Aβ [51]. These findings suggest that microglia can prevent neurons from being damaged by toxic factors and thus play a protective role in the process of AD. In contrast, another study demonstrated that microglia could aid in the propagation of Aβ [52]. And Aβ accumulation in the brain may lead to the release of complement C1q from neurons, which can activate its corresponding receptor, C1qR, on microglia and result in synaptic pruning and phagocytosis by microglia, giving rise to neuronal toxicity and death [46]. Moreover, in response to Aβ, microglia can also secrete cysteine protease cathepsin B to cause apoptosis of neurons [53]. The activation of NF-κB signaling pathway in microglia has also been shown to facilitate the seeding and dissemination of tau (Wang et al., 2022.). Tau, in turn, can activate NF-κB signaling pathway in microglia (Wang et al., 2022.), possibly resulting in a vicious cycle. Additionally, in an AD mouse model, aberrant glycolysis in microglia might raise the level of lactate-dependent histone modification, further leading to increased expression of glycolytic genes and exacerbation of microglial dysfunction, which drives the pathology of AD [54]. Interrupting this positive feedback loop could ameliorate neuroinflammation and reverse the cognitive decline in AD [54]. According to these studies, microglia can amplify neuroinflammation as well as the destructive effects of toxic factors and kill neurons via releasing proteases, and thus appear to be detrimental in AD. The results concerning the role of microglia in AD are contradictory in different studies and the real role of microglia in AD remains unclear. The hypothesis that there are two peaks of microglial activation in the course of AD is gaining traction [42,55]. The early activation of microglia may be protective, whereas later activation may be proinflammatory and destructive [42,55]. Therefore, in the research stated above, microglia may be in different phases, resulting in opposite effects. If so, it is important to understand what factors mediate the activation of microglia at different stages and the mechanisms that underpin these factors. Furthermore, another study indicated that microglia surrounding the amyloid plaques were primarily derived from the bone marrow [56]. Microglia of this type were able to eliminate amyloid deposits [56]. This may imply that microglia playing a protective role in the progression of AD are foreign to the brain rather than resident microglia. If so, it will be important to know if microglia activated at different stages of AD come from distinct origins. Providing a comprehensive and detailed description of the changes, subtypes and origins of microglia in the trajectory of AD will assist in better comprehending the roles of microglia in the initiation and development of AD and identifying more effective potential therapeutic targets.

Astrocytes are also involved in the process of AD. It was demonstrated that the expression of α2-Na+/K+ ATPase was increased in astrocytes in the brains of AD patients [57]. Inhibiting this ATPase suppresses neuroinflammation and the accumulation of tau pathology [57]. Additionally, when exposed to Aβ, astrocytes might release complement component C3, which binds to the G-protein-linked receptor C3aR expressed on neurons, contributing to the changes of dendritic morphology and network dysfunction [58]. And inhibition of this complement component may ameliorate cognitive decline and reverse the loss of synapses in AD (Vainchtein and Molofsky, 2020.). Moreover, in AD, dysregulated astrocytes are likely to exert excitotoxic effects on neurons owing to the accumulation of glutamate [58]. In addition, reducing the expression of apoE3 and apoE4 in astrocytes was able to reduce the deposition of amyloid plaques and the activation of microglia around the plaques [59], which implies that astrocytes can harm neurons indirectly and exacerbate the symptoms of AD via the release of apoE3 and apoE4. Taken together, these studies suggest that astrocytes also have a significant importance in the initiation and development of AD and that they can cooperate with microglia to coordinate this process. However, the precise content and the underlying mechanisms of the interactions between astrocytes and microglia in the progression of AD remain unclear and require further investigation.       ","In the context of neurodegenerative diseases such as Alzheimer's disease (AD), there are notable alterations in both peripheral and central immune responses. These changes involve shifts in the populations and gene expression profiles of immune cells in peripheral tissues, including monocytes and lymphocytes. Peripheral immune alterations can influence neuroimmune interactions by potentially infiltrating the central nervous system (CNS) or releasing immune mediators that affect CNS function. Within the CNS, both microglia and astrocytes are implicated in neuroinflammation and neurotoxicity. While some studies suggest that microglia may have protective roles by limiting toxic factors, others indicate their involvement in neuroinflammatory processes. Astrocytes, on the other hand, can contribute to synaptic dysfunction and neurotoxicity in AD. The intricate relationships between peripheral and central immune responses and their impact on neurodegenerative diseases require further investigation for a comprehensive understanding of disease mechanisms and potential therapeutic targets.





"
"vnd wastewater generation in eco-industrial parks. In addition to other studies, they considered both economic costs (including piping, operating costs and treatment costs) and environmental factors (cover the quality and volume of the water) as a functional objective. Leong et al. (2017) integrated the analytic hierarchy process with a multi-objective optimization method to optimize the design criteria for establishing an eco-industrial park. Aside from environmental impact, network connectivity, and reliability, economic criteria have been examined with a case study.

Unlike previous studies, Liu et al. (2018) synthesized an interplant heat integrated water allocation network (HIWAN) for EIPs and achieved optimum HIWAN by using a nonlinear programming model for the network. In this synthesis, sequential and simultaneous optimization routes have been designed. Although there was a lower requirement in sequential design, it has been observed that the results of the simultaneous design were more acceptable in terms of economic performance","The literature review explores the critical relationship between water and energy in industrial processes and the application of mathematical programming methods and pinch analysis for their minimization. It covers studies from 2014 to 2019, focusing on water network synthesis problems and heat integrated water networks. The review highlights the need for accurately calculating the true cost of water, and it discusses various methods for minimizing water and energy consumption, including technology changes, process integration techniques, and hybrid approaches. Additionally, sensitivity analysis is emphasized as a tool for assessing the impact of parameter changes on optimal solutions. The review underscores the importance of integrating water and heat losses into mathematical models for more realistic results and sustainable industrial practices.





"
"Facile fabrication of quaternized chitosan-incorporated biomolecular patches for non-compressive haemostasis and wound healing
Author links open overlay panelZesheng Chen a b 1, Yixuan Zhang c 1, Kexin Feng b, Tao Hu b, Bohan Huang b, Jinlan Tang b, Junjie Ai d, Liang Guo c, Weikang Hu b, Zijian Wang a b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.fmre.2023.05.009
Get rights and content
Under a Creative Commons license
open access
Abstract
Cell-free wound dressings (WDs) with desirable effectiveness and safety have received much attention in the field of regenerative medicine. However, the weak linkages between bioactive polymers and the spatial structure of WDs frequently result in interventional treatment failure. Herein, we create a series of quaternized chitosan (QCS)-incorporated composite hydrogels (referred to as GHCH-n) by UV cross-linking and then convert them into microneedle patches (MNPs). QCS, which is positively charged and amphiphilic, is essential for broad-spectrum antibacterial and haemostatic activities. QCS is proven to be slightly toxic, so it is immobilized into the methacrylate gelatine (GelMA) molecular cage to minimize adverse effects. A polydimethylsiloxane (PDMS) micro-mould is used to shape the MNPs. MNPs can pierce tissue, seal off bleeding sites, and cling to wounds securely. Thus, MNPs can cooperate with GHCH-n hydrogels to halt bleeding and accelerate wound healing. This study recommends GHCH-10 MNPs as an advanced biomaterial. Several preclinical research models have thoroughly validated the application effect of GHCH-10 MNPs. This research also proposes a novel strategy for integrating the nature of bioactive polymers and the structure of composite biomaterials. This strategy is not only applicable to the fabrication of next-generation WDs but also shows great potential in expanding interdisciplinary domains.

Graphical abstract
Image, graphical abstract
Download : Download high-res image (233KB)
Download : Download full-size image

Keywords
Quaternized chitosanPatchesBiocompatibleAntibacterialHaemostaticWound healing
1. Introduction
The skin is the human body's initial line of defence against external threats [1,2]. When the integrity of the skin is destroyed, the wound-healing process immediately starts and usually lasts for days to months [3]. Wound healing is a dynamic and overlapping process that is divided into four phases: haemostasis, inflammation, proliferation, and remodelling [4,5]. It can be easily disturbed by many negative factors, such as secondary injury, bacterial infection, hyperglycaemia, and hypoxia [6,7]. Thus, the clinical needs of wound healing are quite complicated depending on the types of wounds and intervention factors. In recent decades, the fields of biomedical engineering have made great progress in combating skin injury [8], [9], [10]. However, multifunctional biomaterials that can play a distinctive and differentiated role in different phases of wound healing are still lacking.

Hydrogels feature a 3D network structure similar to that of the extracellular matrix (ECM) and are thus regarded as one of the most valuable wound dressings (WDs) [8,11,12]. Based on clinical needs, ideal hydrogel-based WDs should have good biocompatibility and haemostatic ability, broad-spectrum antibacterial, and pro-regenerative activities [13,14]. It is extremely difficult to integrate all of the above functions into one platform of a biomolecular hydrogel. Chemical composition and spatial structure are both key factors determining the functions of biomaterials [15]. Researchers have developed a series of bioactive chemicals, such as silk fibroin (SF), polyglutamic acid (PGA), polylysine (PL), and chitosan derivatives (CSD), to fabricate next-generation biomaterials [16], [17], [18], [19], [20]. However, the spatial structure of biomaterials has not attracted enough attention. It is worthwhile to explore the synergistic relationships between the chemical composition and spatial structure of biomaterials [21,22].

Quaternized chitosan (QCS) is a bioactive derivative of natural chitosan [2,23]. Compared to natural chitosan, QCS has been modified with hydrophilic groups by direct quaternary ammonium substitution, epoxy derivative open loop, or N-alkylation [24]. QCS is positively charged, which can shield the opposite charge of red blood cells to promote coagulation [25]. Meanwhile, QCS is an amphiphilic long-chain molecule that can be inserted into the cell membrane to cause bacterial lysis and death [26]. As a potential haemostatic and antibacterial polymer, QCS has been preliminarily applied to hydrogel-based WDs [27,28]. However, QCS is water-soluble and chemically inert, making cross-linking difficult. Methacylated gelatine (GelMA) is a photoactive polymer and is widely used as the carrier of other biomolecules [29]. The GelMA and QCS composite hydrogel (named GHCH-n) can be facilely fabricated by sequential blending and UV curing.

The spatial structure of the GHCH-n hydrogel is further optimized to improve its application effect. Microneedle patch (MNP) is a novel physical penetration platform consisting of multiple micron-sized needle tips connected to a base in an array manner [30,31]. The unique structure of MNPs is conducive for transdermal drug delivery and tissue adhesion [32]. Compared to traditional syringe needles, MNPs do not reach deeper tissues. Thus, minimally invasive MNPs not only achieve high drug delivery efficiency but also minimize the risk of infection, bleeding, and pain [33]. As one of the frontiers of biomedical engineering, research on MNP-based haemostatic biomaterials has been increasingly reported. For example, Lee. et al. developed a tranexamic acid-loaded MNP for first-aid haemostasis [34]. The theory and method of MNP have been largely improved to reduce costs and increase repeatability, indicating a broad prospect of clinical transformation [35,36]. In this study, GHCH-n hydrogels were processed into MNPs by a micro-moulding technique. It is hypothesized that the GHCH-n MNPs could puncture into the tissue and seal off the bleeding sites safely. Meanwhile, the release of QCS from the GHCH-n MNPs can be sustained allowing it to play a vital role in combating bleeding, bacterial infection and wound healing. Notably, the haemostatic activity of GHCH-n MNPs could be simultaneously attributed to physical plugging and electrostatic shielding [37,38].

This research presents a novel strategy for integrating the nature of bioactive polymers and the structure of composite biomaterials, resulting in multifunctional biomolecular GHCH-n MNPs for complex wound healing. Fig. 1a-c depict the overall concept of this work. QCS and GelMA were chemically synthesized before being combined to create GHCH-n hydrogels and MNPs. As the foundation of translational research, this study also constructed a variety of preclinical models in vivo and in vitro, and the results confirm the products' efficacy and safety across several dimensions. The biocompatible, haemostatic, broad-spectrum antibacterial, and pro-regenerative properties of GHCH-n MNPs are highlighted in Fig. 1d-e. The products in this study are competitive when compared to other hydrogel-based WDs on the market.

Fig 1
Download : Download high-res image (735KB)
Download : Download full-size image
Fig. 1. Schematic illustration of quaternized chitosan-incorporated biomolecular patches and their biomedical applications. (a) Chemical synthesis method of quaternized chitosan (QCS); (b) Chemical synthesis method of methacrylate gelatine (GelMA); (c) Ultraviolet (UV) cross-linked GelMA/QCS composite hydrogels (named GHCH-n) were prepared; (d) GHCH-n was haemostatic and biocompatible; (e) GHCH-n was shaped as microneedle patches for excellent full-thickness wound healing. ","This study presents a novel approach for the fabrication of biomolecular patches using quaternized chitosan (QCS) incorporated composite hydrogels (GHCH-n) for non-compressive hemostasis and wound healing. The GHCH-n hydrogels are created through UV cross-linking of QCS and methacrylate gelatine (GelMA), combining the positive charge and amphiphilic properties of QCS for antibacterial and hemostatic activities. The hydrogels are further shaped into microneedle patches (MNPs) for tissue penetration and secure wound closure. The MNPs release QCS gradually, providing effective hemostasis and wound healing. This innovative approach offers a multifunctional biomaterial for complex wound healing, with demonstrated biocompatibility and efficacy in preclinical models.





"
"Accounting for central and distributed zero liquid discharge options in interplant water network design
Author links open overlay panelSabla Y. Alnouri a, Patrick Linke a, Mahmoud M. El-Halwagi b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.jclepro.2017.09.236
Get rights and content
Abstract
Brine and bittern waste streams are classified as a byproduct of seawater desalination operations, as well as many inorganic industries. As such, many of those industries are challenged to develop and implement sustainable, cost-effective strategies for managing water usage and wastewater discharge. As a result, zero liquid discharge goals have garnered a lot of interest for the purpose of enhancing strategies for wastewater handling. Zero liquid discharge may be achieved using a number of methods, including technologies through which industrial wastewater is reduced to dry solids/salts. This would mostly involve wastewater processing to brine water quality, using standard brine-producing wastewater treatment methods, followed by conventional zero liquid discharge techniques, which in turn transform brine wastewater to salts/bitterns. Salt sludge and bittern waste from zero liquid discharge processing have no adverse effects on the environment. Moreover, many techniques allow for the recovery of extra-purified water streams, as a result of wastewater-to-brine processing, and/or brine-to-salt processing. Recovered water streams may be directly reused, or even utilized to enhance the quality of other wastewater streams before reuse. Since compliance with stringent industrial wastewater regulations is through zero liquid discharge applications, this work discusses the incorporation of zero liquid discharge processing options onto interplant water network synthesis problems. Accounting for the presence of both central and distributed zero liquid discharge processing schemes enables the identification of cost-optimal interplant water networks, when stringent wastewater discharge requirements are imposed. This paper focuses on a specific class of water integration problems, which involve brine management. A case study is used to illustrate the proposed approach and to highlight the importance of accounting for zero liquid discharge considerations, to develop grassroots designs for interplant water networks. For a given industrial city layout, four different scenarios have been carried out, and each case was associated with certain wastewater discharge requirements. As a result, differing optimal interplant water network designs have been attained for each of the cases, which have been investigated.

Introduction
Standard industrial operations often result in many unwanted effluent streams, including wastewater qualities that may cause adverse environmental impacts, if handled inappropriately. Moreover, environmental compliance to wastewater discharge standards must be considered when handling wastewater effluent. Industrial wastewater discharge is mostly regulated by enforcing contaminant-specific effluent limits. Global guidelines for standard wastewater discharge regulations are available, and may be employed. Additionally, in some cases, wastewater discharge regulations might also consider the types of operations being conducted within an industrial facility, or may even enforce site-specific discharge restrictions. Many wastewater treatment technologies have been developed to help industries meet different requirements on water quality standards for discharge. Moreover, many standard treatment methods may also be coupled with Zero Liquid Discharge (ZLD) techniques, to help alleviate environmental concerns that are associated with side streams (sludge, concentrate etc.) that are produced as a result of wastewater treatment.

ZLD generally refers to an approach which prevents any liquid effluent or wastewater discharge into surface waters (Ranade and Bhandari, 2014). This could involve the application of wastewater reuse strategies that attain near-zero discharge levels, or may involve the incorporation of additional processing steps that essentially reduce any wastewater effluent to dry solids/salts. The latter option often requires the use of standard wastewater-to-brine processing, followed by brine to-salt handling techniques. Integrating ZLD options that entail salt processing may allow for additional water recovery in the system, depending on the type of ZLD devices put into operation. Recovered water may potentially be reused in certain water-consuming applications, or may even be utilized to enhance the quality of other wastewater streams, which in turn may increase total wastewater flows being recycled back into the system. Wastewater treatment coupled with an additional ZLD processing stage may assist in devising enhanced wastewater reuse strategies, whilst eliminating wastewater discharge.

To date, water integration methods have been developed to assess the possibility of attaining ZLD or near-ZLD schemes, as a wastewater minimization problem or a cost minimization problem. Graphical techniques, numerical methods as well as mathematical programming tools can address wastewater minimization problems. Deng and Feng (2009) present a retrofit strategy that targets zero liquid discharge, for a water system in an alumina plant, using a graphical method. Deng et al. (2008) analyze the conditions and optimal targets for zero liquid discharge, as well as optimize the corresponding regenerated water flowrate and regeneration concentration, for a water system with a single contaminant and a fixed mass load. They present a graphical technique to target optimal pre- and post-regeneration concentrations, as well as regenerated water flowrates. According to the analysis presented by Deng et al. (2008) ZLD can be accomplished in systems with no water losses, if the post-regeneration concentration is lower than the minimum limiting inlet concentration of all the operations. When water losses are involved, ZLD is only feasible if the maximum post-regeneration concentration defined for ZLD is always higher that the post-regeneration concentration and an optimal ZLD water network may be constructed. Examples of numerical techniques that rely on the use of Water Pinch Analysis (WPA) and Water Cascade Analysis (WCA), for identifying fresh water and wastewater targets, have been discussed by Foo et al. (2006). Additionally, Foo (2008) presents numerical techniques for plant-wide water integration, and the targeting of threshold problems in water networks using water cascades. Shenoy and Shenoy (2015) discuss an analytical methodology that synthesizes water networks with regeneration options, while incorporating the following requirements for Zero Wastewater Discharge (ZWD): (1) water loss is entirely made up for using freshwater; (2) the net contaminant load equals the regeneration load; and (3) the contaminant load (below the pinch) is picked up by the minimum regenerated water and freshwater. The methodology essentially depends on the Unified Targeting Algorithm (UTA) for targeting ZWD, and is used to determine the minimum regeneration flowrate, as well as identify the pinch concentration. More recently, an automated composite table algorithm (ACTA), has been introduced for targeting water regeneration–recycle network for single contaminant problems, based on pinch analysis techniques (Parand et al., 2016). Parand et al. (2016) present an automated methodology, by taking into consideration the possibility of zero liquid discharge (ZLD) for the water network.  ","This paper discusses the incorporation of zero liquid discharge (ZLD) processing options into interplant water network synthesis problems, focusing on brine management in industrial operations. ZLD aims to prevent liquid effluent discharge into surface waters by reducing wastewater to dry solids/salts and potentially recovering additional purified water. The study presents various scenarios with differing wastewater discharge requirements, highlighting the importance of accounting for ZLD considerations in designing cost-optimal interplant water networks. It emphasizes the need to address environmental compliance and offers insights into achieving near-zero wastewater discharge through ZLD techniques, using both graphical and numerical methods.





"
"Can solar energy help ZLD technologies to reduce their environmental footprint? - A Review
Author links open overlay panelAbhishek Gautam, Tapan Dave, Shankar Krishnan
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.solmat.2023.112334
Get rights and content
Abstract
Water is the primary factor for life on earth, and its demand is continuously increasing significantly. Therefore, freshwater scarcity is one of the world's critical challenges due to the depletion and contamination of its reservoirs. Climate change, irrational exploitation, and waste disposal from various sources to the natural water reserves are the major causes of the contamination. Moreover, the increased utility of desalination systems has also magnified the production of rejected brine, whose disposal in water bodies and land will also result in a hazardous situation for future generations. Therefore, it is necessary to adopt sustainable approaches in the present for the welfare of future generations. Given the same, the concept of zero liquid discharge (ZLD) has emerged as a strong alternative for freshwater production in the most sustainable manner. It is a dual-purpose technology of fresh water production with dry salts from the brine by preventing the brine discharge into the environment. The sustainability of ZLD desalination systems can be further increased by using solar energy to fulfill their energy requirement. Hence, various technical and economic aspects of ZLD desalination technologies are presented in this review, along with the discussion on the feasibility of solar energy to achieve ZLD. An analysis is carried out on the impact of solar energy on the energy, economic and environmental potential of ZLD desalination technologies, whose outcome shows wind-aided intensified evaporation, reverse osmosis, and electrodialysis metathesis have higher potential compared to rest technologies.

Introduction
Water is one of the basic needs for life on earth. The quantity of fresh water is depleted with time due to advancements in industrialization, rising living standards, climate change, and contamination of freshwater reserves. The earth is considered a blue planet because water occupies 71% of its surface. However, humanity and other water-dependent species are dealing with water scarcity, as 96.5% of available water on earth is not potable due to its salinity and brackish form [1]. This quantity of water can only be utilized in industries for various processes such as mining and power generation. As per the United Nations world water development report 2021, nearly 2/3rd of the global population faces water scarcity for at least one month each year, whereas 1/4th of the global population may deal with the same trouble for the whole year by 2040 [2]. The curves between clean water availability and its demand presented by Boretti and Rosa [3] about the alarming situation of water crises till 2040 are shown in Fig. 1.

The stakeholders have been trying to overcome this issue of water scarcity for the last few decades [4]. The distillation and membrane processes have been common practice for many years because of the same. Desalination of saline water is one of the standard methods to get fresh water for utility [5]. In the last decade, a positive phase transformation has been observed worldwide in utilizing desalination systems to overcome water scarcity. Countries like China, India, Israel, Australia, and nations of Arab and Africa are developing desalination systems/plants due to the limited freshwater availability in their region [6]. As per the International Desalination Association (IDA), nearly 22700 desalination plants with a capacity of almost 107 million m3/day are operational worldwide until 2022 [7].

Irrespective of the technology used for water desalination, brine disposal and its environmental impact are major issues [8]. The generalized processes involved in managing brine obtained from the desalination plant are shown in Fig. 2. The rejected water with many concentrated salts (called brine) is usually discharged into inland water bodies or seawater, which harms marine ecosystems and aquatic species and also affects the human health [[9], [10], [11]]. Therefore, several international agreements have been signed to encourage and support building solutions for protecting ecosystems, conserving water bodies, and improving water quality [12,13].

Due to the exponential increment in desalination plants acting as a source for wastewater production, the amount of brine is also increasing rapidly. Thus, the harmful environmental impacts of rejected brine from such sources emphasize the world to improve their disposal methods and explore alternative sustainable technologies. Zero liquid discharge (ZLD) is one such concept for freshwater production using desalination technology to provide an alternative to all the existing disposal methods. Stringent regulations, increasing cost of wastewater disposal, and steepened significance of freshwater emphasize a ZLD approach for freshwater production.

Apart from the associated benefits, the decision to implement the ZLD approach is not easy. The involvement of intensive energy requirements and high capital/operation costs are the major barriers to its success; hence, the ZLD approach is adopted by limited plants worldwide. A feasible ZLD-based plant should have an optimum balance among the benefits, energy requirements, and associated costs [15]. On the other side, fossil fuels are the primary energy sources to fulfill the energy needs of such systems, which are indirectly associated with similar drawbacks to nature as in the case of conventional desalination plants. Solar energy is emerging as a strong alternative to fossil fuels, which has the potential to play a significant role in accomplishing the energy requirement of ZLD systems without leaving any harmful impact on the environment [16].

Solar energy has been established as an effective alternative to fossil fuels due to its abundant quantity and superior quality. It is a freely available and environmentally friendly energy resource [17]. Solar energy-based desalination systems have received significant research attention recently and have achieved cost-effectiveness, environmental benefits, and comparatively simpler operations [18]. Several researchers have also reported their studies on solar energy-based hybrid desalination systems in the last few years [16,[19], [20], [21], [22], [23], [24]]. Moreover, the economic feasibility of such systems is also reported in recent studies [25,26]. Thereby, the potential, feasibility, and challenges allied with using solar energy to achieve ZLD have been explored and discussed in this review paper.

A number of reviews were reported to address the various aspects of the ZLD strategy, such as technologies, energy analysis, salt recovery, economic feasibility, and environmental footprints [27,28]. It was found that the high energy requirement, lesser monetary benefits, and negative impact of conventional energy sources are the common issues raised in such studies. The feasibility of solar energy in desalination and integrated desalination systems with solar power plants have also been reported by researchers. However, solar energy's potential to achieve ZLD has not been reviewed so far. Therefore, the current review presents the potential and feasibility of solar energy to achieve ZLD. Initially, the existing conventional methods of brine disposal are discussed in detail. Thereafter, the need and features of ZLD are discussed along with various technologies, followed by a discussion on the feasibility of solar energy to achieve ZLD. Moreover, the present review also presents a discussion on the economic aspect of the same and an analysis of solar energy's technical, economic, and environmental potential to achieve ZLD with various desalination methods. The conclusions drawn from the extensive literature review are presented with future scope and recommendations.","This review paper explores the potential of utilizing solar energy to achieve Zero Liquid Discharge (ZLD) in desalination systems, addressing the critical issue of brine disposal and its environmental impact. With increasing freshwater scarcity and the environmental consequences of brine discharge, ZLD has emerged as a sustainable approach for freshwater production while minimizing harm to ecosystems. The review discusses various desalination technologies, the challenges associated with ZLD adoption, and the feasibility of solar energy in meeting the energy requirements of ZLD systems. It highlights the economic and environmental benefits of solar-powered ZLD and presents an analysis of its potential across different desalination methods, offering valuable insights for future research and implementation.





"
"An equivalent mathematical program for games with random constraints
Author links open overlay panelVikas Vikram Singh a, Abdel Lisser b, Monika Arora c
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.spl.2021.109092
Get rights and content
Abstract
We consider an 
-player chance-constrained game under elliptically symmetric distributions. For a confidence level greater than 0.5 and certain class of payoff functions and strategy sets, we suitably construct an equivalent mathematical program whose global maximizer is a Nash equilibrium.

Introduction
Nash equilibrium is the standard solution concept used to study strategic interactions. It is a strategy profile of the players where there is no incentive for unilateral deviation by any player. The theory of games involving Nash equilibrium started with the paper by Nash (1950) where he showed that there exists a mixed strategy Nash equilibrium for finite strategic games. Later it has been shown that a Nash equilibrium of a general non-cooperative game exists under certain conditions on payoff functions and strategy sets of the players (Debreu, 1952, Fan, 1966). The games considered in these papers are deterministic in nature, i.e., the players’ strategy sets and payoff functions are defined using real valued functions. However, in practical situations the decision making process usually faces various types of uncertainties due to which payoff functions or strategy sets are modeled using random variables (Couchman et al., 2005, Mazadi et al., 2013, Ratha et al., 2019). The expected value approach is used to model the uncertainties when the decision makers are risk neutral (Ravat and Shanbhag, 2011). For risk averse players, the payoff criterion with the risk measure CVaR (Kannan et al., 2013, Ravat and Shanbhag, 2011) and the variance was considered in the literature (Conejo et al., 2004).  Singh et al. (2016) and Singh and Lisser (2018) considered a finite strategic game where the payoff vector of each player is a random vector. They considered the case where each player is interested in the payoffs that can be obtained with certain confidence. To model this situation, they defined the payoff function of each player using a chance constraint and called such game a chance-constrained game. In Singh et al. (2016), the authors showed the existence of a mixed strategy Nash equilibrium for an 
-player chance-constrained game, when the payoff vector of each player follows a multivariate elliptically symmetric distribution. In Singh and Lisser (2018), the authors proposed an equivalent mathematical program to compute the mixed strategy Nash equilibria of the two player chance-constrained game for elliptically symmetric distributed payoffs. There are some zero-sum chance-constrained games studied in past literature (Blau, 1974, Cassidy et al., 1972, Charnes et al., 1968, Cheng et al., 2016).

The chance-constrained games in the above-mentioned papers model the payoffs’ uncertainties using chance constraints. However, the uncertainties can be present in the strategy sets due to various external factors. The chance constraint based strategy sets are often considered in various applications, e.g., resource constraints in stochastic shortest path problem (Cheng and Lisser, 2012) and risk constraints in portfolio optimization (Ji and Lejeune, 2018) can be modeled using chance constraints. The literature on games with chance constraint based strategy sets have covered only a small portion of open questions (Shen et al., 2018, Singh and Lisser, 2019).  Singh and Lisser (2019) considered a two player zero-sum matrix game where strategy set of each player is defined using individual chance constraints. They showed that the saddle point equilibria of the game can be computed by solving a primal–dual pair of second order cone programs when the random constraint vectors follow multivariate elliptically symmetric distributions. Shen et al. (2018) considered an 
-player game with joint chance constraints and showed that there exists a Nash equilibrium of the game if the row vectors are independent and follow multivariate normal distributions. In this paper, we consider an 
-player game where the strategy sets are defined by individual chance constraints. The random constraint vectors follow multivariate elliptically symmetric distributions. It follows from Henrion (2007) that an individual chance constraint is equivalent to a second order cone constraint and it makes the feasible strategy set of each player a convex set. Then, under standard quasi-concavity and continuity conditions on the payoff functions, there exists a Nash equilibrium of the game (Debreu, 1952, Fan, 1966). In order to compute explicitly a Nash equilibrium, we consider a specific payoff function for each player which satisfy the standard conditions. The first term of a player’s payoff function is multi-linear in all the players strategies and second term is a quadratic concave function of the player’s strategies. Such types of payoff functions are often encountered in practical situations (Gilli et al., 2011, Ratha et al., 2019, Steinbach, 2001, Wang et al., 2019). We propose an equivalent mathematical program for this class of games and show the one-to-one correspondence between a Nash equilibrium of the game and a global maximizer of the mathematical program.

The structure of rest of the paper is as follows. Section 2 contains the definition of a chance-constrained game. Section 3 presents the existence of a Nash equilibrium. The equivalent mathematical program is given in Section 4.

Section snippets
The model
We consider an 
-player non-cooperative game defined by tuple 
, where 
 is the set of players, 
 is a strategy set of player 
 and 
 is a payoff function of player 
. The strategy set 
, 
, is a non-empty, convex and compact set. The product set 
 is a set of all strategy profiles of the game. Let 
 be the set of vectors of strategies of all the players but player 
. The generic elements of 
, 
, and 
 are denoted by 
, 
,

Existence of Nash equilibrium
We consider the case where for each 
, the vectors 
, follow multivariate elliptically symmetric distributions.

Definition 3.1

A 
-dimensional random vector 
 follows an elliptically symmetric distribution 
 if its characteristic function is given by 
 where 
 is the characteristic generator function, 
 is the location parameter, and 
 is the positive semidefinite scale matrix.


The class of multivariate elliptically symmetric distributions generalizes the

Mathematical programming formulation
We consider a class of 
-player chance-constrained games which satisfy the conditions 
 and 
 of Theorem 3.3. For each 
, let 
. Define the product sets 
 and 
. Consider a vector 
, where 
 with 
. The payoff function of player 
 is given by 
where 
 is a positive definite matrix. The first term of the payoff function of player 
 is linear in 
th player’s strategies for a fixed strategy  ","The paper addresses non-cooperative games with random constraints, specifically focusing on chance-constrained games under elliptically symmetric distributions. It introduces a mathematical programming approach to find Nash equilibria in such games, considering individual chance constraints in player strategy sets. The existence of Nash equilibria is demonstrated, and a specific class of payoff functions, which are commonly encountered in practical scenarios, is considered. The proposed mathematical program establishes a one-to-one correspondence between Nash equilibria and global maximizers. This research provides a valuable framework for analyzing strategic interactions in uncertain environments with random constraints.





"
"Portfolio selection and risk control for an insurer in the Lévy market under mean–variance criterion☆
Author links open overlay panelJieming Zhou a, Xiangqun Yang a, Junyi Guo b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.spl.2017.03.008
Get rights and content
Abstract
In this paper, we apply the martingale approach to investigate the optimal investment and risk regulation problem for an insurer. Assume that the insurer is allowed to invest in a financial market consisting of one risk-free asset and one risky asset whose price is modeled by a Lévy process. The risk process of the insurer is described by another Lévy process, and the insurer can regulate the risk by controlling the number of insurance polices. Finally, the closed-form expressions for the efficient strategy and efficient frontier are given under the criterion of mean–variance.

Introduction
In the classical investment problem for an insurer, Browne (1995) first find the optimal investment strategy to maximize the exponential utility of terminal wealth. In his paper, the risk process is approximated by a Brownian motion with drift and the stock price is modeled by a geometric Brownian motion. In the actuarial literature, the optimal reinsurance and investment problem for an insurer is usually studied via dynamic programming approach. Some researchers focus on maximizing the expected constant absolute risk aversion (CARA) utility of the insurers’ terminal wealth, and obtain a series of results by this approach. See, for example, Yang and Zhang (2005), Bai and Guo (2008), Edoli and Runggaldier (2010), Liang et al. (2011), Huang et al. (2016) etc.

The mean–variance criterion to portfolio selection problem was firstly proposed by Markowitz (1952). From then on, a lot of work on mean–variance problem has been done in discrete time models. By using stochastic control theory, Zhou and Li (2000) investigate the continuous-time Markowitz models and the corresponding explicit solutions are obtained for the mean–variance problem for an insurer since the work of Bäuerle (2005). Further extensions and improvements can be found in Bai and Zhang (2008), Zeng et al. (2013), Bi et al. (2014) and so on.

Among most of the literature mentioned above, the authors use the dynamic programming principle to solve the optimization problem. Recently, the martingale approach is considered in Wang et al. (2007), in which the optimal strategies are obtained explicitly for the mean–variance criterion and the CARA utility. Under the objective of maximizing the expected utility, Zhou (2009), Perera (2010), Zou and Cadenillas (2014) obtain the corresponding optimal control strategies for their models by the martingale approach.

In this paper, inspired by the work of Zou and Cadenillas (2014), we consider the optimal investment and risk regulation problem for an insurer with Lévy dynamics under the mean–variance criterion. We derive explicit expressions for the efficient strategy and efficient frontier. This paper proceeds as follows. In Section 2, we introduce the formulation of our model. The explicit expressions and their proofs for the efficient strategy and efficient frontier are derived in Section 3. In Section 4, numerical simulations are presented to illustrate our results.  ","This paper addresses the problem of optimal investment and risk regulation for an insurer in a financial market with one risk-free asset and one risky asset modeled by a Lévy process. The insurer's risk process is also described by a Lévy process, and risk control is achieved by managing insurance policies. The study focuses on the mean-variance criterion and applies the martingale approach to derive closed-form expressions for the efficient investment strategy and efficient frontier. The research provides insights into portfolio selection and risk management strategies for insurers in a Lévy market.





"
"In addition, there exist some similar and distinct processes of neuroimmune interactions in the progression of neurodegenerative diseases. Here, we focus on microglia to make comparisons in terms of neuroimmune interactions among AD, PD and ALS. In the process of these neurodegenerative disorders, many intracellular or extracellular materials that directly or indirectly derived from neurons, such as Aβ in AD [12,13], α-synuclein in PD [12,61] and mutant superoxide dismutase 1 (mSOD1) in ALS [12,13], can activate microglia via binding to pattern recognition receptors expressed on microglia, including TLR2, TLR4 and TLR6 [12]. At first, the activated microglia can help to clear these substances to prevent neurons from injury, but finally they exert toxic effects on neurons owing to overload and chronic activation [13,48,67,68], resulting in neurodegeneration. In this condition, these activated microglia are able to damage and kill neurons through direct or indirect methods [13]. They can directly degrade neurons by phagocytosis [61] or cause excitotoxic neuronal death via releasing glutamate as well as overexpressing iNOS [13]. Additionally, they can release proinflammatory cytokines to amplify local neuroinflammation [61] and reduce the production of neuroprotective factors, including brain-derived neurotrophic factor (BDNF) as well as insulin-like growth factor (IGF) [13], giving rise to the death of neurons indirectly. Via secreting cytokines, such as IL-1α and TNF, they can also promote the conversion of neuroprotective astrocytes to neurotoxic astrocytes [45,61,67] which can cause neuronal demise through reduced trophic support and release of neurotoxic factors [68]. These processes are the same in these three diseases, and there also exist some differences. In AD and PD, dysregulated microglia can spread toxic materials they cannot digest to healthy neurons [13,61], which, in our knowledge, is still not reported in ALS. For instance, in AD, microglia can spread tau aggregates in a non-synaptic transmission pathway [68] and carry Aβ to unaffected brain tissue [52]. Similarly, in PD, microglia can facilitate the propagation of toxic forms of α-synuclein to healthy dopaminergic neurons [61,69]. Moreover, unlike that in AD and ALS, activated microglia in PD are capable of inducing the expression of MHC class I molecules on dopaminergic neurons [70] and functioning as antigen-presenting cells to present dopaminergic neuronal antigens to T cells, leading to the activation of these cells and the arrival of immune attack to dopaminergic neurons [45,61]. Additionally, activated microglia can induce ferroptosis of dopaminergic neurons through disrupting iron homeostasis and increasing oxidative stress in PD [71], whereas in AD, they mediate the apoptosis of neurons via secreting proteases [53]. Why do different neurodegenerative diseases have these similar processes? Are they the common foundations of the initiation or development of neurodegeneration? And whether the distinctions determine the specificity of the diseases? All these questions are still poorly understood. And besides microglia, such similarities and distinctions also exist in other cells. Therefore, it is essential and necessary to depict and compare the systemic changes of the immune system and the alterations of neuroimmune interactions in various neurodegenerative diseases. These findings will help us gain new insights into the role of neuroimmune interactions in neurodegeneration and develop more effective disease-specific therapies.

Although we just emphasize the effects of neuroimmune interactions on neurological disorders in this review, especially neurodegenerative diseases, it should be noticed that they also function in other illnesses, including immune disorders. It has been shown that neuroimmune interactions have a pivotal role in pathophysiology of allergic inflammation, such as asthma [7]. It seems that noradrenaline derived from sympathetic nerves and acetylcholine derived from parasympathetic nerves are able to suppress group 2 innate lymphoid cell (ILC2)-mediated type 2 airway inflammation, resulting in alleviating the symptoms of asthma [7]. Moreover, in the development of rheumatoid arthritis, sympathetic nerves might mediate the differentiation of naive CD4+ T cells into Th1 cells, which accumulate within the synovial fluid and secrete IFN-γ to promote the inflammatory process [6]. Additionally, neuroimmune communications also appear to affect the initiation, maintenance and aggravation of psoriasis (Ayasse et al., 2020.), as well as the development and relapse of EAE [28,29]. All these studies suggest that neuroimmune interplay is also crucial in the progression of immune diseases. Given the general existence of the immune system and the nervous system inside the body, their interactions may also play a vitally important role in other disorders not directly related to these two systems. Deeper insights into this aspect and its details still warrant further research. ","This review discusses the role of neuroimmune interactions in neurodegenerative diseases such as Alzheimer's disease (AD), Parkinson's disease (PD), and amyotrophic lateral sclerosis (ALS). It focuses on the involvement of microglia and their activation in response to neuronal-derived materials, leading to neuroinflammation and neurodegeneration. While there are similarities in these processes across the three diseases, there are also distinctions, such as the spreading of toxic materials by microglia in AD and PD but not in ALS. The review highlights the need for a better understanding of neuroimmune interactions in various neurodegenerative diseases and their potential as targets for disease-specific therapies. Additionally, it acknowledges the role of neuroimmune interactions in immune disorders and other illnesses, emphasizing the importance of further research in this field.





"
"Analysis of the evolution characteristics of international ICT services trade based on complex network
Author links open overlay panelYihan Zhang a b, Jinwen Xu a b, Wancheng Yang a b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.telpol.2023.102697
Get rights and content
Abstract
Digital technology has brought profound changes in the services trade, particularly accelerating the growth of the international Information and Communications Technology (ICT) services trade. Understanding the evolving characteristics of the ICT services trade is pivotal for trade participants to formulate effective policies and enhance their competitive advantages. This study used the data from UN COMTRADE to construct ICT services trade networks spanning from 2004 to 2020. Using complex network analysis methods, it delved into several aspects of ICT services trade, including trade patterns, trade relationships, and participating countries. The findings revealed that central countries in ICT services trade are progressively dominating trade relationships. Moreover, many countries in the middle and lower tiers of the ICT services trade system tend to engage in bilateral trade with major economies or form trade alliances with a select few nations. Examining the evolution of trade participants, early adopter countries such as the United States, Ireland, and the United Kingdom have solidified stable competitive advantages in ICT services trade. Building upon these findings, the study puts forth several recommendations to enhance the competitiveness of the ICT services trade. This study not only enriches the existing knowledge base on ICT services trade networks but also provides valuable insights for shaping policies in this domain. The outcomes offer crucial guidance for interpreting cooperation and conflicts within the realm of ICT services trade.

Introduction
Services trade increasingly plays a crucial role in global trade and has emerged as an essential driver of global trade development (Xu, 2021). A nation's trade competitiveness can be gauged by its performance in services trade (WTO, 2019). As the Fourth Industrial Revolution progressed, the trade of information and communication technology (ICT) services experienced rapid growth, capturing the attention of numerous scholars. Regarding the development and impact of the ICT industry, Nath and Liu (2017) conducted an empirical analysis using panel data from 13 countries to examine the effects of ICT development on import and export services trade. Luong (2021) employed an enhanced gravity model to measure ICT industry development from four dimensions, revealing that the influence of ICT on exporting and importing countries was similar. Biryukova and Matiukhina (2019) examined significant characteristics of ICT applications in BRICS countries and analyzed the evolution of ICT systems within those nations. Njangang, Beleck, Tadadjeu, and Kamguia (2022) investigated the impact of the ICT industry on wealth inequality in 17 developed and developing countries, finding that ICT exacerbated existing wealth disparities. As the ICT industry continues to expand, the trade of ICT services is emerging as a widespread and consequential phenomenon. Regarding existing research on ICT services trade, Noonan and Plekhanova (2020) explored variations in tax systems related to ICT services trade across different countries. Willemyns (2018) delved into digital obstacles in international ICT services trade between nations. Mulenga and Mayondi (2022) analyzed the effects of ICT services trade on the economies of diverse countries.

Most existing studies have focused on the economic impact of ICT services trade and the promotion of ICT-related industries based on trade volume. However, as information and communication technology continues to advance, there is a rapid growth in the demand for ICT services across different countries. More than merely measuring bilateral trade volume is required to accurately assess trade activity value flow (Amador & Cabral, 2017; Tsekeris, 2017). Moreover, within the field of global trade research, it is crucial to explore the role of traders in the trade system and their position in the trade network. This exploration serves as the foundation for comprehending the trading system within a specific area (Xanat, Jiang, Barnett, & Park, 2018; Fracasso, Nguyen, & Schiavo, 2018; Zhao et al., 2020). Therefore, given the increasingly complex nature of ICT services trade, shifting the research focus from trade volume to trade relationships is imperative. This shift involves analyzing the overall pattern of ICT services trade, the developmental characteristics of trade relationships, and changes in the status of participating countries. Conducting such research will contribute to the stable development of the ICT services trade. In recent years, complex network studies have expanded beyond computer science to encompass other disciplines (Watts & Strogatz, 1998). This expansion has provided a fresh perspective for examining complex global trade relationships. The trade activities among countries can be naturally represented as a network, wherein nodes symbolize participating countries or regions, edges represent trade flows, and edge weights denote trade quotas (Ospina-Alvarez et al., 2022). Such a network exhibits typical complex network characteristics (Almog, Bird, & Garlaschelli, 2019). Scholars have already applied complex network methods in studies on international commodity trade, such as crude oil trade (Fracasso, Nguyen, & Schiavo, 2018), agricultural product trade, and electronic waste trade (Petridis, Petridis, & Stiakakis, 2020). By employing complex network methods to study international ICT services trade, it becomes possible to simplify the intricate trade relationships and gain a more intuitive understanding of the overall evolution of trade.

In this study, we constructed the international ICT services trade networks spanning the years 2004–2020. Through a comprehensive analysis, we explored the evolutionary characteristics of ICT services trade, focusing on overall patterns, trade relationships, trade centers, and trade hubs within these networks. Based on the outcomes of our network analysis, we presented several strategic suggestions for developing ICT services trade to achieve stable global growth. The contributions of this study can be summarized as follows: (1) Analysis of Trade Patterns and Evolution Characteristics: While existing literature predominantly delves into processes, tax systems, and digital barriers in ICT services trade, there is a noticeable gap in research regarding the trade patterns and evolution characteristics of this trade. Given the intricate nature of ICT services trade in the contemporary landscape, conducting an in-depth analysis of trade patterns, relationships, and the roles assumed by each participating country becomes imperative. This study utilized data from the United Nations Commodity Trade Statistics Database (UN Comtrade) and applied complex network methodologies to explore the evolutionary characteristics of international ICT services trade. Additionally, we analyzed the positions and roles of trading countries through the lenses of trade centers and trade hubs. These insights not only serve as vital references for bolstering the stability of overall trade patterns but also offer strategic guidance for enhancing the competitiveness of individual participating nations. (2) Global Perspective on ICT Services Trade: While prior research, such as Biryukova and Matiukhina (2019), focused on constructing an ICT services trade network within BRICS countries, few studies have delved into the evolving dynamics of global ICT services trade. This study bridges this gap by creating international ICT services trade networks spanning from 2004 to 2020, facilitating a comprehensive analysis of this trade phenomenon at an international level. Our findings not only provide valuable insights for shaping policies in the realm of ICT services trade but also serve as an indispensable reference for understanding international cooperation and conflicts within this domain.","This study examines the evolution of international Information and Communications Technology (ICT) services trade using complex network analysis. Analyzing data from 2004 to 2020, the study reveals that central countries are increasingly dominating ICT services trade relationships, while many middle and lower-tier countries engage in bilateral trade with major economies or form trade alliances. Early adopter countries like the United States, Ireland, and the United Kingdom have established stable competitive advantages in ICT services trade. The findings offer insights into trade patterns, relationships, and the roles of participating countries, providing valuable guidance for trade policies and competitiveness enhancement in the ICT services sector.





"
"Molecular and morphological evidence for a new species of Isodon (Lamiaceae) from southern China
Author links open overlay panelYa-Ping Chen a, Cun-Zhong Huang b, Yue Zhao a c, Chun-Lei Xiang a
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.pld.2020.06.004
Get rights and content
Under a Creative Commons license
open access
Abstract
Isodon brevipedunculatus, a new species from southern China, is described and illustrated. The phylogenetic position of the new species within the genus was analyzed based on two nuclear ribosomal DNA regions and an ingroup sampling of about 80% of Asian species of Isodon. The results show that I. brevipedunculatus is recovered in a clade that consists of species mainly with glandular mericarps and that are distributed in the Sino-Japanese region. Combining molecular and geographical evidence, our study reveals that I. brevipedunculatus is most closely related to Isodon amethystoides and Isodon bifidocalyx, but differs from the former in lamina shape, number of flowers per cyme, and peduncle length, and from the latter in lamina indumentum, calyx morphology, and corolla length.

Previous article in issueNext article in issue
Keywords
Himalaya-Hengduan Mountains regionIsodoninaeMericarpSino-Japanese region
1. Introduction
Isodon (Schrad. ex Benth.) Spach is one of the largest genera in Lamiaceae with approximately 100 species distributed mainly in tropical and subtropical Asia (Harley et al., 2004; Li, 1988; Li and Hedge, 1994; Mabberley, 2008; Wu and Li, 1977). The Himalaya-Hengduan Mountains (HHM) global biodiversity hotspot, which accommodates ca. 70% of the species of Isodon, is considered the distribution and biodiversity center of the genus (Yu et al., 2014; Zhong et al., 2010). Isodon is recognized as the only genus in subtribe Isodoninae (Zhong et al., 2010), and it differs from other genera of Ocimeae by its pedunculate and bracteolate cymes, slightly or strongly 2-lipped (3/2) calyces, strongly 2-lipped (4/1) corollas, and free filaments inserted at the base of the corolla tube (Harley et al., 2004; Li, 1988; Paton and Ryding, 1998). Some species of Isodon have long been used as traditional folk medicine in China and Japan, and contemporary phytochemical studies of Isodon species have so far isolated and identified more than 1200 diterpenoids, some of which have important pharmaceutical functions (Liu et al., 2017; Sun et al., 2006).

Several new species of Isodon have been reported from China during the last decade (Chen et al., 2014, 2016b, 2017, 2019; Xiang and Liu, 2012). Recently, we collected a distinct species of Isodon from Hunan and Guangdong Provinces in southern China. Critical studies based on specimen and literature examination, as well as molecular phylogenetic analyses revealed it to be an undescribed species. Herein, we describe and illustrate the new species.

2. Material and methods
2.1. Morphological and taxonomic studies
Comparison of morphological features between the new species and other species of Isodon were carried out based on our previous field observations, specimen examination, and unpublished mericarp data (Chen, 2017). Specimens of Isodon from 29 herbaria (A, AU, BM, CDBI, CSFI, E, G, GXMI, HHBG, HIB, IBK, IBSC, K, KUN, KYO, L, LBG, LE, MW, NAS, P, PE, S, SYS, SZ, TAI, TI, W, and WUK; abbreviations follow Thiers, 2020) and our field collections were examined. Meanwhile, protologues of all published names and all other taxonomic literature for Isodon were reviewed. The terminology used by Li (1988) and Li and Hedge (1994) was adopted for the morphological description of the new species.

2.2. Taxon sampling and DNA amplification
The systematic placement of the new species was explored based on an ingroup sampling comprising 90 accessions of 84 species of Isodon from Asia, including two individuals of the new species from the type locality in Guangdong Province and from Hunan Province, respectively (Appendix A). Six genera representing all subtribes of Ocimeae except Isodoninae (Harley et al., 2004; Zhong et al., 2010) were selected as outgroups (Appendix A). Previous studies have shown that Isodon chloroplast DNA sequences have significantly lower numbers of variable sites than nuclear DNA sequences, and consequently generate poorly resolved phylogenies (Chen et al., 2019; Yu et al., 2014; Zhong et al., 2010). Thus, for phylogenetic analyses, we used two nuclear ribosomal DNA markers: the nuclear ribosomal internal and external transcribed spacers (ITS and ETS). A total of 172 sequences were downloaded from GenBank to complement our dataset, of which 168 sequences were generated from our previous study (Chen et al., 2019). Voucher information and GenBank accession numbers for all sequences are listed in Appendix A.

The modified CTAB method (Doyle and Doyle, 1987) was used to extract genomic DNA from the silica-gel-dried leaf material. For polymerase chain reaction (PCR) amplification, ITS was amplified using the primer pairs 17SE/26SE (Sun et al., 1994), ETS using ETS-B (Beardsley and Olmstead, 2002) and 18S-IGS (Baldwin and Markos, 1998). The PCR and sequencing protocols for the two markers followed those of Chen et al. (2016a).

2.3. Sequence alignment and phylogenetic analyses
Sequences were assembled and edited using Sequencher 4.1.4 (Gene Codes, Ann Arbor, Michigan, USA), and then aligned using MUSCLE (Edgar, 2004) and manually adjusted in MEGA v.6.0 (Tamura et al., 2013). Gaps were treated as missing data. Bayesian Inference (BI) and Maximum Likelihood (ML) analyses were conducted to reconstruct the phylogeny of Asian Isodon, using MrBayes v.3.2.6 (Ronquist et al., 2012) and RAxML-HPC2 (Stamatakis, 2014) on the Cyberinfrastructure for Phylogenetic Research Science (CIPRES) Gateway (http://www.phylo.org/; Miller et al., 2010), respectively. Parameters of each category of analysis followed that of Chen et al. (2019). TreeGraph 2 (Stover and Müller, 2010) was used to visualize the topology of phylogenetic trees with posterior probabilities (PP) and Bootstrap support (BS) values.

3. Results and discussion
Consistent with previous molecular phylogenetic studies (Chen et al., 2019; Yu et al., 2014; Zhong et al., 2010), three well-supported clades (Fig. 1; Clades I–III) are recognized for Asian Isodon. Clade III contains ca. 80% of the species, but relationships within the clade are poorly resolved. Within Clade III, a moderately supported subclade Clade IIIa (Fig. 1; BI-PP = 0.99/ML-BS = 72%) can be recognized, with most of the species having a Sino-Japanese distribution, as opposed to the remaining species of Clade III that are predominantly distributed in the HHM region. The two individuals of the new species group together but with moderate support (Fig. 1; BI-PP = 0.94/ML-BS = 73%), which may partially result from our failure in obtaining the ITS sequence of the individual from Guangdong. The new species is further recovered in Clade IIIa. All species of this clade are perennial herbs, most of which are characterized with glandular or glandular and puberulent mericarps (Chen, 2017). One species that has glandular and puberulent mericarps but is not recovered in Clade IIIa is Isodon trichocarpus (Maxim.) Kudô, a species endemic to Japan","A new species of Isodon, Isodon brevipedunculatus, from southern China is described and illustrated in this study. Molecular analysis based on nuclear ribosomal DNA markers places the new species in a clade consisting of species mainly with glandular mericarps, primarily distributed in the Sino-Japanese region. It is closely related to Isodon amethystoides and Isodon bifidocalyx but exhibits distinct morphological characteristics. The study contributes to our understanding of the diversity and evolutionary relationships within the Isodon genus in Asia, particularly in the Himalaya-Hengduan Mountains region.





"
"Exploring ordered patterns in the adjacency matrix for improving machine learning on complex networks
Author links open overlay panelMariane B. Neiva, Odemir M. Bruno
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.physa.2023.129086
Get rights and content
Abstract
The use of complex networks as a modern approach to understanding the world and its dynamics is well-established in the literature. The adjacency matrix, which provides a one-to-one representation of a complex network, can also yield several metrics of the graph. However, it is not always clear whether this representation is unique, as the permutation of rows and columns in the matrix can represent the same graph. To address this issue, the proposed methodology employs a sorting algorithm to rearrange the elements of the adjacency matrix of a complex graph in a specific order. The resulting sorted adjacency matrix is then used as input for feature extraction and machine learning algorithms to classify the networks. The results indicate that the proposed methodology outperforms previous literature results on synthetic and real-world data.

Introduction
The emergence of Big Data has sparked interest in structuring data as closely as possible to reality and evaluating it to extract knowledge. Traditional data analysis often reduces complex phenomena to simplified objects. However, technological advances and the ability to gather, process, and store larger amounts of data allow us to explore information from various viewpoints. The capability to create a system with elements and relationships shifts the reductionist approach to an integrative one.

In addition, pattern recognition has been a prominent branch of data science in understanding the world through the perspective of technology. If one were to think of a method that could combine the benefits of artificial intelligence techniques and integrative data analysis, complex networks would be a natural choice. Pattern recognition in complex networks includes a range of algorithms, such as classification and clustering. Although clustering plays a significant role in the field, classification enables us to recognize diseases, species, structures, and cities, among others. This task is crucial nowadays due to the large amount of data generated that would be too time-consuming and costly to analyze manually. Furthermore, complex networks have a significant advantage for pattern recognition in the era of Big Data, as they can be used to model a wide variety of data, from images to biological systems. It has been demonstrated over the years that most real systems exhibit characteristics of small-world and scale-free networks. The former refers to structures in which elements are connected, on average, by short minimum paths, similar to what occurs in social networks where there is a high probability that a person’s friend is also a friend of the person in question. The latter refers to the fact that there are frequently reached elements in a network, such as prominent researchers in a field or influential articles in a text. The latter example illustrates the importance of using graphs to analyze the patterns and structures of a given organization. Therefore, this work’s quantitative analysis focuses on classifying synthetic and real networks.

Based on the advantages mentioned above, researchers have successfully used the model for pattern recognition in various applications, such as the classification of static and dynamic textures [1], shapes [2], authorship [3], and others. Recently, some works, such as the use of cellular automata in [4], [5], [6], the construction of multidimensional and deep embeddings from networks in [7], and the analysis of angles formed in the graph of shapes in [8], [9], have distinguished themselves from traditional analysis that uses classical statistical metrics such as degree and clustering coefficient to create a one-dimensional graph representation. The recent efforts of some researchers to find novel techniques that overcome the redundant information found in the composition of descriptors based on classical metrics have produced good results in graph classification. As shown in [10], the concatenation of some correlated metrics is sometimes not helpful for pattern recognition. However, have we exhausted all the simplest analyses on complex networks? This study aims to partially answer this question by investigating a simple alternative to represent the graph: the adjacency matrix.

The adjacency matrix of a graph has a one-to-one correspondence with the graph itself. This representation allows for the quick computation of metrics such as degree and co-citation. The matrix has been applied in graph visualization [11] or visualizing the temporal evolution of contact networks in [12]. However, a visual inspection of the adjacency matrix provides little information for classification, as permutations of the rows or columns do not change the underlying graph. To address this issue, we propose an ordination of the rows of the matrix such that patterns within the matrix become consistent and allow for the distinction of global network labels in synthetic and real networks. In addition, we evaluate various feature extraction methods applied to the sorted adjacency matrix for classification purposes, including data projection, deep learning feature extraction, CLBP analysis, Hu moments, and classical measurements. Our results on synthetic models, metabolic networks, and social networks demonstrate that our approach can classify networks with over 90% accuracy, outperforming the accuracy rates of compared works [4], [5], [6].

The paper is structured as follows: Section 2 discusses the construction of the adjacency matrix, its significance for complex network analysis, and the proposed ordination. Section 3 describes the datasets evaluated in the study and the literature methods for comparison. In Section 4, we present two evaluations: first, a visual analysis of synthetic networks to examine whether the proposed ordination highlights important characteristics of the model (see Section 4.1). Second, we use various signature methods as descriptors of the networks for classification, and the results are presented in Section 4.2. Finally, in Section 5, we summarize the discussion and highlight the paper’s main contributions.","This study introduces a novel methodology for improving machine learning on complex networks by reordering the adjacency matrix based on specific patterns. The adjacency matrix, which represents complex networks, can have multiple permutations that don't alter the underlying graph. To address this issue, the proposed approach rearranges the matrix to highlight consistent patterns, enabling more effective feature extraction and classification. The results demonstrate that this methodology outperforms previous approaches in classifying synthetic and real-world networks, offering a promising avenue for enhancing pattern recognition in complex network analysis.


"
"Dense Hebbian neural networks: A replica symmetric picture of supervised learning
Author links open overlay panelElena Agliari a 1, Linda Albanese b f g, Francesco Alemanno b f, Andrea Alessandrelli c, Adriano Barra b f, Fosca Giannotti d e, Daniele Lotito c f, Dino Pedreschi c
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.physa.2023.129076
Get rights and content
Abstract
We consider dense, associative neural-networks trained by a teacher (i.e., with supervision) and we investigate their computational capabilities analytically, via statistical-mechanics tools, and numerically, via Monte Carlo simulations. In particular, we obtain a phase diagram which summarizes their performance as a function of the control parameters (e.g., quality and quantity of the training dataset, network storage, noise), that is valid in the limit of large network-size and structureless datasets. We also numerically test the learning, storing and retrieval capabilities of these networks on structured datasets such as MNist and Fashion MNist. As technical remarks, on the analytic side, we extend Guerra’s interpolation to tackle the non-Gaussian distributions involved in the post-synaptic potentials while, on the computational side, we insert Plefka’s approximation in the Monte Carlo scheme, to speed up the evaluation of the synaptic tensors, overall obtaining a novel and broad approach to investigate supervised learning in neural networks, beyond the shallow limit.

Introduction
The research covered in this paper, and in its twin [1] addressing the unsupervised counterpart, aims to provide an exhaustive picture of (supervised) Hebbian learning by dense networks (namely networks where interactions involve assemblies of 
 units rather than standard couples, i.e. 
), inspecting their emerging computational capabilities by means of statistical mechanics of disordered systems [2], [3], [4], [5]. Indeed, in the last decades, statistical mechanical has played a pivotal role for describing and quantifying information processing by neural networks for shallow (see e.g. [6], [7], [8], [9], [10], [11], [12]), deep (see e.g., [13], [14], [15], [16], [17]) and dense (see e.g. [18], [19], [20], [21], [22], [23]) architectures; in particular, investigations on dense spin-glasses [24], [25], [26], [27], [28], [29], [30], [31] constitute a fertile ground where Hebbian theories on dense neural networks can germinate. The ultimate reward we obtain by this approach lies in the knowledge of phase diagrams, namely plots in the space of the network control parameters (e.g., network size and connectivity, storage load, noise, dataset quality and quantity) where different emerging computational capabilities (e.g., learning, storage, recognition, associativity, denoising) are effectively related to particular regions of these diagrams, much as like the phase diagram of the water summarizes in a plot with solely three control parameters (i.e., pressure, volume and temperature) the different regimes (vapor, liquid, solid) in which a network of water molecules can be found. For the various modern neural architectures, the knowledge of such diagrams – where different working regions are split by “computational phase transitions” – can be helpful in the field of Sustainable AI (SAI) as this allows preparing the network in an optimal setting for a given task, with possible energy saving (e.g., by choosing the minimal architecture, or by avoiding training when a successful learning is theoretically forbidden) [32], [33], [34].

Since the first wave of the statistical mechanical formalization in the late eighties and early nineties of the past century, inspecting how these networks can be trained and can retrieve the learnt information has been a central question, addressed from various perspectives (see e.g. [9], [35], [36], [37]), yet in the dense network scenario most of the results are limited to retrieval issues [38], [39], [40]. In that simpler context, the network experiences just once a set of patterns and stores them in its Hebbian synaptic tensor for successive pattern-recognition usage but it does not undergo a real “learning process”, where, instead, the network typically has access only to a (noisy) sample of the reality by which it forms its own representation. Here we deepen this phenomenon moving from Hebbian storing to supervised Hebbian learning (and refer to [1] for its unsupervised counterpart).

We stress that, while from a biological modeling perspective these dense networks lack a clear inspiration (as neurons interact mainly in couples, although higher-order generalizations can still be seen as effective models [41]), in Machine Learning there are no restrictions preventing their usage, and here we prove that density can lead to significant rewards. In fact, dense networks can be used in two different operational modes, both forbidden to shallow machines, (i) a “ultra-storage” regime (that extends the high-load regime of the standard Hopfield model to the dense case), where these networks handle a by far larger number of patterns w.r.t. the standard 
 limit and (ii) a “ultra-tolerance” regime (that extends the low-load regime of the standard Hopfield model to the dense case), where these networks perform pattern recognition at prohibitive signal-to-noise ratio w.r.t. the 
 limit. As we will quantify along the paper, these capabilities have a cost: the main flaw in the usage of dense networks lays in the large volume of training examples required to achieve a satisfactory learning. However, unlike the unsupervised case [1], in the supervised setting this cost is independent of the order of interactions 
, thus, if the dataset is sufficiently large to train a dense network with a relatively low interaction order, we can “freely” increase the interaction order and take advantage of the related benefits.

The theory we work out here deals with networks learning from random datasets, where the analytical treatment is feasible, and it is successfully tested against Monte Carlo (MC) simulations and further corroborated on structured datasets (MNist and Fashion-MNist), where numerical results return overall a very good qualitative agreement with theoretical predictions. More technically, our analytical investigations allow us to solve for the statistical mechanics of these networks by adapting Guerra’s interpolation technique (see e.g. [20], [42], [43]). This generalization is non-trivial: as interactions are dense the fluctuations in the post-synaptic potentials are no longer Gaussian and the universality property of the quenched noise in spin glasses [44] cannot be applied directly, however, we could prove its validity also for the current case and this required a few passages, implying, for instance, the evaluation of the lower-order momenta of the distributions of the post-synaptic potentials and the application of central limit theorem (CLT). At the numerical level, as the update of the dense synaptic tensor results is a bottleneck for any dynamical update rule, we implement Plefka’s effective scheme [45] on the restricted Boltzmann machine (RBM) equivalent to the supervised dense Hopfield model to speed up this evaluation; this route provides overall a new approach for tackling the statistical mechanics of neural networks with complex architectures in broad generality.       ","This study explores supervised Hebbian learning in dense neural networks using statistical mechanics and Monte Carlo simulations. It aims to provide a comprehensive understanding of the computational capabilities of dense networks, considering factors such as dataset quality, quantity, and noise. The research presents a phase diagram that summarizes network performance and identifies different computational capabilities. The results demonstrate the potential of dense networks in tasks like pattern recognition and information retrieval, highlighting their advantages and limitations. The study contributes to the field of neural network research and offers insights into optimizing network settings for specific tasks.





"
"Time series clustering based on normal cloud model and complex network
Author links open overlay panelHailin Li, Manhua Chen
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.asoc.2023.110876
Get rights and content
Abstract
When data mining research is conducted, it is difficult to obtain precise domain knowledge to set a similarity threshold. Furthermore, noise and missing values are inevitable. Missing values and noise without pre-processing are challenges for many algorithms. A time-series clustering method is proposed based on the normal cloud model and complex networks. Matrix profile similarity measurement, normal cloud model generation and filtering, cloud model expectation curve weighting, degree centrality reweighting, and community discovery in complex networks are the five stages of the proposed clustering algorithm. Local features are considered, and the effects of missing values are reduced when performing similarity measurements. The normal cloud model can be used to set thresholds adaptively. The Louvain algorithm accomplishes the clustering task in complex networks without specifying the clusters. Experiments are conducted on 94 datasets and are compared with 8 clustering methods in the UCR time-series clustering benchmark study. Experimental results indicate that the proposed method can perform well on many datasets.

Graphical abstract

Download : Download high-res image (283KB)
Download : Download full-size image

Introduction
With the development of information science, technologies such as information acquisition, storage, transmission, and processing have become more and more mature. Consequently, a huge volume of data is saved, containing time-series data sorted according to a time axis [1]. These massive amounts of data are generated in daily life and contain plenty of potential value. As a process that can discover interests from vast quantities of data, data mining has naturally become a requirement. Data mining is also called knowledge discovery in databases (KDD) by some scholars, and its approaches involve clustering, classification, and others. Time series clustering is one of the research directions of time-series data mining [2]. Time series clustering has been widely used in financial investment [3], environmental engineering [4], biomedicine [5], and other fields. Time series clustering has become one of the main research directions in time-series data mining [6].

Time series clustering algorithms have been fully developed over time. Time series clustering algorithms have various branches. Generally speaking, there are algorithms based on statistical models, such as Gaussian mixture models and ARIMA models, and algorithms based on machine learning and data mining [7]. Algorithms based on statistical models assume that data in the same cluster follow a particular distribution. This algorithm requires setting assumptions and a long computation time, making it challenging to meet the demands of the big data era. Algorithms based on machine learning and data mining can be divided into different categories according to clustering ideologies and similarity measurement.

From the perspective of clustering ideologies, time series clustering algorithms can be divided into three categories: Partitioning, Hierarchy, and Density. K-medoids, K-means, and Fuzzy C-means are basic partitioning algorithms that optimize clustering results by minimizing the distance between data points and clustering centers. The partitioning algorithm has better time performance than other algorithms, but it needs to assume the number of clusters, and different parameters considerably influence the results. Fuzzy C-means (FCM) clustering uses the affiliation function to evaluate fuzzy concepts. It has proven to be efficient for image segmentation [8], [9]. Density Peaks and DBSCAN are popular density-based algorithms [10]. DBSCAN can find clusters of arbitrary shape and is insensitive to noise but sensitive to radius and density threshold. The Density Peaks algorithm requires the number of clusters as one of the input parameters. Agglomerative and divisive algorithms are hierarchical algorithms. For the divisive algorithm, all data points are initially assigned to a single cluster, and then clusters are split through the algorithm. Each data point is considered a cluster for the agglomerative algorithm, and then the clusters are merged by this algorithm.

From the perspective of similarity measurement, there are time series clustering algorithms based on Euclidean Distance (ED), shape-based (SBD), and Dynamic Time Warping (DTW). Algorithms based on ED and DTW are widely used. ED and its variants are suitable for time series of equal length and will not perform well when measuring unequal or distorted time series. DTW can be applied to time series with unequal length or distortion, but its high complexity makes it challenging to apply to high-dimensional time series. There are pruning algorithms to accelerate the calculation of DTW in order to resolve the high complexity problem [11]. Li proposed an extension of dynamic time warping based on time weight analysis, where the weights of pairs of time points from two series can be automatically calculated by measuring how far the historical time points are from the latest ones [12]. Duan proposed a dynamic time warping based on linear fuzzy information particles [13]. Li and Wei proposed a feature-weighted clustering method based on DTW and SBD [14]. Gharghabi pointed out that there were generally significant differences in the comparison of algorithms using ED, DTW, CVDTW, RotF (Rotating Forest), and MP (Matrix Profile) as similarity measurements. The RotF algorithm has the best performance, and Matrix Profile is obviously superior to Euclidean Distance but not significantly inferior to other methods [15]. The matrix profile algorithm can be regarded as an effective similarity measurement method.

There are numerous novel clustering techniques emerging, such as multi-task clustering, multivariate time series clustering, and subspace clustering [14]. Multivariate time series clustering is receiving increasing attention due to the growing complexity of daily life data. Li combined PCA and traditional K-Means to achieve MTS clustering and constructed projection axes as prototypes of each cluster. He proposed variable-based principal component analysis (VPCA) for the dimensionality reduction of MTS [16]. Li and Du proposed a multivariate time series clustering method based on a component relationship network (CRN) [17]. Li and Liu proposed a novel method based on complex networks for multivariate time series. This method relies on community detection technology to complete multivariate time series clustering [18].

It is challenging to obtain domain knowledge of all data when conducting data mining research in the era of big data. It takes much time and energy to set the similarity threshold without domain knowledge manually. Moreover, domain knowledge may be constantly changing in the era of big data. That is to say, even if domain knowledge is known, it is still difficult to maintain accurate domain knowledge to set similarity thresholds. At present, the difficulty of manually setting similarity thresholds has also been pointed out by some scholars. For example, Ferreira has proposed an approach in which the time series are considered as nodes in a network. This approach considers the relationships between time series as edges in a network. There is an edge between the nodes if the distance or correlation coefficient between the time series is greater than a threshold [19]. However, without domain knowledge, it is hard to set the threshold manually. Zhao Meng also believed it was difficult to determine the consensus threshold in advance when constructing complex networks to study the interaction consensus model of large group decision-making in social networks. The value of the consensus threshold greatly affects the overall consensus level [20].

According to the results of the studies on time series clustering, the main problems that still need to be solved to improve time series clustering are as follows: (1) Difficulty in adaptively setting the similarity threshold. Some traditional clustering algorithms have difficulty in determining which computed distances are similar or in setting similarity thresholds. (2) It is challenging to deal with missing values and noise. Missing values and noise pose a challenge to many algorithms. Filling and prediction are often used. However, the filled and predicted values do not fully represent the missing values. Similarly, smoothing can somewhat solve the noise problem, but the smoothed data still differs from the original data. Artificial data preprocessing, such as filling, prediction, and smoothing, can improve performance to some extent, but it still changes the original data. (3) Some algorithms with better clustering results require more parameters than others and do not perform well on large amounts of data. Algorithms that require less domain knowledge and parameters and can be scaled to large datasets are needed to meet the demands of the big data era.

In this paper, we proposed a clustering method based on the normal cloud model and complex network. The major contributions of this work can be summarized as follows: (1) We use the normal cloud model to determine which distances are similar and to set the similarity thresholds adaptively. (2) We use the matrix profile, the normal cloud model, and the complex network to cope with missing values and noise. Without data preprocessing, filling, predicting, smoothing, and other operations that may change the original data, the effect of missing values and noise is reduced. (3) We use the matrix profile and the community discovery algorithm to reduce the number of parameters required by the algorithm. There is no need to specify the number of clusters in our method. Moreover, both algorithms have the potential for further improvement and application to massive datasets.

For different similarity measurements, Gharghabi pointed out that Matrix Profile is superior to Euclidean Distance but not significantly inferior to other measurement methods [15]. Matrix Profile can be regarded as an effective similarity measurement method. Li and Liu proposed a novel method based on complex networks for multivariate time series. This method relies on community detection to complete multivariate time series clustering [18]. Community discovery algorithms for complex networks can be used to complete time series clustering. Cloud model theory is a cognitive method to deal with the uncertainties of human thinking. It is based on fuzzy sets and probability statistics [21]. The cloud model can be applied to deal with uncertainty problems. Lin introduced cloud models into risk assessment to optimize the results [22]. Wang improved the cloud model and made it possible to describe multiple types of uncertainty [23]. Zhou measured the similarity of two sequences by calculating the overlapping area of the cloud model and the deviation of a random concept from the normal distribution [24]. Gao used the cloud model to handle heterogeneous judgments effectively [25]. Li proposed fuzzy-based approaches to effectively handle uncertainty and other problems in risk evaluations [26]. Liu developed an effective multi-attribute decision-making method based on normal cloud models to solve multi-stage evaluation problems with multi-source heterogeneous information [27]. Dai provided a new way to quantitatively analyze danger nodes by applying cloud model theory to evaluate the risk level of each indicator [28]. The above study shows that there is a theoretical foundation for the method we proposed. ","This research introduces a time-series clustering method that combines the normal cloud model and complex networks to address challenges in data mining, particularly in the absence of precise domain knowledge to set similarity thresholds and in the presence of noise and missing values. The method involves stages such as matrix profile similarity measurement, normal cloud model generation, cloud model expectation curve weighting, degree centrality reweighting, and community discovery in complex networks. By considering local features and reducing the effects of missing values, this method offers an adaptive way to set similarity thresholds. Experimental results on various datasets demonstrate its effectiveness compared to existing clustering methods, making it suitable for time-series data mining in the era of big data.





"
"s), much as like ice, vapor and liquid for the water in thermodynamics. The phase diagram is painted in the plane of the control parameters: the fast noise 
 and the load 
, namely, the number of patterns 
 per neuron 
 that the network stores in the thermodynamic limit 
. Remarkably, this knowledge allows setting a priori the system in the desired regime. Since that milestone, phase transitions entered the field of computer science in a broad variety of aspects (Mézard and Montanari, 2009, Mézard et al., 2002a, Moore and Mertens, 2011). Not surprisingly, thus, much efforts have been spent to outline phase diagrams also in machine learning (Engel and Van den Broeck, 2001, Seung et al., 1992) and, in particular, for Boltzmann machines as they can serve as building blocks of deep architectures (Hinton and Salakhutdinov, 2006, Salakhutdinov and Hinton, 2009) and one can possibly rely on a formal equivalence between Boltzmann machines and Hopfield networks (Agliari et al., 2012, Barra et al., 2012, Barra et al., 2018, Leonelli et al., 2021, Mézard, 2017, Tubiana and Monasson, 2017).","This research investigates the emergence of a concept in shallow neural networks, focusing on restricted Boltzmann machines (RBMs) trained on blurred copies of unavailable ""archetypes."" The study reveals a critical sample size beyond which RBMs can successfully learn archetypes and function as generative models or classifiers. The authors use statistical mechanics methods and Monte Carlo simulations to establish a phase diagram for RBMs and Hopfield networks, highlighting regions in the parameter space where learning can occur. This work bridges the gap between biological neural networks and artificial machine learning, contributing to our understanding of neural network capabilities.





"
"A novel clustering method is proposed to achieve the improvements discussed above. First, we use the Matrix Profile algorithm in the similarity measurement. The Matrix Profile algorithm can make maximum use of the information contained in the time series without using specific numbers to fill in the missing values. At the same time, the sliding window of the matrix profile allows us to somewhat deal with the noise that is present in some fragments of the time series. In other words, we reduce the effects of missing values and noise without artificially altering the original data. Next, we use the normal cloud model to calculate different similarity thresholds based on different time series. The normal cloud model can be applied to set similarity thresholds adaptively without domain knowledge. Then some distances that are not similar because of noise or other reasons are filtered out. Next, the time series are mapped into a complex network to represent the similarity between time series fragments. The similarity is computed by the matrix profile algorithm. The use of the complex network robustness property further mitigates the effect of noise. Finally, the complex network community discovery algorithm is used to complete the time series clustering without specifying the number of clusters. Experiments are conducted on 94 datasets to verify the effectiveness and are compared with eight clustering methods in the UCR time-series clustering benchmark study. Experiments show that the proposed algorithm improves clustering accuracy.

The remainder of this paper is organized as follows: In Section 2, the basic definitions and related algorithms required for implementing the proposed method are introduced. In Section 3, a time series clustering method based on the normal cloud model and complex network(MPNCMI) is proposed. In this section, we introduce the work motivation and goals and the specific steps of the algorithm. In Section 4, the proposed method is compared with eight clustering methods in the UCR time series clustering benchmark to verify the algorithm’s validity. In this section, we present the datasets used for the experiment, the evaluation index, and some premises. Some improvements and parameters are also discussed in this section. In Section 5, the conclusions and future work are summarized","A novel time series clustering method, known as MPNCMI, is introduced in this paper to address key challenges in clustering time series data. The method leverages the Matrix Profile algorithm for similarity measurement, effectively handling missing values and noise without altering the original data. It utilizes the normal cloud model to set similarity thresholds adaptively, filters out dissimilar distances, maps time series into a complex network, and employs a community discovery algorithm for clustering without specifying the number of clusters. Experimental results on 94 datasets demonstrate that MPNCMI improves clustering accuracy compared to eight other clustering methods in the UCR time-series clustering benchmark study.





"
"Assessing road network resilience in disaster areas from a complex network perspective: A real-life case study from China
Author links open overlay panelMeng Wei, Jiangang Xu
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.ijdrr.2023.104167
Get rights and content
Abstract
Recently, resilience assessment has become a significant research topic, as it is crucial for regional disaster mitigation. It is important to identify road network resilience characteristics to improve regional traffic and handling capacity during emergencies, which ensures regional transport safety management and sustainable development. In this paper, a complex-network-based resilience assessment framework coupled with resilience curve characteristics, such as vulnerability, survivability, adaptability, responsiveness and resilience, was established. Typical Chinese regions with the most extensive disaster distribution were selected to reproduce real historical disaster scenarios for comparative analysis and research. The results showed that 1) the process of the change in the resilience of the road network during a disaster conforms to the characteristics of the resilience curve; 2) dense, homogeneous networks are more resilient than sparse, clustered networks; 3) road sections with more redundancy and substitutability are more resilient; and 4) the severity of the decline in traffic network performance in disaster areas is related to the attributes of road segments and network interference methods.

Introduction
In recent years, the frequent occurrence of regional disasters caused by global climate change has aroused extensive concern among governments and academia, leading to a broad focus on resilience research [[1], [2], [3]]. China, with its vast territory and complex geographical and climatic conditions, experiences frequent disasters. Moreover, the close interconnection and strong interdependence between urban and rural systems in China make the country highly susceptible to larger-scale damages when local regional disasters occur. For example, according to data released by the Chinese government, on May 12, 2008, the Wenchuan earthquake in China resulted in massive loss of life and property in the region. On 24 September of the same year, weather events such as heavy rainfall occurred in Beichuan County, near the epicentre of the Wenchuan earthquake. On August 8, 2010, a very large mudslide occurred in Zhouqu city, also located in the earthquake-stricken area, resulting in 1434 deaths and 331 missing persons. According to the latest research data of the National Oceanic and Atmospheric Administration (NOAA), in 2021, fires, floods, hurricanes, and other climate disasters caused $145 billion in losses (accounting for nearly 52 % of the global total) and 688 deaths. To scientifically address such crises, the Intergovernmental Panel on Climate Change (IPCC) [4] explicitly warned governments in its latest report that they must fully predict the major impacts of climate change on human settlements and infrastructure, such as the congestion caused by the failure of certain infrastructure systems and cascade failure in areas with large transport networks. Currently, modern society relies heavily on road networks for many functions and progress [5], and an understanding of the resilience laws of road networks can help us assess the potential impact of natural disasters (e.g., earthquakes, floods, etc.) on road infrastructure. A road network with a high level of resilience can reduce the extent of damage to the transportation system from disaster events and reduce the risk of traffic disruption, thereby increasing the resilience and rapid recovery of society and enabling faster repairs to damaged parts and a rapid return to normal service levels. At the same time, a high level of resilience can support rescue and emergency response operations to maximize the efficiency and effectiveness of rescue operations, compared to a low resilience network that is prone to persistent traffic congestion and network delays [[6], [7], [8], [9]]. Therefore, with limited overall resources, the resilience characteristics of disaster area road networks are analysed and studied, the characteristics of the resilience law of disaster area road networks are fully understood, and targeted improvement measures that encourage the regional transportation system to adapt to climate change and enhance the sustainable transportation safety in disaster areas are suggested.

Resilience originates from the Latin term resilio, which reflects the ability of a system to recover to the normal state after a disturbance. The study of resilience originated in the field of ecology, and this concept has since been widely used in many fields, such as sociology, economics, and engineering. In recent years, increasing attention has been given to the safety and stability of road traffic under the impact of extreme weather disasters, and the connotation of resilience has been further expanded in the field of transportation. Researchers have defined traffic resilience for different types networks, such as traffic [10,11], aviation [12,13], maritime transportation [14], and urban rail [15]networks. Although the understanding of traffic resilience varies, there is a consensus that resilience is an attribute of road networks, and it refers to the return to normal conditions after system disruption through adaptation and absorption, quickly recovering to the expected state [[16], [17], [18]]. With the development of traffic resilience research, the concept of criticality, which is used to express the critical state of a system, has been proposed. It is used to describe the contribution of certain traffic infrastructure components to the overall system performance. Road network criticality analysis has been employed to rank network components according to their relevance in the system and has been reported as a useful framework for planners to prioritise interventions [19,20].

In the abovementioned studies, transport systems are mostly modelled as networks, and complex networks, as a powerful tool for analysing large complex systems, constitute an effective method for describing traffic systems, which can highly abstract real complex traffic systems, help extract information on topological connections in transport networks, reveal the structural stability of transport networks and the propagation characteristics of dynamics when under attack and are useful for studying the resilience of traffic systems. This has practical value for studying the resilience of transport systems. Complex networks first originated from graph theory and topology applied in the field of mathematics in the early 18th century, and with the continuous development of computers and mathematical methods [[21], [22], [23]], they have now been widely used in various disciplines, and a series of research results have been achieved [24,25]. In the field of transportation, the most common approach to resilience analysis is to study the topology of a network and assess its structural characteristics. Typically, in the current literature, certain metrics are commonly used such as the maximum connected sub-graph size, average shortest path, intermediate centrality and connectivity to measure the road network resilience [26,27] or extended studies are performed based on these metrics [28]. At the same time, complex network analysis methods provide researchers with the means to simulate the impact of disasters and to monitor the remaining capacity response of the system after damage, and it becomes thus possible to simulate the cumulative failure of nodes or line segments of the transport system according to different modes [29].

However, previous work has not fully elaborated on the coupling relationship between resilience indicators and road network characteristics. Most studies only analyse the resilience of the transport system under stochastic or single scenarios, without considering comparisons between different hazard areas, as well as simulation scenarios more notably oriented towards virtual scenarios that must learn from historical data. For example, in existing studies, the resilience of road networks has been measured by quantifying road performance indicators (capacity and travel time) under different disaster scenarios [30,31], as well as by quantifying the network topology and the role of topology characteristics in the capacity of the transport system to manage disasters [32]. Several researchers [33] analysed and compared reliability under different perturbation scenarios, vulnerability levels and resilience indicators regarding transport similarities and differences. However, at the operational level, virtual scenarios are distorted. Furthermore, when analysing the impact on traffic resilience, it is necessary to identify the key factors that trigger disturbances and the key meta-elements that impact the overall system performance of the road network [34]. We must understand the corresponding resilience characteristics of road networks as accurately as possible and apply them as a basis for regional disaster preparedness and mitigation policy recommendations","This paper presents a complex network-based framework for assessing road network resilience in disaster-prone regions, focusing on typical regions in China with a history of disasters. The framework includes resilience curve characteristics such as vulnerability, survivability, adaptability, responsiveness, and resilience. The study finds that the resilience of road networks during disasters follows a resilience curve, with denser and more homogeneous networks showing higher resilience. Road sections with redundancy and substitutability exhibit greater resilience. The severity of traffic network performance decline in disaster areas is related to road segment attributes and network interference methods. This research contributes to understanding road network resilience in disaster scenarios, which is crucial for regional disaster mitigation and transportation safety management.





"
"This paper presents a complex network-based framework for assessing road network resilience in disaster-prone regions, focusing on typical regions in China with a history of disasters. The framework includes resilience curve characteristics such as vulnerability, survivability, adaptability, responsiveness, and resilience. The study finds that the resilience of road networks during disasters follows a resilience curve, with denser and more homogeneous networks showing higher resilience. Road sections with redundancy and substitutability exhibit greater resilience. The severity of traffic network performance decline in disaster areas is related to road segment attributes and network interference methods. This research contributes to understanding road network resilience in disaster scenarios, which is crucial for regional disaster mitigation and transportation safety management.




User

Therefore, the research objectives of this paper are as follows: 1) to establish a resilience assessment framework based on complex network theory that fits the changing characteristics of transport networks in disaster situations; and 2) to explore and understand the characteristics of resilience patterns of transport networks in disaster areas by using real disaster data based on the first objective. It should be noted that the resilience capacity discussion in this paper involves a value calculated from data obtained via disaster disturbance scenarios. For this reason, this paper is based on data collated from a research project on the Wenchuan earthquake, thereby reproducing real historical disaster scenarios, combining complex networks and resilience curve characteristics, assessing the corresponding index change characteristics of road networks under real disaster scenarios, and employing other disaster areas and corresponding simulated scenarios as references to comprehensively assess and study the resilience characteristics of road networks in disaster areas. The research objects are two road networks in two distinct disaster-prone regions in China, namely, the extremely hard-hit areas of the Wenchuan earthquake and the Wujiang region of the Taihu Lake basin, where the former is the primary research object with real historical seismic disaster data throughout, and the latter is the secondary research object as a control group for analysis. Although there is no real disaster data for the road network in the plain flood disaster area, it can be used as a comparative study to evaluate the characteristics and critical values of the indicators selected for the original road network in the two disaster areas, thus making the research results more scientific and informative. These two regions have different geographical environments and socio-economic levels; they are typical and widely representative of the region, thus increasing the adaptability of the research results.

Section snippets
Research data
“Disaster areas"" refer to specific areas that are more susceptible to natural or human-made disasters due to geographical, climatic, or other natural conditions. Specifically, these regions may be more vulnerable to earthquakes, floods, hurricanes, rainstorms, mountain fires and other natural disasters than other regions due to their geological, topographical or meteorological conditions. This definition is not only based on the degree of physical damage, but also involves the impact of

K-core
Since the characteristics of the 4- and 1-core changes are not obvious, the change in the ""3-core, 2-core, 0-core"" disaster process was analysed. As shown in Fig. 9, both the 3- and 2-core specimens experienced a decline to rise change process, and the change characteristics are consistent with the characteristics of the resilience curve. The proportions of 3- and 2-core nodes decreased by 12.39 % and 19.21 %, respectively, after the earthquake. However, the 4-core proportion of the road

The process of the change in the resilience of the road network during a disaster conforms to the characteristics of the resilience curve
Based on the previous section, we examined the resilience change of the road network in the disaster area during the disaster process and selected different regions for comparative analysis. From the analysis results, we could draw the following conclusions, which directly or indirectly verify that the resilience change process of the road network is consistent with the characteristics of the resilience curve. First, the road network in the disaster area has high vulnerability and low    ","This paper aims to achieve two research objectives: firstly, to establish a resilience assessment framework based on complex network theory tailored to the dynamics of transport networks in disaster scenarios, and secondly, to analyze the resilience patterns of transport networks in disaster-prone regions using real disaster data. The study focuses on two distinct disaster-prone regions in China, namely the Wenchuan earthquake-affected areas and the Wujiang region of the Taihu Lake basin. Real historical seismic disaster data is used for the primary research object (Wenchuan earthquake), while the latter serves as a control group for analysis. The research examines changes in resilience indicators, K-core analysis, and resilience curve characteristics to comprehensively assess road network resilience in disaster areas, enhancing the scientific understanding of resilience in different geographical and socio-economic contexts.





"
"Esketamine mitigates cognitive impairment following exposure to LPS by modulating the intestinal flora/subdiaphragmatic vagus nerve/spleen axis
Author links open overlay panelYuming Wu a b 1, Yujing Zhang a b 1, Bing Xie a b, Xinyu Zhang a b, Guangzhi Wang c, Shiying Yuan a b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.intimp.2023.111284
Get rights and content
Abstract
Introduction
Susceptibility to secondary infection often increases after primary infection. Secondary infections can lead to more severe inflammatory injuries; however, the underlying mechanisms are not yet fully elucidated.

Objective
To investigate whether esketamine treatment immediately after primary lipopolysaccharide (LPS) exposure could alleviate cognitive impairment caused by secondary infection.

Methods
Mice were injected intraperitoneally (IP) with LPS (5 mg/kg) 10 days apart. Esketamine (10, 15, or 30 mg/kg) was administered IP immediately after the primary LPS injection. Splenectomy or subdiaphragmatic vagotomy (SDV) was performed 7 days before secondary LPS exposure or broad-spectrum antibiotic administration.

Results
Splenomegaly was observed after the primary LPS injection on Days 3 and 10. Splenomegaly was attenuated by treatment with 30 mg/kg esketamine. Esketamine treatment prevented increased plasma proinflammatory cytokines levels and cognitive dysfunction induced by secondary LPS exposure. Mice that underwent splenectomy or SDV had lower proinflammatory cytokines levels, higher hippocampal brain-derived neurotrophic factor (BDNF) levels, and improved cognitive function 1 day after secondary infection, which was not further improved by esketamine. Fecal microbiota transplantation (FMT) from endotoxic mice treated with esketamine attenuated hippocampal BDNF downregulation and cognitive dysfunction only in pseudo germ-free (PGF) mice without splenectomy. FMT with fecal suspensions from esketamine-treated endotoxic mice abrogated splenomegaly only in PGF mice without SDV. Blocking BDNF signaling blocked esketamine’s ameliorating effects on secondary LPS exposure-induced cognitive dysfunction.

Conclusion
The intestinal flora/subdiaphragmatic vagus nerve/spleen axis-mediated hippocampal BDNF downregulation significantly affected secondary LPS-induced systemic inflammation and cognitive dysfunction. Esketamine preserves cognitive function via this mechanism.

Introduction
In mice, systemic injection of bacterial lipopolysaccharides (LPS) triggers the innate immune inflammatory response, causing illness [1], [2], [3], [4]. LPS injection animal models are widely used to study the psychiatric and cognitive responses to gram-negative bacterial infection, systemic inflammation, and neuroinflammation [5], [6]. Systemic administration of interleukin (IL)-1 receptor antagonists [7], [8] or IL-17A-neutralizing antibodies [9] is effective in the treatment of LPS-induced acute cognitive impairment. This suggests that LPS-induced systemic inflammation is crucial to cognitive dysfunction. Critically ill patients or mice recovering from an initial become more susceptible to secondary infections [10], [11], leading to severe inflammatory responses and tissue damage [11]. However, the underlying mechanisms of secondary infection-induced inflammatory injury are not fully understood.

Esketamine is a fast-acting and effective novel antidepressant medication for severe treatment-resistant depression. Esketamine mitigates surgery-induced cognitive impairment by enhancing the immune system's anti-inflammatory capabilities and lowering TNF-α, IL-6, and IL-1β production and secretion by macrophages, astrocytes, and microglia [12]. As an anesthetic, esketamine lowers IL-6 and IL-8 and raises IL-10 serum levels during cardiopulmonary bypass surgery [13]. Administering multiple doses of esketamine during radical prostatectomy decreases the production of inflammatory factors such as TNF-α and IL-6, resulting in a more significant anti-inflammatory impact [14]. However, the role of esketamine in sepsis-induced encephalopathy remains unknown.

(R,S)-ketamine, an N-methyl-D-aspartate (NMDA) receptor antagonist, has significant anti-inflammatory effects [15], [16], [17], [18], [19]. In fetal sheep, (R,S)-ketamine decreased the number of hypothalamic and hippocampal microglia and macrophages, leading to decreased cerebral immune response activation to transient hypoxia [18]. The combined use of (R,S)-ketamine may reduce the central and peripheral expression of inflammatory modulators in traumatic brain injury (TBI) [17]. In vitro and in vivo, (R,S)-ketamine, an equal mixture of (R)-ketamine and esketamine, reversed LPS-mediated upregulation of proinflammatory cytokines [16], [19]. (R)-ketamine exerts protective effects in various inflammatory diseases, including LPS-induced depression-like behaviors [20], ischemic stroke [21], Parkinson’s disease [22], and ulcerative colitis [23]. Esketamine also possesses anti-inflammatory properties [24], [25], [26]. Nonetheless, the role of esketamine in alleviating systemic inflammation and cognitive dysfunction in secondary LPS exposure, mimicking secondary infection post-recovery from primary LPS injec tion-mediated endotoxemia, remains unclear.","This study investigates the potential of esketamine in alleviating cognitive impairment caused by secondary infection following primary lipopolysaccharide (LPS) exposure. The research involves mouse models injected with LPS and examines the effects of esketamine treatment. Results indicate that esketamine treatment can mitigate splenomegaly, reduce proinflammatory cytokine levels, and prevent cognitive dysfunction induced by secondary LPS exposure. The study also explores the role of the intestinal flora/subdiaphragmatic vagus nerve/spleen axis and its impact on hippocampal brain-derived neurotrophic factor (BDNF) levels. Esketamine is found to preserve cognitive function through this mechanism, highlighting its potential in managing cognitive impairment associated with secondary infections.





"
"Dimensionality reduction in stochastic complex dynamical networks
Author links open overlay panelChengyi Tu a b, Jianhong Luo a, Ying Fan c, Xuwei Pan a
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.chaos.2023.114034
Get rights and content
Abstract
Complex systems often exhibit diverse dynamical behaviors in high-dimensional spaces that depend on various factors. Dimensionality reduction is a powerful tool for analyzing and understanding complex systems, aiming to find a low-dimensional representation of the complex system that preserves its essential features and reveals its underlying mechanisms and long-term dynamics. However, most existing methods for dimensionality reduction are limited to deterministic systems and cannot account for the stochastic effects that are ubiquitous in real-world complex networks. Here we develop a general analytical framework for dimensionality reduction of stochastic complex dynamical networks that can capture the essential features and long-term dynamics of the original system in a low-dimensional effective equation. The effective equation is a function of a set of effective parameters that are associated with specific system states and determine the network's dynamical behavior. We show that the standard deviation of the effective equation can be used to analyze the dynamic behavior and possible convergence of the stochastic complex dynamical network. Our framework can be applied to various types of stochastic complex dynamical networks and can reveal the underlying mechanisms and emergent phenomena of these systems.

Introduction
Dimensionality reduction of complex systems is a topic of growing interest across a wide range of scientific disciplines, such as biology, data science, chemistry, microelectronics and socioeconomics [[1], [2], [3], [4], [5], [6]]. Complex systems often exhibit rich and diverse dynamical behaviors that depend on various factors, such as interaction structures, parameter configurations and initial conditions [7,8]. These behaviors are embedded in high-dimensional spaces that pose significant challenges for analysis and understanding. Dimensionality reduction aims to find a low-dimensional representation of the complex system that preserves its essential features and reveals its underlying mechanisms and long-term dynamics. Such a representation can facilitate the control and design of the system, as well as the prevention of undesirable transitions. Dimensionality reduction is therefore a valuable tool for both practical applications and theoretical investigations.

Despite the considerable efforts devoted to dimensionality reduction of complex systems [9], many challenges remain, especially for complex dynamical networks. Previous methods have limited applicability or generality [[10], [11], [12], [13], [14]]. Gao et al. made the first attempt to apply dimensionality reduction to complex dynamical networks by developing an analytical tool that can extract the effective parameters of a high-dimensional networked system using mean-field approximations [15]. However, Tu et al. found a new condition that severely restricts the validity of Gao's framework [16]. They also extended Gao's framework to account for the heterogeneity in node-specific self- and coupling-dynamics [17], and discrete-time version [18]. To deal with arbitrary network structures, Laurence et al. proposed a polynomial approximation framework to reduce the complexity of dynamical networks [19,20]. Vegue apply their method to modular and heterogeneous directed networks [21]. Wu et al. introduced an improved dimension reduction method based on information entropy to predict network resilience [22]. Ghosh et al. constructed a one-dimensional reduced model of susceptible-infected-susceptible dynamics with higher-order interactions [23].","This study addresses the challenge of dimensionality reduction in complex stochastic dynamical networks. Complex systems often exhibit diverse behaviors in high-dimensional spaces, and dimensionality reduction aims to find a lower-dimensional representation that captures essential features and long-term dynamics. Existing methods for dimensionality reduction are typically designed for deterministic systems and do not account for stochastic effects present in real-world complex networks. The authors develop a general analytical framework that allows for dimensionality reduction of stochastic complex dynamical networks. This framework provides a low-dimensional effective equation, which is a function of effective parameters associated with specific system states. The standard deviation of this effective equation is used to analyze dynamic behavior and potential convergence in stochastic complex dynamical networks, enabling the study of underlying mechanisms and emergent phenomena in such systems.





"
"Subdiaphragmatic vagus nerve-regulated splenic immunoinflammatory cells contribute to splenomegaly and immunosuppression [27], [28]. The spleen also exacerbates inflammatory injury. Splenectomy attenuates systemic inflammation and enhances survival in mice with polymicrobial sepsis without impairing bacterial clearance [29]. Splenectomy reverses learning impairments and anxiety-like behaviors in response to red light exposure after LPS-induced lethal sepsis [30]. Splenectomy nullifies plasma proinflammatory cytokine augmentation and inhibits diminution in pseudo germ-free (PGF) mice transplanted with fecal bacteria from sleep-deprived LPS-injected mice [31]. Surgically-induced myocardial ischemia can cause migration of the splenic reservoir Ly-6Chigh monocytes into the circulation and ischemic myocardium [32]. Repeated social defeat (RSD) amplifies bone marrow monocytopoiesis, causing selective accumulation of Ly-6Chigh monocytes in the engorged spleen [33], [34]. These monocytes then traffic into the circulation and brain and exaggerate immunological and behavioral responses to acute subthreshold stress [35], [36]. Preventive splenectomy before RSD may block subthreshold stress-induced monocyte trafficking to the brain and anxiety-like behavior [35].

Therefore, this study tested the hypothesis that splenomegaly induced by primary systemic LPS administration could mediate systemic inflammation and cognitive dysfunction after secondary LPS exposure-mediated endotoxemia. We also investigated whether esketamine treatment immediately after primary LPS exposure could alleviate second-infection-induced cognitive impairment by inhibiting splenomegaly-mediated downregulation of hippocampal brain-derived neurotrophic factor (BDNF).

Section snippets
Animals
Eight-week-old male C57BL/6 mice were purchased from Vital River Laboratory Animal Technology Co Ltd., Beijing, China. The mice were housed under a 12 h dark/light cycle with ad libitum access to food and water. Mice were acclimated for 1 week before the study. All experiments followed the National Institute of Health (NIH) Guide for the Care and Use of Laboratory Animals (publications no. 80–23, revised 1996), and animal suffering, pain, and discomfort were minimized. The Animal Care and","This study explores the role of splenomegaly induced by primary systemic lipopolysaccharide (LPS) administration in mediating systemic inflammation and cognitive dysfunction following secondary LPS exposure-mediated endotoxemia. The research investigates whether esketamine treatment immediately after the initial LPS exposure can alleviate cognitive impairment by inhibiting splenomegaly-induced downregulation of hippocampal brain-derived neurotrophic factor (BDNF). The experiments involve eight-week-old male C57BL/6 mice, and all procedures adhere to ethical guidelines for animal care and use. The study aims to provide insights into the potential therapeutic effects of esketamine in mitigating cognitive dysfunction associated with secondary infections and systemic inflammation.





"
"A computational method for small molecule-RNA binding sites identification by utilizing position specificity and complex network information
Author links open overlay panelLichao Zhang a b, Kang Xiao a, Liang Kong b c
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.biosystems.2023.105094
Get rights and content
Abstract
Some computational methods have been given for small molecule-RNA binding site identification due to that it plays a significant role in revealing biology function researches. However, it is still challenging to design an accurate model, especially for MCC. We designed a feature extraction technology from two aspects (position specificity and complex network information). Specifically, complex network was employed to express the space topological structure and sequence position information for improving prediction effect. Then, the features fused position specificity and complex network information were input into random forest classifier for model construction. The AUC of 88.22%, 77.92% and 81.46% were obtained on three independent datasets (RB19, CS71, RB78). Compared with the existing method, the best MCC were obtained on three datasets, which were 8.19%, 0.59% and 4.35% higher than the state-of-the-art prediction methods, respectively. The outstanding performances show that our method is a powerful tool to identify RNA binding sites, helping to the design RNA-targeting small molecule drugs. The data and resource codes are available at https://github.com/Kangxiaoneuq/PCN_RNAsite.

Introduction
RNA plays a crucial role in different biological processes, such as protein translation (Joo and Benavides, 2021; Liu et al., 2022; Kim et al., 2020), amino acid transport (Yu et al., 2022; Liao et al., 2019; Scholz et al., 2021) and ribosome formation (Thomaidou et al., 2021; Sosorev and Kharlanov, 2021). Besides, some experiments show that small molecules contribute significantly to a number of biological processes (Venerito et al., 2019; Costales et al., 2020; Feng et al., 2021; Yu et al., 2020). In recent years, RNA has been proved to be an attractive target for small molecule drugs to treat diseases, (Disney, 2019) and the interaction between small molecules and RNA has become a hot research topic, such as the interaction of small organic compounds with different types of RNA (Nemr et al., 2019) as well as the biophysical properties of interaction between small molecules and RNA (Umuhire Juru et al., 2019). However, with the development of sequencing technology and drug researches, it is still challenging to determine the binding sites due to the biological experimental approaches are labor-intensive and time-consuming. Therefore, it is essential to establish computational methods to identify small molecule-RNA binding sites efficiently.

So far, with the development of bioinformatics, some computational methods have been designed and proven to be effective to identify RNA binding sites. Rsite calculated the Euclidean distance between any two nucleotides using the coordinates derived from the tertiary structure data, and selected the extreme points as the functional sites of the ncRNA (Zeng et al., 2015). RBind converted tertiary structure of RNA into a nucleotide interaction network and calculated some features in the network (Wang et al., 2018). Similar to Rsite, Zeng et al. developed Rsite2, (Zeng and Cui, 2016) a distance-based predictor by calculating the Euclidean distance of predicted secondary structure to identify the functional sites of noncoding RNA. Considering the distance-based methods may lead to false positive predictions, Zhang et al. proposed a sequence-based method CSBPI_Site, (Zhang et al., 2021) which improved the prediction of binding sites based on chemical shift, chemical bond, chemical property and site position. They demonstrated that motifs and the local intervals of sites were the core for identifying the binding sites. A recently proposed method RNAsite (Su et al., 2021) consisted of two independent components: a structure-based method RNAsite_str and a sequence-based method RNAsite_seq. Su et al. showed they were complementary to improve the overall prediction performance. Specifically, when no structural information was available, RNAsite_seq could be used to identify the binding nucleotides. When structure was available, they combined RNAsite_str and RNAsite_seq, and the performance was better than either of the two methods. The satisfied results demonstrated that the sequence and structural information of RNA is essential to identify functional sites.                       ","This study presents a computational method for identifying small molecule-RNA binding sites, which are crucial for understanding biological functions and drug design. The method utilizes a feature extraction approach that combines position specificity and complex network information to improve prediction accuracy. These features are then input into a random forest classifier for model construction. The method demonstrates excellent performance with high AUC values and outperforms existing methods, especially in terms of the Matthews Correlation Coefficient (MCC). The research provides a powerful tool for identifying RNA binding sites, which has implications for the development of RNA-targeting small molecule drugs.





"
"Using complex networks and multiple artificial intelligence algorithms for table tennis match action recognition and technical-tactical analysis
Author links open overlay panelHonglin Song, Yutao Li, Chenyi Fu, Feng Xue, Qiyue Zhao, Xingyu Zheng, Kunkun Jiang, Tianbiao Liu
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.chaos.2023.114343
Get rights and content
Abstract
This study developed two action recognition models using the YOLOv8-Alphapose two-stream spatial temporal graph convolutional networks (2s-STGCN), and the networks were used to recognize technical actions in table tennis. This study proposed a novel framework that merges dynamic and static complex network analysis with a community detection algorithm aimed at evaluating table tennis players' techniques, tactical patterns and styles. Two datasets that contain 8015 high-definition action videos of 37 elite players were constructed: a front-facing player technical action dataset (4154 videos) and a backwards-facing player technical action dataset (3861 videos). The results showed that YOLOv8-Alphapose-2s-STGCN achieved better recognition performance than seven other YOLOv8-Alphapose-based artificial intelligence algorithms (transformer, BiGRU, BiLSTM, GRU, LSTM, TCN and RNN algorithms) on both datasets and exhibited robust performance in practical applications. In the case study, multiple indicators were used to measure the importance of nodes (players' techniques) within the serving and receiving networks and within the two-round (winning and losing) networks. Dynamic complex network analysis was adopted to evaluate tactical styles and patterns. Furthermore, this study examined whether players and their opponents exhibit variability or similarity in their tactical patterns, focusing on the player networks and the two-round winning and losing networks. By integrating action recognition with process-focused match analysis, this study explored an innovative and comprehensive way to analyse matches, with implications for the performance analysis of table tennis players and players in related racket sports.

Introduction
The application of artificial intelligence in interdisciplinary research has increased, and studies that leverage computer vision and deep learning algorithms offer new perspectives and opportunities for understanding and analysing human actions in sports science [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11]]. In table tennis, these technologies have enabled the precise identification of rotation trajectories [12], classification of technical actions [[13], [14], [15]], and analysis and enhancement of players' performances [[16], [17], [18]]. These research results have facilitated critical developments in improving players' abilities and enhancing coaches' decision-making. Moreover, table tennis performance analysis has been applied in China to develop a mature and evolving theoretical system that attempts to translate complicated match processes into more understandable outcome-focused data for analysis [[19], [20], [21], [22], [23], [24], [25]]. Notably, there have been studies that have attempted to combine artificial intelligence algorithms with outcome-focused analysis, making it easier for table tennis practitioners to understand matches, evaluate differences between players, and provide tactical decisions about matches from a nonlinear perspective.

However, previous studies have overlooked the following two aspects. First, the constrained application scenarios and datasets of table tennis action recognition models have curtailed their utility for match performance analysis. Many AI-based algorithms have been adopted to improve the efficiency and accuracy of action recognition tasks in table tennis [26]. However, due to occlusion, high similarity, and subtle variations in table tennis players' execution techniques [[27], [28], [29], [30]], algorithms and models may not correctly recognize players' actions during a match. These challenges lead to a tendency to construct datasets in controlled environments [29,31,32]. This leads to standardized actions that facilitate higher model performance but are not applicable for action recognition in match scenarios [29]. Moreover, the differentiation of player actions during table tennis matches is challenging because players often use nonstandard technical actions during matches. Therefore, the action recognition dataset in table tennis match scenarios requires data annotated by experts who can precisely distinguish subtle and varied actions; such a dataset could enhance model applicability. The two factors mentioned above contribute to the high cost of producing this type of dataset [31], and few studies have addressed this issue [18], limiting the further development and application of technical action recognition in table tennis matches.    ","This study presents two action recognition models, YOLOv8-Alphapose two-stream spatial temporal graph convolutional networks (2s-STGCN), for recognizing technical actions in table tennis. The models outperform other artificial intelligence algorithms on two datasets containing high-definition action videos of elite players. The research combines dynamic and static complex network analysis with a community detection algorithm to evaluate table tennis players' techniques, tactical patterns, and styles. It provides a comprehensive approach to match analysis, enhancing the understanding of players' performances and aiding coaches in decision-making, with potential applications in related racket sports.





"