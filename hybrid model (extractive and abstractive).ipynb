{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be51c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation as punctuations\n",
    "from heapq import nlargest\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Extractive Part------------------------------------------\n",
    "stopwords = list(STOP_WORDS)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def generate_extractive_summary(doc):\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    punctuation = punctuations + '\\n'\n",
    "\n",
    "    word_frequencies = {}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in stopwords:\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "\n",
    "    if not word_frequencies:\n",
    "        return \"No valid words in the document.\"\n",
    "\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "\n",
    "    sentence_tokens = [sent for sent in doc.sents]\n",
    "\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "    select_length = int(len(sentence_tokens) * 0.3)\n",
    "    summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
    "    final_summary = [word.text for word in summary]\n",
    "    return ' '.join(final_summary)\n",
    "\n",
    "def generate_abstractive_summary(encoding):\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=encoding['input_ids'].to(device),\n",
    "        attention_mask=encoding['attention_mask'].to(device),\n",
    "        max_length=150,  # Adjust the max_length as needed\n",
    "        num_beams=4,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Abstractive Part-------------------------------------------\n",
    "checkpoint_path = 't5_English_final.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Create a file to store the final abstractive summaries\n",
    "output_file_path = 'final_abstractive_summaries.txt'\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "    for idx, section_data in enumerate(section_data_list):\n",
    "        section_doc = nlp(section_data)\n",
    "\n",
    "        # Generate extractive summary\n",
    "        extractive_summary = generate_extractive_summary(section_doc)\n",
    "\n",
    "        # Generate abstractive summary for the extractive summary\n",
    "        encoding = tokenizer(\n",
    "            extractive_summary,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        abstractive_summary = generate_abstractive_summary(encoding)\n",
    "\n",
    "        # Write the summaries to the output file\n",
    "        output_file.write(f\"Section {idx + 1}\\n{abstractive_summary}\\n\\n\")\n",
    "\n",
    "print(\"Final abstractive summaries saved to 'final_abstractive_summaries.txt'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
